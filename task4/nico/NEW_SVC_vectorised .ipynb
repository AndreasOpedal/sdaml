{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import kurtosis,skew\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pyeeg\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import log, floor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor, GradientBoostingClassifier )\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import yasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_eeg1 = pd.read_csv(\"train_eeg1.csv\").drop(\"Id\", axis = 1)\n",
    "xtrain_eeg2 = pd.read_csv(\"train_eeg2.csv\").drop(\"Id\", axis = 1)\n",
    "xtrain_emg = pd.read_csv(\"train_emg.csv\").drop(\"Id\", axis = 1)\n",
    "\n",
    "ytrain = pd.read_csv(\"train_labels.csv\").drop(\"Id\", axis = 1)\n",
    "\n",
    "xtest_eeg1 = pd.read_csv(\"test_eeg1.csv\").drop(\"Id\", axis = 1)\n",
    "xtest_eeg2 = pd.read_csv(\"test_eeg2.csv\").drop(\"Id\", axis = 1)\n",
    "xtest_emg = pd.read_csv(\"test_emg.csv\").drop(\"Id\", axis = 1)\n",
    "\n",
    "xtrain_eeg1 = xtrain_eeg1.values\n",
    "xtrain_eeg2 = xtrain_eeg2.values\n",
    "xtrain_emg = xtrain_emg.values\n",
    "\n",
    "xtest_eeg1 = xtest_eeg1.values \n",
    "xtest_eeg2 = xtest_eeg2.values\n",
    "xtest_emg = xtest_emg.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if (ypred_crf[i] != ypred_boost[i] and  \\nypred_crf[i] != ypred_svm[i] and\\nypred_svm[i] != ypred_boost[i]): \\n\\nrand = np.random.choice(np.arange(1, 4), p=[p_crf,p_boost,p_svm])\\n\\npredictions[i] = rand'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _log_n(min_n, max_n, factor):\n",
    "    \"\"\"\n",
    "    Creates a list of integer values by successively multiplying a minimum\n",
    "    \"\"\"\n",
    "    max_i = int(floor(log(1.0 * max_n / min_n) / log(factor)))\n",
    "    ns = [min_n]\n",
    "    for i in range(max_i + 1):\n",
    "        n = int(floor(min_n * (factor ** i)))\n",
    "        if n > ns[-1]:\n",
    "            ns.append(n)\n",
    "    return np.array(ns, dtype=np.int64)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def _linear_regression(x, y):\n",
    "    \"\"\"Fast linear regression using Numba.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : ndarray, shape (n_times,)\n",
    "        Variables\n",
    "    Returns\n",
    "    -------\n",
    "    slope : float\n",
    "        Slope of 1D least-square regression.\n",
    "    intercept : float\n",
    "        Intercept\n",
    "    \"\"\"\n",
    "    n_times = x.size\n",
    "    sx2 = 0\n",
    "    sx = 0\n",
    "    sy = 0\n",
    "    sxy = 0\n",
    "    for j in range(n_times):\n",
    "        sx2 += x[j] ** 2\n",
    "        sx += x[j]\n",
    "        sxy += x[j] * y[j]\n",
    "        sy += y[j]\n",
    "    den = n_times * sx2 - (sx ** 2)\n",
    "    num = n_times * sxy - sx * sy\n",
    "    slope = num / den\n",
    "    intercept = np.mean(y) - slope * np.mean(x)\n",
    "    return slope, intercept\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "\n",
    "def _dfa(x):\n",
    "    \"\"\"\n",
    "    Utility function for detrended fluctuation analysis\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    nvals = _log_n(4, 0.1 * N, 1.2)\n",
    "    walk = np.cumsum(x - x.mean())\n",
    "    fluctuations = np.zeros(len(nvals))\n",
    "\n",
    "    for i_n, n in enumerate(nvals):\n",
    "        d = np.reshape(walk[:N - (N % n)], (N // n, n))\n",
    "        ran_n = np.array([float(na) for na in range(n)])\n",
    "        d_len = len(d)\n",
    "        slope = np.empty(d_len)\n",
    "        intercept = np.empty(d_len)\n",
    "        trend = np.empty((d_len, ran_n.size))\n",
    "        for i in range(d_len):\n",
    "            slope[i], intercept[i] = _linear_regression(ran_n, d[i])\n",
    "            y = np.zeros_like(ran_n)\n",
    "            # Equivalent to np.polyval function\n",
    "            for p in [slope[i], intercept[i]]:\n",
    "                y = y * ran_n + p\n",
    "            trend[i, :] = y\n",
    "        # calculate standard deviation (fluctuation) of walks in d around trend\n",
    "        flucs = np.sqrt(np.sum((d - trend) ** 2, axis=1) / n)\n",
    "        # calculate mean fluctuation over all subsequences\n",
    "        fluctuations[i_n] = flucs.sum() / flucs.size\n",
    "\n",
    "    # Filter zero\n",
    "    nonzero = np.nonzero(fluctuations)[0]\n",
    "    fluctuations = fluctuations[nonzero]\n",
    "    nvals = nvals[nonzero]\n",
    "    if len(fluctuations) == 0:\n",
    "        # all fluctuations are zero => we cannot fit a line\n",
    "        dfa = np.nan\n",
    "    else:\n",
    "        dfa, _ = _linear_regression(np.log(nvals), np.log(fluctuations))\n",
    "    return dfa\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "def matrix_dfa(x):\n",
    "    dfa_ = np.zeros((x.shape[0],), dtype=np.float64)\n",
    "    for i in (np.arange(x.shape[0])):\n",
    "        dfa_[i] = _dfa(np.asarray(x[i], dtype=np.float64))\n",
    "    return np.reshape(dfa_, (-1, 1))\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def _embed(x, order=3, delay=1):\n",
    "    \"\"\" VECTORIZED!! Kind of TESTED. Time-delay embedding. x is an (nxd) matrix\n",
    "    \"\"\"\n",
    "    N = x.shape[1]\n",
    "    if order * delay > N:\n",
    "        raise ValueError(\"Error: order * delay should be lower than x.size\")\n",
    "    if delay < 1:\n",
    "        raise ValueError(\"Delay has to be at least 1.\")\n",
    "    if order < 2:\n",
    "        raise ValueError(\"Order has to be at least 2.\")\n",
    "    Y = np.zeros((x.shape[0], N - (order - 1) * delay, order))\n",
    "    for i in range(order):\n",
    "        Y[:, :, i] = x[:, i * delay:i * delay + Y.shape[1]]\n",
    "    return Y\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def hfd(X, Kmax):\n",
    "    \"\"\" VECTORIZED!!! TESTED: Matches the for loop output. Can test easily comparing\n",
    "    to the pyeeg.hfd() function. X now a (nxd) matrix.\n",
    "    Compute Higuchi Fractal Dimension of a time series X. kmax\n",
    "     is an HFD parameter\n",
    "    \"\"\"\n",
    "    L = []\n",
    "    x = []\n",
    "    N = X.shape[1]\n",
    "    for k in (range(1, Kmax)):\n",
    "        # Lk = np.empty(shape=[0, ])\n",
    "        Lk = np.empty(shape=[X.shape[0], 1])\n",
    "        for m in range(0, k):\n",
    "            Lmk = 0\n",
    "            for i in range(1, int(np.floor((N - m) / k))):\n",
    "                Lmk += np.abs(X[:, m + i * k] - X[:, m + i * k - k])\n",
    "            Lmk = Lmk * (N - 1) / np.floor((N - m) / float(k)) / k\n",
    "            Lmk = np.reshape(Lmk, (Lmk.shape[0], 1))\n",
    "            Lk = np.hstack((Lk, Lmk))\n",
    "\n",
    "        # Remove that first placeholder column of zeros in Lk:\n",
    "        Lk = Lk[:, 1:]\n",
    "        L.append(np.log(np.mean(Lk, axis=1)))\n",
    "        x.append([np.log(float(1) / k), 1]) # Fix this!!!\n",
    "\n",
    "    (p, _, _, _) = np.linalg.lstsq(x, L)\n",
    "    return p[0]\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def pfd(X):\n",
    "    \"\"\"VECTORIZED!!! TESTED, matches the 1d time series output. Now accepts (nxd) matrices as input\n",
    "    Compute Petrosian Fractal Dimension of a time series from either two\n",
    "    cases below:\n",
    "        1. X, the time series of type list (default)\n",
    "        2. D, the first order differential sequence of X (if D is provided,\n",
    "           recommended to speed up)\n",
    "    In case 1, D is computed using Numpy's difference function.\n",
    "    To speed up, it is recommended to compute D before calling this function\n",
    "    because D may also be used by other functions whereas computing it here\n",
    "    again will slow down.\n",
    "    \"\"\"\n",
    "    n = X.shape[1]\n",
    "    diff = np.diff(X, axis=1)\n",
    "    N_delta = np.sum(diff[:, 1:-1] * diff[:, 0:-2] < 0, axis=1)\n",
    "    return np.log10(n) / (np.log10(n) + np.log10(n / (n + 0.4 * N_delta)))\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def fisher_info(X, Tau=3, DE=1, W=None):\n",
    "    \"\"\" VECTORIZED, TESTED, gives approximate results but not exact. Compute SVD Entropy from either two cases below:\n",
    "    \"\"\"\n",
    "    if W is None:\n",
    "        Y = _embed(X, Tau, DE)\n",
    "        W = np.linalg.svd(Y, compute_uv=0)\n",
    "        W = np.divide(W, np.reshape(np.sum(W, axis=1), (-1, 1)))  # normalize singular values\n",
    "    return -1 * np.sum(W * np.log(W), axis=1)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def extract_bandpower_eeg(signal, frequency = 128):\n",
    "    for i in (np.arange(signal.shape[0] / 100) + 1):\n",
    "        if i == 1:\n",
    "            df = yasa.bandpower(signal[0:int(100*i),:], sf=frequency)\n",
    "        else:\n",
    "            df = df.append(yasa.bandpower(signal[int(100*(i-1)):int(100*i),:], sf=frequency))\n",
    "    \n",
    "    df = df.set_index(np.arange(signal.shape[0]))\n",
    "    df = df.drop(columns = [\"FreqRes\",\"Relative\"], axis = 1)\n",
    "    return df\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "'''input must be np.ndarray'''\n",
    "\n",
    "def vectorized_adv_stat(signal, fs=128):\n",
    "    K_boundary = 10         # to be tuned\n",
    "    t_fisher = 12          # to be tuned\n",
    "    d_fisher = 40          # to be tuned\n",
    "    features_num = 11\n",
    "    threshold =  0.0009\n",
    "    advanced_stats = np.zeros((signal.shape[0],features_num))\n",
    "    # Missing fisher info and dfa\n",
    "    feat_array = np.array([\n",
    "                           fisher_info(signal, t_fisher, d_fisher),\n",
    "                           pfd(signal),\n",
    "                           hfd(signal, K_boundary),\n",
    "                           np.sum((np.power(np.abs(signal),(-0.3)) > 20), axis=1),\n",
    "                           np.sum((np.abs(signal)) > threshold, axis=1),\n",
    "                           np.std(np.power(np.abs(signal),(0.05)), axis=1),\n",
    "                           np.sqrt(np.mean(np.power(np.diff(signal, axis=1), 2), axis=1)),\n",
    "                           np.mean(np.abs(np.diff(signal, axis=1)), axis=1),\n",
    "                           np.mean(np.power(signal, 5), axis=1),\n",
    "                           np.sum(np.power(signal, 2), axis=1)\n",
    "                           ]).T\n",
    "    #feat_array = np.concatenate((feat_array, matrix_dfa(signal)), axis=1) # Concatenate the lengthy dfa calculation\n",
    "\n",
    "    return feat_array\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def simple_stats(signal, fs = 128):\n",
    "           \n",
    "    if (len(signal.shape) > 1) and (signal.shape[1]!=1):\n",
    "        simple_stats = np.array([np.mean(signal, axis=1), \n",
    "                        np.median(signal, axis=1),\n",
    "                        np.std(signal, axis=1), \n",
    "                        np.max(signal, axis=1),\n",
    "                        np.min(signal, axis=1), \n",
    "                        kurtosis(signal, axis=1),\n",
    "                        skew(signal, axis=1), \n",
    "                        np.sum(np.abs(signal), axis = 1)]).T\n",
    "                        \n",
    "    else:\n",
    "        print(\"Not Tested with this input!\")\n",
    "        simple_stats =  np.array([np.mean(signal), \n",
    "                        np.median(signal), \n",
    "                        np.std(sigal),\n",
    "                        np.max(signal), \n",
    "                        np.min(signal), \n",
    "                        float(kurtosis(signal)),\n",
    "                        float(skew(signal))])\n",
    "        \n",
    "    \n",
    "\n",
    "    return (simple_stats)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def EEG_feature_extraction(signal, fs = 128): \n",
    "    eeg_features = np.concatenate((extract_bandpower_eeg(signal, frequency = 128), vectorized_adv_stat(signal, fs=128),\n",
    "                                   simple_stats(signal, fs = 128)), axis = 1)\n",
    "    \n",
    "    return(eeg_features)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def EMG_feature_extraction(signal, fs = 128):\n",
    "    \n",
    "    features_num_emg = 6\n",
    "    \n",
    "    if (len(signal.shape) > 1) and (signal.shape[1]!=1):\n",
    "        simple_stats = np.array([np.mean(signal, axis=1), \n",
    "                        np.median(signal, axis=1),\n",
    "                        np.std(signal, axis=1), \n",
    "                        np.max(signal, axis=1),\n",
    "                        np.min(signal, axis=1), \n",
    "                        kurtosis(signal, axis=1),\n",
    "                        skew(signal, axis=1), \n",
    "                        pd.Series(np.sum(np.abs(signal), axis = 1))]).T\n",
    "                        \n",
    "    else:\n",
    "        print(\"Not Tested with this input!\")\n",
    "        simple_stats =  np.array([np.mean(signal), \n",
    "                        np.median(signal), \n",
    "                        np.std(sigal),\n",
    "                        np.max(signal), \n",
    "                        np.min(signal), \n",
    "                        float(kurtosis(signal)),\n",
    "                        float(skew(signal))])\n",
    "\n",
    "    advanced_stats = np.zeros((signal.shape[0],features_num_emg))\n",
    "    for i in tqdm((np.arange(signal.shape[0]))):\n",
    "        feat_array = np.array([\n",
    "                              np.median(signal[i,:] ** 2), \n",
    "                              np.median(np.abs(np.diff(signal[i,:]))), \n",
    "                              np.std(np.abs(np.diff(signal[i,:]))), \n",
    "                              np.sum(np.abs(np.diff(signal[i,:]))), \n",
    "                              np.mean(np.power(np.diff(signal[i,:]), 2)),\n",
    "                              np.std(abs(signal[i,:]))\n",
    "                            ])\n",
    "\n",
    "        advanced_stats[i, :] = feat_array \n",
    "        \n",
    "        union_smpl_adv = np.concatenate((simple_stats, advanced_stats), axis = 1)\n",
    "\n",
    "    return(union_smpl_adv)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def hard_voter(ypred_crf, ypred_boost, ypred_svm, \n",
    "               ypred_logi, ypred_RandFor):             # add p_boost, p_crf, p_svm, p_logi,p_RandFor\n",
    "    \n",
    "    predictions = np.zeros(xtest.shape[0])\n",
    "    \n",
    "    for i in range(xtest.shape[0]):\n",
    "\n",
    "        counts = np.bincount(np.array([ypred_crf[i],ypred_boost[i],ypred_svm[i], \n",
    "                                       ypred_logi[i], ypred_RandFor[i]]))\n",
    "        predictions[i] = np.argmax(counts)\n",
    "        \n",
    "    return(np.asarray(predictions, dtype=np.int32)) \n",
    "\n",
    "\n",
    "'''if (ypred_crf[i] != ypred_boost[i] and  \n",
    "ypred_crf[i] != ypred_svm[i] and\n",
    "ypred_svm[i] != ypred_boost[i]): \n",
    "\n",
    "rand = np.random.choice(np.arange(1, 4), p=[p_crf,p_boost,p_svm])\n",
    "\n",
    "predictions[i] = rand'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicolò Grometto\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "C:\\Users\\Nicolò Grometto\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:202: RuntimeWarning: divide by zero encountered in power\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6361a4d935e54ababed4a88f83889804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=64800), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7358e02ac545aca6671afe284fb54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(64800, 60) (43200, 60)\n"
     ]
    }
   ],
   "source": [
    "xtrain_eeg1_processed = EEG_feature_extraction(xtrain_eeg1, 128)\n",
    "xtrain_eeg2_processed = EEG_feature_extraction(xtrain_eeg2, 128)\n",
    "xtrain_emg_processed = EMG_feature_extraction(xtrain_emg, 128)\n",
    "xtrain = np.concatenate((xtrain_eeg1_processed, \n",
    "                         xtrain_eeg2_processed, \n",
    "                         xtrain_emg_processed), \n",
    "                         axis = 1)\n",
    "\n",
    "xtest_eeg1_processed = EEG_feature_extraction(xtest_eeg1, 128)\n",
    "xtest_eeg2_processed = EEG_feature_extraction(xtest_eeg2, 128)\n",
    "xtest_emg_processed = EMG_feature_extraction(xtest_emg, 128)\n",
    "xtest = np.concatenate((xtest_eeg1_processed, \n",
    "                         xtest_eeg2_processed, \n",
    "                         xtest_emg_processed), \n",
    "                         axis = 1)\n",
    "\n",
    "print(xtrain.shape, xtest.shape)\n",
    "\n",
    "xtrain_grid = pd.DataFrame(xtrain)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "xtrain_rescaled = scaler.fit_transform(xtrain_grid)\n",
    "xtest_rescaled = scaler.fit_transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] classifier__C=0.01, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.01, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total= 1.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] classifier__C=0.01, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.01, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total= 1.1min\n",
      "[CV] classifier__C=0.01, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.01, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total= 1.1min\n",
      "[CV] classifier__C=0.1, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.1, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  26.4s\n",
      "[CV] classifier__C=0.1, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.1, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  37.4s\n",
      "[CV] classifier__C=0.1, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.1, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  37.3s\n",
      "[CV] classifier__C=0.3, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.3, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  22.0s\n",
      "[CV] classifier__C=0.3, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.3, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  32.1s\n",
      "[CV] classifier__C=0.3, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.3, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  31.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  6.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9200190564541171\n",
      "{'classifier__C': 0.1, 'classifier__class_weight': 'balanced', 'classifier__gamma': 'auto', 'classifier__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "xtrain_grid = pd.DataFrame(xtrain)\n",
    "\n",
    "steps = [(\"scaler\", StandardScaler()), (\"classifier\", SVC())]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__kernel\": [\"rbf\"],\n",
    "              \"classifier__gamma\": [\"auto\"],\n",
    "              \"classifier__C\": [0.01, 0.1,0.3],  \n",
    "              \"classifier__class_weight\": [\"balanced\"]\n",
    "             }\n",
    "grid_svm = GridSearchCV(pipeline, parameters, cv = 3, scoring = 'balanced_accuracy', verbose = 2)\n",
    "\n",
    "grid_svm.fit(xtrain_grid, ytrain.values.ravel())\n",
    "print(grid_svm.best_score_)\n",
    "print(grid_svm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS Gradient Boosting Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 10.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8409677034053105\n",
      "{'classifier__learning_rate': 0.1, 'classifier__max_depth': 8, 'classifier__max_features': 20, 'classifier__n_estimators': 200, 'impute__fill_value': 0, 'impute__strategy': 'median'}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Gradient Boosting APPROACH -- GRID-SEARCH CV\n",
    "''' \n",
    "xtrain_grid = pd.DataFrame(xtrain)\n",
    "\n",
    "\n",
    "steps = [(\"impute\", SimpleImputer()),\n",
    "         (\"scaler\", preprocessing.StandardScaler()), \n",
    "         (\"classifier\", GradientBoostingClassifier())]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"impute__strategy\": [\"median\"],\n",
    "              \"impute__fill_value\": [0],\n",
    "              \"classifier__max_depth\": [8],\n",
    "              \"classifier__n_estimators\": [200],\n",
    "              \"classifier__learning_rate\": [0.1],\n",
    "              \"classifier__max_features\": [20]\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, cv = 3,scoring = 'balanced_accuracy', verbose = 1)\n",
    "grid.fit(xtrain_grid, ytrain.values.ravel())\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:   50.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9314424377687452\n",
      "{'classifier__C': 0.05, 'classifier__class_weight': 'balanced', 'classifier__max_iter': 20, 'classifier__multi_class': 'auto', 'classifier__verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "steps = [(\"scaler\", StandardScaler()), (\"classifier\", LogisticRegression())]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__multi_class\": [\"auto\"],\n",
    "              \"classifier__C\": [0.001,0.05,0.08,0.01],  \n",
    "              \"classifier__class_weight\": [\"balanced\"], \n",
    "              \"classifier__max_iter\": [10,20],\n",
    "              \"classifier__verbose\": [0]\n",
    "             }\n",
    "grid_logi = GridSearchCV(pipeline, parameters, cv = 3, scoring = 'balanced_accuracy', verbose = 1)\n",
    "\n",
    "grid_logi.fit(xtrain_grid, ytrain.values.ravel())\n",
    "print(grid_logi.best_score_)\n",
    "print(grid_logi.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8869171527404393\n",
      "{'classifier__class_weight': 'balanced', 'classifier__max_depth': 3, 'classifier__n_estimators': 40}\n"
     ]
    }
   ],
   "source": [
    "steps = [(\"scaler\", StandardScaler()), (\"classifier\", RandomForestClassifier())]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__n_estimators\": [20,30,40,45],\n",
    "              \"classifier__max_depth\": [4,3,2],  \n",
    "              \"classifier__class_weight\": [\"balanced\"]\n",
    "              }\n",
    "grid_RandFor = GridSearchCV(pipeline, parameters, cv = 3, scoring = 'balanced_accuracy', verbose = 1)\n",
    "\n",
    "grid_RandFor.fit(xtrain_grid, ytrain.values.ravel())\n",
    "print(grid_RandFor.best_score_)\n",
    "print(grid_RandFor.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM done\n",
      "Gradient Boosting done\n",
      "Multinomial Logistic done\n",
      "Random Forest Done\n"
     ]
    }
   ],
   "source": [
    "'''SVM'''\n",
    "clf_svm = svm.SVC(kernel='rbf', C = 0.1, gamma = 'auto', class_weight = 'balanced')\n",
    "clf_svm.fit(xtrain_rescaled, ytrain)\n",
    "ypred_svm = clf_svm.predict(xtest_rescaled)\n",
    "print('SVM done')\n",
    "\n",
    "'''Gradient Boosting'''\n",
    "clf_boost = GradientBoostingClassifier(n_estimators = 200, max_depth = 8, learning_rate = 0.1, max_features = 20)\n",
    "clf_boost.fit(xtrain_rescaled, ytrain)\n",
    "ypred_boost = clf_boost.predict(xtest_rescaled)\n",
    "print('Gradient Boosting done')\n",
    "\n",
    "'''Multinomial Logsistic'''\n",
    "clf_logi = LogisticRegression(random_state=10, C = 0.05, class_weight = 'balanced', max_iter = 20, multi_class='auto')\n",
    "clf_logi.fit(xtrain_rescaled, ytrain)\n",
    "ypred_logi = clf_logi.predict(xtest_rescaled)\n",
    "print('Multinomial Logistic done')\n",
    "\n",
    "'''Random Forest'''\n",
    "clf_RandFor = RandomForestClassifier(max_depth=3, random_state=1, class_weight = 'balanced', n_estimators = 40)\n",
    "clf_RandFor.fit(xtrain_rescaled, ytrain)\n",
    "ypred_RandFor = clf_RandFor.predict(xtest_rescaled)\n",
    "print('Random Forest Done')\n",
    "\n",
    "'''CRF'''\n",
    "ypred_crf = pd.read_csv(\"ypred_crf.csv\").drop(\"Id\", axis = 1)\n",
    "ypred_crf = np.reshape(ypred_crf.values, (-1,))\n",
    "print('CRF Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Classifier -- Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = hard_voter(ypred_crf, ypred_boost, ypred_svm, ypred_logi, ypred_RandFor)\n",
    "\n",
    "index = pd.read_csv(\"sample.csv\")\n",
    "index['y'] = submission\n",
    "index.to_csv(\"hard_voter.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
