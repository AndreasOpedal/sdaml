{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import kurtosis,skew\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pyeeg\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import log, floor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor, GradientBoostingClassifier )\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "import yasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_eeg1 = pd.read_csv(\"train_eeg1.csv\").drop(\"Id\", axis = 1)\n",
    "xtrain_eeg2 = pd.read_csv(\"train_eeg2.csv\").drop(\"Id\", axis = 1)\n",
    "xtrain_emg = pd.read_csv(\"train_emg.csv\").drop(\"Id\", axis = 1)\n",
    "\n",
    "ytrain = pd.read_csv(\"train_labels.csv\").drop(\"Id\", axis = 1)\n",
    "\n",
    "xtest_eeg1 = pd.read_csv(\"test_eeg1.csv\").drop(\"Id\", axis = 1)\n",
    "xtest_eeg2 = pd.read_csv(\"test_eeg2.csv\").drop(\"Id\", axis = 1)\n",
    "xtest_emg = pd.read_csv(\"test_emg.csv\").drop(\"Id\", axis = 1)\n",
    "\n",
    "xtrain_eeg1 = xtrain_eeg1.values\n",
    "xtrain_eeg2 = xtrain_eeg2.values\n",
    "xtrain_emg = xtrain_emg.values\n",
    "\n",
    "xtest_eeg1 = xtest_eeg1.values \n",
    "xtest_eeg2 = xtest_eeg2.values\n",
    "xtest_emg = xtest_emg.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if (ypred_crf[i] != ypred_boost[i] and  \\nypred_crf[i] != ypred_svm[i] and\\nypred_svm[i] != ypred_boost[i]): \\n\\nrand = np.random.choice(np.arange(1, 4), p=[p_crf,p_boost,p_svm])\\n\\npredictions[i] = rand'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _log_n(min_n, max_n, factor):\n",
    "    \"\"\"\n",
    "    Creates a list of integer values by successively multiplying a minimum\n",
    "    \"\"\"\n",
    "    max_i = int(floor(log(1.0 * max_n / min_n) / log(factor)))\n",
    "    ns = [min_n]\n",
    "    for i in range(max_i + 1):\n",
    "        n = int(floor(min_n * (factor ** i)))\n",
    "        if n > ns[-1]:\n",
    "            ns.append(n)\n",
    "    return np.array(ns, dtype=np.int64)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def _linear_regression(x, y):\n",
    "    \"\"\"Fast linear regression using Numba.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : ndarray, shape (n_times,)\n",
    "        Variables\n",
    "    Returns\n",
    "    -------\n",
    "    slope : float\n",
    "        Slope of 1D least-square regression.\n",
    "    intercept : float\n",
    "        Intercept\n",
    "    \"\"\"\n",
    "    n_times = x.size\n",
    "    sx2 = 0\n",
    "    sx = 0\n",
    "    sy = 0\n",
    "    sxy = 0\n",
    "    for j in range(n_times):\n",
    "        sx2 += x[j] ** 2\n",
    "        sx += x[j]\n",
    "        sxy += x[j] * y[j]\n",
    "        sy += y[j]\n",
    "    den = n_times * sx2 - (sx ** 2)\n",
    "    num = n_times * sxy - sx * sy\n",
    "    slope = num / den\n",
    "    intercept = np.mean(y) - slope * np.mean(x)\n",
    "    return slope, intercept\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "\n",
    "def _dfa(x):\n",
    "    \"\"\"\n",
    "    Utility function for detrended fluctuation analysis\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    nvals = _log_n(4, 0.1 * N, 1.2)\n",
    "    walk = np.cumsum(x - x.mean())\n",
    "    fluctuations = np.zeros(len(nvals))\n",
    "\n",
    "    for i_n, n in enumerate(nvals):\n",
    "        d = np.reshape(walk[:N - (N % n)], (N // n, n))\n",
    "        ran_n = np.array([float(na) for na in range(n)])\n",
    "        d_len = len(d)\n",
    "        slope = np.empty(d_len)\n",
    "        intercept = np.empty(d_len)\n",
    "        trend = np.empty((d_len, ran_n.size))\n",
    "        for i in range(d_len):\n",
    "            slope[i], intercept[i] = _linear_regression(ran_n, d[i])\n",
    "            y = np.zeros_like(ran_n)\n",
    "            # Equivalent to np.polyval function\n",
    "            for p in [slope[i], intercept[i]]:\n",
    "                y = y * ran_n + p\n",
    "            trend[i, :] = y\n",
    "        # calculate standard deviation (fluctuation) of walks in d around trend\n",
    "        flucs = np.sqrt(np.sum((d - trend) ** 2, axis=1) / n)\n",
    "        # calculate mean fluctuation over all subsequences\n",
    "        fluctuations[i_n] = flucs.sum() / flucs.size\n",
    "\n",
    "    # Filter zero\n",
    "    nonzero = np.nonzero(fluctuations)[0]\n",
    "    fluctuations = fluctuations[nonzero]\n",
    "    nvals = nvals[nonzero]\n",
    "    if len(fluctuations) == 0:\n",
    "        # all fluctuations are zero => we cannot fit a line\n",
    "        dfa = np.nan\n",
    "    else:\n",
    "        dfa, _ = _linear_regression(np.log(nvals), np.log(fluctuations))\n",
    "    return dfa\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "def matrix_dfa(x):\n",
    "    dfa_ = np.zeros((x.shape[0],), dtype=np.float64)\n",
    "    for i in (np.arange(x.shape[0])):\n",
    "        dfa_[i] = _dfa(np.asarray(x[i], dtype=np.float64))\n",
    "    return np.reshape(dfa_, (-1, 1))\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def _embed(x, order=3, delay=1):\n",
    "    \"\"\" VECTORIZED!! Kind of TESTED. Time-delay embedding. x is an (nxd) matrix\n",
    "    \"\"\"\n",
    "    N = x.shape[1]\n",
    "    if order * delay > N:\n",
    "        raise ValueError(\"Error: order * delay should be lower than x.size\")\n",
    "    if delay < 1:\n",
    "        raise ValueError(\"Delay has to be at least 1.\")\n",
    "    if order < 2:\n",
    "        raise ValueError(\"Order has to be at least 2.\")\n",
    "    Y = np.zeros((x.shape[0], N - (order - 1) * delay, order))\n",
    "    for i in range(order):\n",
    "        Y[:, :, i] = x[:, i * delay:i * delay + Y.shape[1]]\n",
    "    return Y\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def hfd(X, Kmax):\n",
    "    \"\"\" VECTORIZED!!! TESTED: Matches the for loop output. Can test easily comparing\n",
    "    to the pyeeg.hfd() function. X now a (nxd) matrix.\n",
    "    Compute Higuchi Fractal Dimension of a time series X. kmax\n",
    "     is an HFD parameter\n",
    "    \"\"\"\n",
    "    L = []\n",
    "    x = []\n",
    "    N = X.shape[1]\n",
    "    for k in (range(1, Kmax)):\n",
    "        # Lk = np.empty(shape=[0, ])\n",
    "        Lk = np.empty(shape=[X.shape[0], 1])\n",
    "        for m in range(0, k):\n",
    "            Lmk = 0\n",
    "            for i in range(1, int(np.floor((N - m) / k))):\n",
    "                Lmk += np.abs(X[:, m + i * k] - X[:, m + i * k - k])\n",
    "            Lmk = Lmk * (N - 1) / np.floor((N - m) / float(k)) / k\n",
    "            Lmk = np.reshape(Lmk, (Lmk.shape[0], 1))\n",
    "            Lk = np.hstack((Lk, Lmk))\n",
    "\n",
    "        # Remove that first placeholder column of zeros in Lk:\n",
    "        Lk = Lk[:, 1:]\n",
    "        L.append(np.log(np.mean(Lk, axis=1)))\n",
    "        x.append([np.log(float(1) / k), 1]) # Fix this!!!\n",
    "\n",
    "    (p, _, _, _) = np.linalg.lstsq(x, L)\n",
    "    return p[0]\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def pfd(X):\n",
    "    \"\"\"VECTORIZED!!! TESTED, matches the 1d time series output. Now accepts (nxd) matrices as input\n",
    "    Compute Petrosian Fractal Dimension of a time series from either two\n",
    "    cases below:\n",
    "        1. X, the time series of type list (default)\n",
    "        2. D, the first order differential sequence of X (if D is provided,\n",
    "           recommended to speed up)\n",
    "    In case 1, D is computed using Numpy's difference function.\n",
    "    To speed up, it is recommended to compute D before calling this function\n",
    "    because D may also be used by other functions whereas computing it here\n",
    "    again will slow down.\n",
    "    \"\"\"\n",
    "    n = X.shape[1]\n",
    "    diff = np.diff(X, axis=1)\n",
    "    N_delta = np.sum(diff[:, 1:-1] * diff[:, 0:-2] < 0, axis=1)\n",
    "    return np.log10(n) / (np.log10(n) + np.log10(n / (n + 0.4 * N_delta)))\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def fisher_info(X, Tau=3, DE=1, W=None):\n",
    "    \"\"\" VECTORIZED, TESTED, gives approximate results but not exact. Compute SVD Entropy from either two cases below:\n",
    "    \"\"\"\n",
    "    if W is None:\n",
    "        Y = _embed(X, Tau, DE)\n",
    "        W = np.linalg.svd(Y, compute_uv=0)\n",
    "        W = np.divide(W, np.reshape(np.sum(W, axis=1), (-1, 1)))  # normalize singular values\n",
    "    return -1 * np.sum(W * np.log(W), axis=1)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def extract_bandpower_eeg(signal, frequency = 128):\n",
    "    for i in (np.arange(signal.shape[0] / 100) + 1):\n",
    "        if i == 1:\n",
    "            df = yasa.bandpower(signal[0:int(100*i),:], sf=frequency)\n",
    "        else:\n",
    "            df = df.append(yasa.bandpower(signal[int(100*(i-1)):int(100*i),:], sf=frequency))\n",
    "    \n",
    "    df = df.set_index(np.arange(signal.shape[0]))\n",
    "    df = df.drop(columns = [\"FreqRes\",\"Relative\"], axis = 1)\n",
    "    return df\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "'''input must be np.ndarray'''\n",
    "\n",
    "def vectorized_adv_stat(signal, fs=128):\n",
    "    K_boundary = 10         # to be tuned\n",
    "    t_fisher = 12          # to be tuned\n",
    "    d_fisher = 40          # to be tuned\n",
    "    features_num = 11\n",
    "    threshold =  0.0009\n",
    "    advanced_stats = np.zeros((signal.shape[0],features_num))\n",
    "    # Missing fisher info and dfa\n",
    "    feat_array = np.array([\n",
    "                           fisher_info(signal, t_fisher, d_fisher),\n",
    "                           pfd(signal),\n",
    "                           hfd(signal, K_boundary),\n",
    "                           np.sum((np.power(np.abs(signal),(-0.3)) > 20), axis=1),\n",
    "                           np.sum((np.abs(signal)) > threshold, axis=1),\n",
    "                           np.std(np.power(np.abs(signal),(0.05)), axis=1),\n",
    "                           np.sqrt(np.mean(np.power(np.diff(signal, axis=1), 2), axis=1)),\n",
    "                           np.mean(np.abs(np.diff(signal, axis=1)), axis=1),\n",
    "                           np.mean(np.power(signal, 5), axis=1),\n",
    "                           np.sum(np.power(signal, 2), axis=1)\n",
    "                           ]).T\n",
    "    #feat_array = np.concatenate((feat_array, matrix_dfa(signal)), axis=1) # Concatenate the lengthy dfa calculation\n",
    "\n",
    "    return feat_array\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def simple_stats(signal, fs = 128):\n",
    "           \n",
    "    if (len(signal.shape) > 1) and (signal.shape[1]!=1):\n",
    "        simple_stats = np.array([np.mean(signal, axis=1), \n",
    "                        np.median(signal, axis=1),\n",
    "                        np.std(signal, axis=1), \n",
    "                        np.max(signal, axis=1),\n",
    "                        np.min(signal, axis=1), \n",
    "                        kurtosis(signal, axis=1),\n",
    "                        skew(signal, axis=1), \n",
    "                        np.sum(np.abs(signal), axis = 1)]).T\n",
    "                        \n",
    "    else:\n",
    "        print(\"Not Tested with this input!\")\n",
    "        simple_stats =  np.array([np.mean(signal), \n",
    "                        np.median(signal), \n",
    "                        np.std(sigal),\n",
    "                        np.max(signal), \n",
    "                        np.min(signal), \n",
    "                        float(kurtosis(signal)),\n",
    "                        float(skew(signal))])\n",
    "        \n",
    "    \n",
    "\n",
    "    return (simple_stats)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def EEG_feature_extraction(signal, fs = 128): \n",
    "    eeg_features = np.concatenate((extract_bandpower_eeg(signal, frequency = 128), vectorized_adv_stat(signal, fs=128),\n",
    "                                   simple_stats(signal, fs = 128)), axis = 1)\n",
    "    \n",
    "    return(eeg_features)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def EMG_feature_extraction(signal, fs = 128):\n",
    "    \n",
    "    features_num_emg = 6\n",
    "    \n",
    "    if (len(signal.shape) > 1) and (signal.shape[1]!=1):\n",
    "        simple_stats = np.array([np.mean(signal, axis=1), \n",
    "                        np.median(signal, axis=1),\n",
    "                        np.std(signal, axis=1), \n",
    "                        np.max(signal, axis=1),\n",
    "                        np.min(signal, axis=1), \n",
    "                        kurtosis(signal, axis=1),\n",
    "                        skew(signal, axis=1), \n",
    "                        pd.Series(np.sum(np.abs(signal), axis = 1))]).T\n",
    "                        \n",
    "    else:\n",
    "        print(\"Not Tested with this input!\")\n",
    "        simple_stats =  np.array([np.mean(signal), \n",
    "                        np.median(signal), \n",
    "                        np.std(sigal),\n",
    "                        np.max(signal), \n",
    "                        np.min(signal), \n",
    "                        float(kurtosis(signal)),\n",
    "                        float(skew(signal))])\n",
    "\n",
    "    advanced_stats = np.zeros((signal.shape[0],features_num_emg))\n",
    "    for i in tqdm((np.arange(signal.shape[0]))):\n",
    "        feat_array = np.array([\n",
    "                              np.median(signal[i,:] ** 2), \n",
    "                              np.median(np.abs(np.diff(signal[i,:]))), \n",
    "                              np.std(np.abs(np.diff(signal[i,:]))), \n",
    "                              np.sum(np.abs(np.diff(signal[i,:]))), \n",
    "                              np.mean(np.power(np.diff(signal[i,:]), 2)),\n",
    "                              np.std(abs(signal[i,:]))\n",
    "                            ])\n",
    "\n",
    "        advanced_stats[i, :] = feat_array \n",
    "        \n",
    "        union_smpl_adv = np.concatenate((simple_stats, advanced_stats), axis = 1)\n",
    "\n",
    "    return(union_smpl_adv)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "'''change for submissions predictions'''\n",
    "\n",
    "def hard_voter(ypred_boost, ypred_svm, \n",
    "               ypred_logi, ypred_RandFor, ypred_KNN, ypred_SGD):  #ypred_crf  -- add p_boost, p_crf, p_svm, p_logi,p_RandFor\n",
    "    \n",
    "    predictions = np.zeros(xtest_subj3.shape[0])   ##  \n",
    "    \n",
    "    for i in range(xtest_subj3.shape[0]):\n",
    "\n",
    "        counts = np.bincount(np.array([ypred_boost[i],ypred_svm[i], #ypred_crf[i]\n",
    "                    ypred_logi[i], ypred_RandFor[i], ypred_KNN[i], ypred_SGD[i]]))\n",
    "        predictions[i] = np.argmax(counts)\n",
    "        \n",
    "    return(np.asarray(predictions, dtype=np.int32)) \n",
    "\n",
    "\n",
    "'''if (ypred_crf[i] != ypred_boost[i] and  \n",
    "ypred_crf[i] != ypred_svm[i] and\n",
    "ypred_svm[i] != ypred_boost[i]): \n",
    "\n",
    "rand = np.random.choice(np.arange(1, 4), p=[p_crf,p_boost,p_svm])\n",
    "\n",
    "predictions[i] = rand'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features --- Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfffda2963804bd98af438547504d5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=64800), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec057b0a9bfa44e891d7bb998fcd184b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(64800, 60) (43200, 60)\n"
     ]
    }
   ],
   "source": [
    "xtrain_eeg1_processed = EEG_feature_extraction(xtrain_eeg1, 128)\n",
    "xtrain_eeg2_processed = EEG_feature_extraction(xtrain_eeg2, 128)\n",
    "xtrain_emg_processed = EMG_feature_extraction(xtrain_emg, 128)\n",
    "xtrain = np.concatenate((xtrain_eeg1_processed, \n",
    "                         xtrain_eeg2_processed, \n",
    "                         xtrain_emg_processed), \n",
    "                         axis = 1)\n",
    "\n",
    "xtest_eeg1_processed = EEG_feature_extraction(xtest_eeg1, 128)\n",
    "xtest_eeg2_processed = EEG_feature_extraction(xtest_eeg2, 128)\n",
    "xtest_emg_processed = EMG_feature_extraction(xtest_emg, 128)\n",
    "xtest = np.concatenate((xtest_eeg1_processed, \n",
    "                         xtest_eeg2_processed, \n",
    "                         xtest_emg_processed), \n",
    "                         axis = 1)\n",
    "\n",
    "print(xtrain.shape, xtest.shape)\n",
    "\n",
    "xtrain_grid = pd.DataFrame(xtrain)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "xtrain_rescaled = scaler.fit_transform(xtrain_grid)\n",
    "xtest_rescaled = scaler.fit_transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS SVM  --- OPT DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] classifier__C=0.01, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.01, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total= 1.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] classifier__C=0.01, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.01, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total= 1.1min\n",
      "[CV] classifier__C=0.01, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.01, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total= 1.1min\n",
      "[CV] classifier__C=0.1, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.1, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  26.4s\n",
      "[CV] classifier__C=0.1, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.1, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  37.4s\n",
      "[CV] classifier__C=0.1, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.1, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  37.3s\n",
      "[CV] classifier__C=0.3, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.3, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  22.0s\n",
      "[CV] classifier__C=0.3, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.3, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  32.1s\n",
      "[CV] classifier__C=0.3, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.3, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  31.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  6.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9200190564541171\n",
      "{'classifier__C': 0.1, 'classifier__class_weight': 'balanced', 'classifier__gamma': 'auto', 'classifier__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "xtrain_grid = pd.DataFrame(xtrain)\n",
    "\n",
    "steps = [(\"scaler\", StandardScaler()), (\"classifier\", SVC())]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__kernel\": [\"rbf\"],\n",
    "              \"classifier__gamma\": [\"auto\"],\n",
    "              \"classifier__C\": [0.01, 0.1,0.3],  \n",
    "              \"classifier__class_weight\": [\"balanced\"]\n",
    "             }\n",
    "grid_svm = GridSearchCV(pipeline, parameters, cv = 3, scoring = 'balanced_accuracy', verbose = 2)\n",
    "\n",
    "grid_svm.fit(xtrain_grid, ytrain.values.ravel())\n",
    "print(grid_svm.best_score_)\n",
    "print(grid_svm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS Gradient Boosting Method (very slow search)  --- MANUAL OPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  5.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8374339081162436\n",
      "{'classifier__learning_rate': 0.2, 'classifier__max_depth': 20, 'classifier__max_features': 20, 'classifier__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Gradient Boosting APPROACH -- GRID-SEARCH CV\n",
    "''' \n",
    "xtrain_grid = pd.DataFrame(xtrain)\n",
    "\n",
    "\n",
    "steps = [(\"scaler\", preprocessing.StandardScaler()), \n",
    "         (\"classifier\", GradientBoostingClassifier())]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__max_depth\": [20],\n",
    "              \"classifier__n_estimators\": [100],\n",
    "              \"classifier__learning_rate\": [0.2],\n",
    "              \"classifier__max_features\": [20]\n",
    "             }\n",
    "\n",
    "grid_boost = GridSearchCV(pipeline, parameters, cv = 3,scoring = 'balanced_accuracy', verbose = 1)\n",
    "grid_boost.fit(xtrain_grid, ytrain.values.ravel())\n",
    "\n",
    "print(grid_boost.best_score_)\n",
    "print(grid_boost.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS Logistic  --- OPT DONE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9314424377687452\n",
      "{'classifier__C': 0.05, 'classifier__class_weight': 'balanced', 'classifier__max_iter': 15, 'classifier__multi_class': 'auto', 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'classifier__verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "steps = [(\"scaler\", StandardScaler()), (\"classifier\", LogisticRegression(fit_intercept=True))]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__multi_class\": [\"auto\"],\n",
    "              \"classifier__C\": [0.02,0.05,0.07,0.01],  \n",
    "              \"classifier__class_weight\": [\"balanced\"], \n",
    "              \"classifier__max_iter\": [15,20,25],\n",
    "              \"classifier__verbose\": [0],\n",
    "              \"classifier__penalty\": ['l2'],\n",
    "              \"classifier__solver\": ['liblinear']\n",
    "             }\n",
    "grid_logi = GridSearchCV(pipeline, parameters, cv = 3, scoring = 'balanced_accuracy', verbose = 1)\n",
    "\n",
    "grid_logi.fit(xtrain_grid, ytrain.values.ravel())\n",
    "print(grid_logi.best_score_)\n",
    "print(grid_logi.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS Random Forest  --- OPT DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed: 11.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.893864379402196\n",
      "{'classifier__class_weight': 'balanced', 'classifier__max_depth': 5, 'classifier__min_samples_leaf': 100, 'classifier__min_samples_split': 200, 'classifier__n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "steps = [(\"scaler\", StandardScaler()), (\"classifier\", RandomForestClassifier(criterion='entropy', max_features='auto'))]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__n_estimators\": [20,30,40,60,100],\n",
    "              \"classifier__max_depth\": [2,3,5,8], \n",
    "              \"classifier__class_weight\": [\"balanced\"], \n",
    "              \"classifier__min_samples_split\":[100,200], \n",
    "              \"classifier__min_samples_leaf\": [100]\n",
    "             }\n",
    "grid_RandFor = GridSearchCV(pipeline, parameters, cv = 3, scoring = 'balanced_accuracy', verbose = 1)\n",
    "\n",
    "grid_RandFor.fit(xtrain_grid, ytrain.values.ravel())\n",
    "print(grid_RandFor.best_score_)\n",
    "print(grid_RandFor.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS KNN (Very slow search)  --- OPT DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 15.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9065325243946395\n",
      "{'classifier__leaf_size': 200, 'classifier__n_neighbors': 20, 'classifier__weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "xtrain_grid = pd.DataFrame(xtrain)\n",
    "\n",
    "steps = [(\"scaler\", preprocessing.StandardScaler()), \n",
    "         (\"classifier\", KNeighborsClassifier())]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__n_neighbors\": [5,10,20],\n",
    "              \"classifier__weights\": ['uniform'],\n",
    "              \"classifier__leaf_size\": [200]\n",
    "             }\n",
    "\n",
    "grid_KNN = GridSearchCV(pipeline, parameters, cv = 3, scoring = 'balanced_accuracy', verbose = 1)\n",
    "grid_KNN.fit(xtrain_grid, ytrain.values.ravel())\n",
    "\n",
    "print(grid_KNN.best_score_)\n",
    "print(grid_KNN.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS SGD Classifier --- OPT DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8794014124318326\n",
      "{'classifier__alpha': 0.0001, 'classifier__learning_rate': 'optimal', 'classifier__loss': 'hinge', 'classifier__max_iter': 1000, 'classifier__penalty': 'l1', 'classifier__power_t': 2}\n"
     ]
    }
   ],
   "source": [
    "xtrain_grid = pd.DataFrame(xtrain)\n",
    "\n",
    "steps = [(\"scaler\", preprocessing.StandardScaler()), \n",
    "         (\"classifier\", SGDClassifier(shuffle = False, fit_intercept=True))]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__penalty\": ['l1'],\n",
    "              \"classifier__alpha\": [0.0001],\n",
    "              \"classifier__max_iter\": [1000],\n",
    "              \"classifier__loss\": ['hinge'],\n",
    "              \"classifier__learning_rate\": ['optimal'],\n",
    "              \"classifier__power_t\":[2]\n",
    "              }\n",
    "\n",
    "grid_SGD = GridSearchCV(pipeline, parameters, cv = 3, scoring = 'balanced_accuracy', verbose = 1)\n",
    "grid_SGD.fit(xtrain_grid, ytrain.values.ravel())\n",
    "\n",
    "print(grid_SGD.best_score_)\n",
    "print(grid_SGD.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Validation --- train on subj 1,2 --- test on 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3c204f687c411ab850fd2dd0f12084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b335e7db48f40c2a32553b39d771417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21600), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''extracting features'''\n",
    "\n",
    "subj12_eeg1 = xtrain_eeg1[0:43200,:]\n",
    "subj12_eeg2 = xtrain_eeg2[0:43200,:]\n",
    "subj12_emg = xtrain_emg[0:43200,:]\n",
    "ytrain_subj12 = np.reshape(ytrain.values, (-1,))[0:43200]\n",
    "\n",
    "xtrain_subj12_eeg_1 = EEG_feature_extraction(subj12_eeg1, 128)  \n",
    "xtrain_subj12_eeg_2 = EEG_feature_extraction(subj12_eeg2, 128)\n",
    "xtrain_subj12_emg = EMG_feature_extraction(subj12_emg, 128)\n",
    "xtrain_subj12 = np.concatenate((xtrain_subj12_eeg_1, xtrain_subj12_eeg_2, xtrain_subj12_emg), axis = 1)\n",
    "\n",
    "\n",
    "'''creating validation data'''\n",
    "\n",
    "subj3_eeg1 = xtrain_eeg1[43200:64800,:]\n",
    "subj3_eeg2 = xtrain_eeg2[43200:64800,:]\n",
    "subj3_emg = xtrain_emg[43200:64800,:]\n",
    "ytest_subj3 = np.reshape(ytrain.values, (-1,))[43200:64800]\n",
    "\n",
    "xtest_subj3_eeg_1 = EEG_feature_extraction(subj3_eeg1, 128)\n",
    "xtest_subj3_eeg_2 = EEG_feature_extraction(subj3_eeg2, 128)\n",
    "xtest_subj3_emg = EMG_feature_extraction(subj3_emg, 128)\n",
    "xtest_subj3 = np.concatenate((xtest_subj3_eeg_1, xtest_subj3_eeg_2, xtest_subj3_emg), axis = 1)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "xtrain_subj12_rescaled = scaler.fit_transform(xtrain_subj12)\n",
    "xtest_subj3_rescaled = scaler.fit_transform(xtest_subj3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nclf_boost = GradientBoostingClassifier(n_estimators = 200, max_depth = 8, \\n                                       learning_rate = 0.1, max_features = 20)\\nclf_boost.fit(xtrain_subj12, ytrain_subj12)\\nypred_boost = clf_boost.predict(xtest_subj3)\\nprint(\\'Gradient Boosting done\\')\\n\\n\\nclf_logi = LogisticRegression(random_state=10, C = 0.05, class_weight = \\'balanced\\', max_iter = 15, \\n                              multi_class=\\'auto\\', penalty = \\'l2\\', solver = \\'liblinear\\')\\nclf_logi.fit(xtrain_subj12, ytrain_subj12)\\nypred_logi = clf_logi.predict(xtest_subj3)\\nprint(\\'Multinomial Logistic done\\')\\n\\n\\nclf_RandFor = RandomForestClassifier(max_depth=5, random_state=1, class_weight = \\'balanced\\', \\n                         min_samples_leaf = 100, min_samples_split = 200, n_estimators = 20)\\nclf_RandFor.fit(xtrain_subj12, ytrain_subj12)\\nypred_RandFor = clf_RandFor.predict(xtest_subj3)\\nprint(\\'Random Forest Done\\')\\n\\n\\nclf_KNN = KNeighborsClassifier(leaf_size = 200, n_neighbors = 20, weights = \\'uniform\\')\\nclf_KNN.fit(xtrain_subj12, ytrain_subj12)\\nypred_KNN = clf_KNN.predict(xtest_subj3)\\nprint(\\'KNN done\\')\\n\\n\\nclf_SGD = SGDClassifier(alpha = 0.0001, learning_rate = \\'optimal\\', loss = \\'hinge\\',\\n                       max_iter = 1000, penalty = \\'l1\\', power_t = 2, shuffle = False)\\nclf_SGD.fit(xtrain_subj12, ytrain_subj12)\\nypred_SGD = clf_SGD.predict(xtest_subj3)\\nprint(\\'SGD done\\')\\n\\n\\n## ypred_crf = pd.read_csv(\"ypred_crf.csv\").drop(\"Id\", axis = 1)\\n## ypred_crf = np.reshape(ypred_crf.values, (-1,))[43199:64799]\\n## print(\\'CRF Done\\')\\n\\n\\n\\ny_val = hard_voter(ypred_boost, ypred_svm, ypred_logi, ypred_RandFor, ypred_KNN, ypred_SGD)  #ypred_crf,\\nBMAC = balanced_accuracy_score(ytest_subj3, y_val)\\n\\nprint(BMAC)\\n\\n'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''SVM\n",
    "clf_svm = svm.SVC(kernel='rbf', C = 0.1, gamma = 'auto', class_weight = 'balanced')\n",
    "clf_svm.fit(xtrain_subj12, ytrain_subj12)\n",
    "ypred_svm = clf_svm.predict(xtest_subj3)\n",
    "print('SVM done')\n",
    "'''\n",
    "\n",
    "clf_SGD = SGDClassifier(alpha = 0.0001, learning_rate = 'optimal', loss = 'hinge',\n",
    "                       max_iter = 1000, penalty = 'l1', power_t = 2, shuffle = False)\n",
    "clf_SGD.fit(xtrain_subj12, ytrain_subj12)\n",
    "ypred_SGD = clf_SGD.predict(xtest_subj3)\n",
    "print('SGD done')\n",
    "\n",
    "\n",
    "'''\n",
    "clf_boost = GradientBoostingClassifier(n_estimators = 200, max_depth = 8, \n",
    "                                       learning_rate = 0.1, max_features = 20)\n",
    "clf_boost.fit(xtrain_subj12, ytrain_subj12)\n",
    "ypred_boost = clf_boost.predict(xtest_subj3)\n",
    "print('Gradient Boosting done')\n",
    "\n",
    "\n",
    "clf_logi = LogisticRegression(random_state=10, C = 0.05, class_weight = 'balanced', max_iter = 15, \n",
    "                              multi_class='auto', penalty = 'l2', solver = 'liblinear')\n",
    "clf_logi.fit(xtrain_subj12, ytrain_subj12)\n",
    "ypred_logi = clf_logi.predict(xtest_subj3)\n",
    "print('Multinomial Logistic done')\n",
    "\n",
    "\n",
    "clf_RandFor = RandomForestClassifier(max_depth=5, random_state=1, class_weight = 'balanced', \n",
    "                         min_samples_leaf = 100, min_samples_split = 200, n_estimators = 20)\n",
    "clf_RandFor.fit(xtrain_subj12, ytrain_subj12)\n",
    "ypred_RandFor = clf_RandFor.predict(xtest_subj3)\n",
    "print('Random Forest Done')\n",
    "\n",
    "\n",
    "clf_KNN = KNeighborsClassifier(leaf_size = 200, n_neighbors = 20, weights = 'uniform')\n",
    "clf_KNN.fit(xtrain_subj12, ytrain_subj12)\n",
    "ypred_KNN = clf_KNN.predict(xtest_subj3)\n",
    "print('KNN done')\n",
    "\n",
    "\n",
    "clf_SGD = SGDClassifier(alpha = 0.0001, learning_rate = 'optimal', loss = 'hinge',\n",
    "                       max_iter = 1000, penalty = 'l1', power_t = 2, shuffle = False)\n",
    "clf_SGD.fit(xtrain_subj12, ytrain_subj12)\n",
    "ypred_SGD = clf_SGD.predict(xtest_subj3)\n",
    "print('SGD done')\n",
    "\n",
    "\n",
    "## ypred_crf = pd.read_csv(\"ypred_crf.csv\").drop(\"Id\", axis = 1)\n",
    "## ypred_crf = np.reshape(ypred_crf.values, (-1,))[43199:64799]\n",
    "## print('CRF Done')\n",
    "\n",
    "\n",
    "\n",
    "y_val = hard_voter(ypred_boost, ypred_svm, ypred_logi, ypred_RandFor, ypred_KNN, ypred_SGD)  #ypred_crf,\n",
    "BMAC = balanced_accuracy_score(ytest_subj3, y_val)\n",
    "\n",
    "print(BMAC)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5559904764593745\n"
     ]
    }
   ],
   "source": [
    "print(balanced_accuracy_score(ytest_subj3, ypred_SGD))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM done\n",
      "Gradient Boosting done\n",
      "Multinomial Logistic done\n",
      "Random Forest Done\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_neighbours'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-199-855f8399d81e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;34m'''KNN'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mclf_KNN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleaf_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbours\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'uniform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mclf_KNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain_rescaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mypred_KNN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_KNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtest_rescaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\classification.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_neighbors, weights, algorithm, leaf_size, p, metric, metric_params, n_jobs, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[0mleaf_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleaf_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mmetric_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             n_jobs=n_jobs, **kwargs)\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_neighbours'"
     ]
    }
   ],
   "source": [
    "'''SVM'''\n",
    "clf_svm = svm.SVC(kernel='rbf', C = 0.1, gamma = 'auto', class_weight = 'balanced')\n",
    "clf_svm.fit(xtrain_rescaled, ytrain)\n",
    "ypred_svm = clf_svm.predict(xtest_rescaled)\n",
    "print('SVM done')\n",
    "\n",
    "\n",
    "'''Gradient Boosting'''\n",
    "clf_boost = GradientBoostingClassifier(n_estimators = 200, max_depth = 8, learning_rate = 0.1, max_features = 20)\n",
    "clf_boost.fit(xtrain_rescaled, ytrain)\n",
    "ypred_boost = clf_boost.predict(xtest_rescaled)\n",
    "print('Gradient Boosting done')\n",
    "\n",
    "\n",
    "'''Multinomial Logistic'''\n",
    "clf_logi = LogisticRegression(random_state=10, C = 0.05, class_weight = 'balanced', max_iter = 15, \n",
    "                              multi_class='auto', penalty = 'l2', solver = 'liblinear')\n",
    "clf_logi.fit(xtrain_rescaled, ytrain)\n",
    "ypred_logi = clf_logi.predict(xtest_rescaled)\n",
    "print('Multinomial Logistic done')\n",
    "\n",
    "\n",
    "'''Random Forest'''\n",
    "clf_RandFor = RandomForestClassifier(max_depth=5, random_state=1, class_weight = 'balanced', \n",
    "                         min_samples_leaf = 100, min_samples_split = 200, n_estimators = 20)\n",
    "clf_RandFor.fit(xtrain_rescaled, ytrain)\n",
    "ypred_RandFor = clf_RandFor.predict(xtest_rescaled)\n",
    "print('Random Forest Done')\n",
    "\n",
    "\n",
    "'''KNN'''\n",
    "clf_KNN = KNeighborsClassifier(leaf_size = 200, n_neighbors = 20, weights = 'uniform')\n",
    "clf_KNN.fit(xtrain_rescaled, ytrain)\n",
    "ypred_KNN = clf_KNN.predict(xtest_rescaled)\n",
    "print('KNN done')\n",
    "\n",
    "\n",
    "'''SGD'''\n",
    "clf_SGD = SGDClassifier(alpha = 0.0001, learning_rate = 'optimal', loss = 'hinge',\n",
    "                       max_iter = 1000, penalty = 'l1', power_t = 2)\n",
    "clf_SGD.fit(xtrain_rescaled, ytrain)\n",
    "ypred_SGD = clf_SGD.predict(xtest_rescaled)\n",
    "print('SGD done')\n",
    "\n",
    "\n",
    "'''CRF'''\n",
    "ypred_crf = pd.read_csv(\"ypred_crf.csv\").drop(\"Id\", axis = 1)\n",
    "ypred_crf = np.reshape(ypred_crf.values, (-1,))\n",
    "print('CRF Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Classifier -- Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = hard_voter(ypred_crf, ypred_boost, ypred_svm, ypred_logi, ypred_RandFor, ypred_KNN, ypred_SGD)\n",
    "\n",
    "index = pd.read_csv(\"sample.csv\")\n",
    "index['y'] = submission\n",
    "index.to_csv(\"hard_voter.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
