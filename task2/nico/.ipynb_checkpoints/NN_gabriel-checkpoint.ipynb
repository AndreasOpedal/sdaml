{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all necessary preprocessing, calling prepro.py\n",
    "import utils\n",
    "from utils import *\n",
    "importlib.reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X, X_test_original, y = load_data() \n",
    "y = y.ravel()\n",
    "scores = np.array([])\n",
    "xtrain = X  # For andreas cross validation\n",
    "ytrain = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN1():\n",
    "    def __init__(self):\n",
    "        self.mlp = KerasClassifier(build_fn=self.create_model, \n",
    "                                   epochs=10, batch_size=35, \n",
    "                                   verbose=1)\n",
    "        return\n",
    "    \n",
    "    def score(self, X_, y_):\n",
    "        pred = self.predict(X_)\n",
    "        if y_.shape[1] < 2:\n",
    "            y_normal = y_\n",
    "        else:\n",
    "            y_normal = np.argmax(y_, axis=1)\n",
    "        BMAC = balanced_accuracy_score(y_normal, pred)\n",
    "        return BMAC\n",
    "    \n",
    "    def fit(self, X_, y_):\n",
    "        # One hot encode data\n",
    "        y_enc = np.zeros((y_.shape[0], 3))\n",
    "        y_enc[np.arange(y_.shape[0]), y_] = 1\n",
    "        \n",
    "        xscaled = preprocessing.StandardScaler().fit_transform(X_)\n",
    "        cw = sklearn.utils.class_weight.compute_class_weight('balanced', np.unique(y_), y_)\n",
    "        class_weight = {0: cw[0], 1: cw[1], 2: cw[2]}\n",
    "        self.mlp.fit(xscaled, y_enc, class_weight=class_weight)\n",
    "        return\n",
    "    \n",
    "    def predict(self, X_):\n",
    "        return self.mlp.predict(preprocessing.StandardScaler().fit_transform(X_))\n",
    "    \n",
    "    def create_model(self):\n",
    "        # create model\n",
    "        neurons = 1500\n",
    "        dropout_rate = 0.6\n",
    "        weight_constraint = 4\n",
    "        model = Sequential()\n",
    "        model.add(Dense(neurons, input_dim=1000, activation='relu', kernel_constraint=maxnorm(weight_constraint)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(neurons, activation='relu', kernel_constraint=maxnorm(weight_constraint)))\n",
    "        model.add(Dropout(dropout_rate*0.4))\n",
    "        model.add(Dense(int(neurons/2), activation='relu', kernel_constraint=maxnorm(weight_constraint)))\n",
    "        model.add(Dropout(dropout_rate*0.2))\n",
    "        model.add(Dense(3, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def predict_proba(self, X_):\n",
    "        return self.mlp.predict_proba(preprocessing.StandardScaler().fit_transform(X_))\n",
    "\n",
    "# nn = NN1()\n",
    "# nn.fit(X[1:10], y[1:10])\n",
    "# ypredy = nn.predict(X[1:10])\n",
    "# probbb = nn.predict_proba(X[1:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ensemble voting classifier fn:\n",
    "\n",
    "def ensemby(y1_prob, y2_prob, voting='soft'):\n",
    "    # Return the argmax of the sum of the probabilities\n",
    "    return np.argmax(y1_prob + y2_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting SVM...\n",
      "Fitting NN...\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Epoch 1/10\n",
      "3840/3840 [==============================] - 13s 3ms/step - loss: 0.5675 - accuracy: 0.7623\n",
      "Epoch 2/10\n",
      "3840/3840 [==============================] - 12s 3ms/step - loss: 0.4602 - accuracy: 0.7907\n",
      "Epoch 3/10\n",
      "3840/3840 [==============================] - 12s 3ms/step - loss: 0.4063 - accuracy: 0.8165\n",
      "Epoch 4/10\n",
      "3840/3840 [==============================] - 12s 3ms/step - loss: 0.3813 - accuracy: 0.8277\n",
      "Epoch 5/10\n",
      "3840/3840 [==============================] - 12s 3ms/step - loss: 0.3420 - accuracy: 0.8431\n",
      "Epoch 6/10\n",
      "3840/3840 [==============================] - 12s 3ms/step - loss: 0.3138 - accuracy: 0.8573\n",
      "Epoch 7/10\n",
      "3840/3840 [==============================] - 13s 3ms/step - loss: 0.3150 - accuracy: 0.8650\n",
      "Epoch 8/10\n",
      "3840/3840 [==============================] - 12s 3ms/step - loss: 0.2801 - accuracy: 0.8786\n",
      "Epoch 9/10\n",
      "3840/3840 [==============================] - 12s 3ms/step - loss: 0.2672 - accuracy: 0.8799\n",
      "Epoch 10/10\n",
      "3840/3840 [==============================] - 12s 3ms/step - loss: 0.2601 - accuracy: 0.8793\n",
      "960/960 [==============================] - 0s 223us/step\n",
      "960/960 [==============================] - 0s 222us/step\n",
      "BMAC Ensemble Scores:  0.6619954180919856\n",
      "BMAC SVM Scores:  0.6884410784288594\n",
      "BMAC NN Scores:  0.6801539581855797\n",
      "Fitting SVM...\n",
      "Fitting NN...\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Epoch 1/10\n",
      "3840/3840 [==============================] - 12s 3ms/step - loss: 0.5551 - accuracy: 0.7628\n",
      "Epoch 2/10\n",
      "3840/3840 [==============================] - 12s 3ms/step - loss: 0.4612 - accuracy: 0.7885\n",
      "Epoch 3/10\n",
      "3840/3840 [==============================] - 13s 3ms/step - loss: 0.4016 - accuracy: 0.8102\n",
      "Epoch 4/10\n",
      "3840/3840 [==============================] - 12s 3ms/step - loss: 0.3987 - accuracy: 0.8163\n",
      "Epoch 5/10\n",
      "3840/3840 [==============================] - 11s 3ms/step - loss: 0.3523 - accuracy: 0.8314\n",
      "Epoch 6/10\n",
      "3840/3840 [==============================] - 11s 3ms/step - loss: 0.3198 - accuracy: 0.8517\n",
      "Epoch 7/10\n",
      "3840/3840 [==============================] - 11s 3ms/step - loss: 0.3159 - accuracy: 0.8522\n",
      "Epoch 8/10\n",
      "3840/3840 [==============================] - 13s 3ms/step - loss: 0.2862 - accuracy: 0.8733\n",
      "Epoch 9/10\n",
      "3840/3840 [==============================] - 12s 3ms/step - loss: 0.2672 - accuracy: 0.8761\n",
      "Epoch 10/10\n",
      "3840/3840 [==============================] - 12s 3ms/step - loss: 0.2570 - accuracy: 0.8823\n",
      "960/960 [==============================] - 0s 206us/step\n",
      "960/960 [==============================] - 0s 188us/step\n",
      "BMAC Ensemble Scores:  0.6688982156379467\n",
      "BMAC SVM Scores:  0.7181605302115526\n",
      "BMAC NN Scores:  0.6898396666539971\n",
      "Fitting SVM...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7ed04f280fae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Models to fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fitting SVM...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0msvm_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fitting NN...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    354\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[0;32m    355\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    266\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Big cross val for loop\n",
    "\n",
    "# Try a nn Estimator with SMOTE\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "\n",
    "BMAC_means = np.array([])\n",
    "BMAC_stds = np.array([])\n",
    "BMAC_scores = np.array([])\n",
    "svm_scores = np.array([])\n",
    "nn_scores = np.array([])\n",
    "for train_index, test_index in kf.split(xtrain):\n",
    "\n",
    "    # Prepare the data\n",
    "    x_train = xtrain[train_index]\n",
    "    x_test = xtrain[test_index]\n",
    "    y_train = ytrain[train_index]\n",
    "    y_test = ytrain[test_index]\n",
    "\n",
    "    # Prepare the SVM\n",
    "    andreas_svm = svm.SVC(C=0.5, class_weight='balanced', degree=1, gamma='auto', kernel='rbf', probability=True)\n",
    "    steps = [(\"scaler\", preprocessing.StandardScaler()), (\"classifier\", andreas_svm)]\n",
    "    svm_pipeline = Pipeline(steps = steps)\n",
    "    \n",
    "    # Prepare the NN1\n",
    "    nn = NN1()\n",
    "    \n",
    "    # Models to fit\n",
    "    print(\"Fitting SVM...\")\n",
    "    svm_pipeline.fit(x_train, y_train.ravel())\n",
    "    print(\"Fitting NN...\")\n",
    "    nn.fit(x_train, y_train.ravel())\n",
    "    \n",
    "    # Predict and join the predictions\n",
    "    svm_pred = svm_pipeline.predict(x_test)\n",
    "    nn_pred = nn.predict(x_test)\n",
    "    svm_prob = svm_pipeline.predict_proba(x_test)\n",
    "    nn_prob = nn.predict_proba(x_test)\n",
    "    ensemble_pred = ensemby(svm_prob, nn_prob)\n",
    "    \n",
    "    # Record scores\n",
    "    BMAC_ensemble = balanced_accuracy_score(y_test, ensemble_pred)\n",
    "    BMAC_svm = balanced_accuracy_score(y_test, svm_pred)\n",
    "    BMAC_nn = balanced_accuracy_score(y_test, nn_pred)\n",
    "    print(\"BMAC Ensemble Scores: \", BMAC_ensemble)\n",
    "    print(\"BMAC SVM Scores: \", BMAC_svm)\n",
    "    print(\"BMAC NN Scores: \", BMAC_nn)\n",
    "    BMAC_scores = np.append(BMAC_scores, BMAC_ensemble)\n",
    "    svm_scores = np.append(svm_scores, BMAC_svm)\n",
    "    nn_scores = np.append(nn_scores, BMAC_nn)\n",
    "    \n",
    "BMAC_means = np.append(BMAC_means, np.mean(BMAC_scores))\n",
    "BMAC_stds = np.append(BMAC_stds, np.std(BMAC_scores))\n",
    "\n",
    "print(\"Scores:\", BMAC_scores)\n",
    "print(\"SVM Scores:\", svm_scores)\n",
    "print(\"NN Scores:\", nn_scores)\n",
    "print(\"Mean Scores:\", BMAC_means)\n",
    "print(\"Std Scores:\", BMAC_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
