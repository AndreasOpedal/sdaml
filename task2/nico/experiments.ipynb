{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import ensemble\n",
    "from sklearn import neural_network\n",
    "from sklearn import svm\n",
    "from sklearn import kernel_ridge\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.feature_selection import RFECV\n",
    "import csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.utils import resample\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions for loading data and producing final CSV \n",
    "\n",
    "'''\n",
    "eliminate highly correlated features\n",
    "'''\n",
    "def to_be_eliminated(df):\n",
    "    # Create correlation matrix\n",
    "    corr_matrix = df.corr().abs()\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    # Find features with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "    return to_drop\n",
    "\n",
    "'''\n",
    "loading training and test datasets\n",
    "'''\n",
    "def load_data():\n",
    "    X_train = pd.read_csv(\"X_train.csv\")\n",
    "    X_test = pd.read_csv(\"X_test.csv\")\n",
    "    y_train = pd.read_csv(\"y_train.csv\")\n",
    "     \n",
    "    #dropping id column\n",
    "    X_train = X_train.drop('id', axis = 1)\n",
    "    X_test = X_test.drop('id', axis = 1)\n",
    "    y_train = y_train.drop('id', axis = 1)\n",
    "   \n",
    "    #reshuffling data \n",
    "    X_train['y'] = y_train\n",
    "    X_train = X_train.sample(frac=1).reset_index(drop=True)\n",
    "    y_train = X_train['y']\n",
    "    X_train = X_train.drop('y', axis = 1)\n",
    "    \n",
    "    to_drop = to_be_eliminated(X_train)\n",
    "    \n",
    "    for i in range(len(to_drop)):\n",
    "        X_train = X_train.drop(to_drop[i], axis = 1)\n",
    "    \n",
    "    for i in range(len(to_drop)):\n",
    "        X_test = X_test.drop(to_drop[i], axis = 1)\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_train = np.array(y_train)\n",
    "    return X_train, X_test, y_train\n",
    "\n",
    "\n",
    "'''\n",
    "produce submission file\n",
    "'''\n",
    "def produce_solution(y):\n",
    "    with open('out.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', lineterminator=\"\\n\")\n",
    "        writer.writerow(['id', 'y'])\n",
    "        for i in range(y.shape[0]):\n",
    "            writer.writerow([float(i), y[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-be056532c0af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#X_test_original corresponds to X_test.csv as given in the task\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_original\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-750fc0ff50bb>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mto_drop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_be_eliminated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-750fc0ff50bb>\u001b[0m in \u001b[0;36mto_be_eliminated\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mto_be_eliminated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Create correlation matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mcorr_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Select upper triangle of correlation matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mupper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorr_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorr_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mcorr\u001b[1;34m(self, method, min_periods)\u001b[0m\n\u001b[0;32m   7003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7004\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'pearson'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7005\u001b[1;33m             \u001b[0mcorrel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibalgos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnancorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_float64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_periods\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7006\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'spearman'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7007\u001b[0m             correl = libalgos.nancorr_spearman(ensure_float64(mat),\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Approach 1: \n",
    "\n",
    "model assessment via 10 fold CV \n",
    "class imbalance is taken care of by oversamplingfrom classes 0 and 2\n",
    "\n",
    "Important Note\n",
    "Always split into test and train sets BEFORE trying oversampling techniques!\n",
    "Oversampling before splitting the data can allow the exact same observations \n",
    "to be present in both the test and train sets. This can allow our model to simply \n",
    "memorize specific data points and cause overfitting and poor generalization to the test data.\n",
    "'''\n",
    "\n",
    "#X and y are training x and y data \n",
    "#X_test_original corresponds to X_test.csv as given in the task \n",
    "\n",
    "X, X_test_original, y = load_data() \n",
    "y = y.ravel()\n",
    "scores = np.array([])\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "BMAC_scores = np.array([])\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #define X_train and y_train as data in training folds (model is fitted here)\n",
    "    #similarly, X_test, y_test as data in test fold (model is evaluated here)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    \n",
    "    y_train.columns = ['y']\n",
    "    y_test.columns = ['y']\n",
    "\n",
    "    #oversampling to offset class imbalance\n",
    "    X_concat = pd.concat([X_train, y_train], axis=1)\n",
    "    \n",
    "    # separate minority and majority classes\n",
    "    class_0 = X_concat[X_concat.y==0]\n",
    "    class_1 = X_concat[X_concat.y==1]\n",
    "    class_2 = X_concat[X_concat.y==2]\n",
    "\n",
    "    #upsample minority -- classes 0 and 2\n",
    "    class_0_upsampled = resample(class_0,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(class_1), # match number in majority class\n",
    "                          random_state=27) \n",
    "    class_2_upsampled = resample(class_2,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(class_1), # match number in majority class\n",
    "                          random_state=32)\n",
    "\n",
    "    upsampled = pd.concat([class_1, class_0_upsampled, class_2_upsampled])\n",
    "   \n",
    "    y_train = upsampled.y\n",
    "    X_train = upsampled.drop('y', axis=1)\n",
    "    \n",
    "    #1. Zero Mean, Unit Variance\n",
    "    print(\"Standardize data\")\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "#################################################################\n",
    "#begin fitting model to training folds -- X_train \n",
    "\n",
    "    #2. Outlier detection\n",
    "    print(\"Outlier Detection\")\n",
    "    isf = IsolationForest(n_estimators=100, contamination=0.30)\n",
    "    outliers = isf.fit_predict(X_train)\n",
    "\n",
    "    #DBScan = DBSCAN(eps = .5, metric='euclidean', min_samples = 30, n_jobs = -1)    \n",
    "    #outliers = DBScan.fit_predict(X_train)\n",
    "\n",
    "    unique, counts = np.unique(outliers, return_counts=True)\n",
    "    count_dict = dict(zip(unique, counts))\n",
    "    X_train = X_train[outliers == 1]\n",
    "    y_train = y_train[outliers == 1]\n",
    "    \n",
    "    \n",
    "    #3. Feature selection \n",
    "    print(\"Feature Selection\")\n",
    "    select = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    select.fit(X_train, y_train)\n",
    "    X_train = select.transform(X_train)\n",
    "    \n",
    "    print(\"Fitting the model\")\n",
    "    clf = xgb.XGBClassifier(random_state=42, learning_rate=0.6, n_estimators=100, max_depth=10)\n",
    "    #clf = RandomForestClassifier(n_estimators=300, max_depth=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "#end model fitting on X_train\n",
    "############################################################\n",
    "        \n",
    "    #prediction \n",
    "    print(\"Predicting\")\n",
    "    #selecting features based on training results\n",
    "    X_test = select.transform(X_test)\n",
    "    pred = clf.predict(X_test)\n",
    "    \n",
    "    #scoring\n",
    "    score = balanced_accuracy_score(y_test, pred)\n",
    "    print(score)\n",
    "    scores = np.append(scores,score)\n",
    "    \n",
    "##########################################################    \n",
    "\n",
    "truth = np.mean(scores)\n",
    "std = np.std(scores)\n",
    "print(\"mean expected error: \", truth, \"std: \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardize data\n",
      "Outlier Detection\n",
      "Feature Selection\n",
      "Fitting the model\n",
      "Predicting\n",
      "0.6808789358226535\n",
      "Standardize data\n",
      "Outlier Detection\n",
      "Feature Selection\n",
      "Fitting the model\n",
      "Predicting\n",
      "0.7039115747640338\n",
      "Standardize data\n",
      "Outlier Detection\n",
      "Feature Selection\n",
      "Fitting the model\n",
      "Predicting\n",
      "0.6732747789857854\n",
      "Standardize data\n",
      "Outlier Detection\n",
      "Feature Selection\n",
      "Fitting the model\n",
      "Predicting\n",
      "0.6534338960640921\n",
      "Standardize data\n",
      "Outlier Detection\n",
      "Feature Selection\n",
      "Fitting the model\n",
      "Predicting\n",
      "0.6792808859943152\n",
      "Standardize data\n",
      "Outlier Detection\n",
      "Feature Selection\n",
      "Fitting the model\n",
      "Predicting\n",
      "0.6757575757575758\n",
      "Standardize data\n",
      "Outlier Detection\n",
      "Feature Selection\n",
      "Fitting the model\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Approach 2: \n",
    "\n",
    "model assessment via 10 fold CV \n",
    "class imbalance is taken care of by undersampling from class 1 \n",
    "'''\n",
    "\n",
    "#X and y are training x and y data \n",
    "#X_test_original corresponds to X_test.csv as given in the task \n",
    "\n",
    "X, X_test_original, y = load_data() \n",
    "y = y.ravel()\n",
    "scores = np.array([])\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "BMAC_scores = np.array([])\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #define X_train and y_train as data in training folds (model is fitted here)\n",
    "    #similarly, X_test, y_test as data in test fold (model is evaluated here)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    \n",
    "    y_train.columns = ['y']\n",
    "    y_test.columns = ['y']\n",
    "\n",
    "    #undersampling from class 1 to offset class imbalance\n",
    "    X_concat = pd.concat([X_train, y_train], axis=1)\n",
    "    \n",
    "    # separate minority and majority classes\n",
    "    class_0 = X_concat[X_concat.y==0]\n",
    "    class_1 = X_concat[X_concat.y==1]\n",
    "    class_2 = X_concat[X_concat.y==2]\n",
    "\n",
    "    #upsample minority -- classes 0 and 2\n",
    "    class_1_under = resample(class_1,\n",
    "                          replace=False, # sample with replacement\n",
    "                          n_samples=len(class_0), # match number in minority classes\n",
    "                          random_state=27) \n",
    "\n",
    "    undersampled = pd.concat([class_1_under, class_0, class_2])\n",
    "   \n",
    "    y_train = undersampled.y\n",
    "    X_train = undersampled.drop('y', axis=1)\n",
    "    \n",
    "    #1. Zero Mean, Unit Variance\n",
    "    print(\"Standardize data\")\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "#################################################################\n",
    "#begin fitting model to training folds -- X_train \n",
    "\n",
    "    #2. Outlier detection\n",
    "    print(\"Outlier Detection\")\n",
    "    isf = IsolationForest(n_estimators=300, contamination=0.30)\n",
    "    outliers = isf.fit_predict(X_train)\n",
    "    \n",
    "    #DBScan = DBSCAN(eps = .5, metric=”euclidean”,min_samples = 30, n_jobs = -1)    \n",
    "    #outliers = DBScan.fit_predict(X_train)\n",
    "\n",
    "    unique, counts = np.unique(outliers, return_counts=True)\n",
    "    count_dict = dict(zip(unique, counts))\n",
    "    X_train = X_train[outliers == 1]\n",
    "    y_train = y_train[outliers == 1]\n",
    "    \n",
    "    \n",
    "    #3. Feature selection \n",
    "    print(\"Feature Selection\")\n",
    "    select = SelectFromModel(RandomForestClassifier(n_estimators=300, random_state=42))\n",
    "    select.fit(X_train, y_train)\n",
    "    X_train = select.transform(X_train)\n",
    "    \n",
    "    print(\"Fitting the model\")\n",
    "    clf = xgb.XGBClassifier(random_state=42, learning_rate=0.6, n_estimators=300, max_depth=10)\n",
    "    #clf = RandomForestClassifier(n_estimators=300, max_depth=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "#end model fitting on X_train\n",
    "############################################################\n",
    "        \n",
    "    #prediction \n",
    "    print(\"Predicting\")\n",
    "    #selecting features based on training results\n",
    "    X_test = select.transform(X_test)\n",
    "    pred = clf.predict(X_test)\n",
    "    \n",
    "    #scoring\n",
    "    score = balanced_accuracy_score(y_test, pred)\n",
    "    print(score)\n",
    "    scores = np.append(scores,score)\n",
    "\n",
    "##########################################################\n",
    "\n",
    "truth = np.mean(scores)\n",
    "std = np.std(scores)\n",
    "print(\"mean expected error: \", truth, \"std: \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction \n",
    "'''\n",
    "fit model on entire training dataset\n",
    "'''\n",
    "##missing steps \n",
    "##will be completed once all models will be cross-validated\n",
    "\n",
    "X_test_original = select.transform(X_test_original)\n",
    "pred = clf.predict(X_test_original)\n",
    "produce_solution(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
