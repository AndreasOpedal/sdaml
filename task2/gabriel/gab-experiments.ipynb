{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/Users/gabrielarpino/Documents/ETH_Semester_1/Advanced_Machine_Learning/sdaml/task2/gabriel/utils.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do all necessary preprocessing, calling prepro.py\n",
    "import utils\n",
    "from utils import *\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X, X_test_original, y = load_data() \n",
    "y = y.ravel()\n",
    "scores = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we construct different pipelines and test them using cross validation\n",
    "\n",
    "The intermediate estimators have to have the fit and transform methods implemented, and the final estimator has to have the fit method implemented. If running cross validation, the models that should output a y (don't just transform data, like SVC or XGBClassifier need to have a predict method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69934747 0.66892573 0.68201391]\n"
     ]
    }
   ],
   "source": [
    "# Make a SVM pipeline\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "svm_estimators = [('standardization', preprocessing.StandardScaler()), \n",
    "              ('feature_selection', SelectFromModel(RandomForestClassifier(n_estimators=300, random_state=42))),\n",
    "              ('classifier', svm.SVC(class_weight='balanced'))\n",
    "             ]\n",
    "svm_pipe = Pipeline(svm_estimators)\n",
    "\n",
    "score = cross_val_score(svm_pipe, X, y, cv=kf, scoring=make_scorer(balanced_accuracy_score))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.53687055 0.56981949 0.58575454]\n"
     ]
    }
   ],
   "source": [
    "# Make an XGBoost pipeline\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "xgb_estimators = [('standardization', preprocessing.StandardScaler()), \n",
    "              ('feature_selection', SelectFromModel(RandomForestClassifier(n_estimators=300, random_state=42))),\n",
    "              ('classifier', xgb.XGBClassifier(random_state=42))\n",
    "             ]\n",
    "xgb_pipe = Pipeline(xgb_estimators)\n",
    "\n",
    "xgb_score = cross_val_score(xgb_pipe, X, y, cv=kf, scoring=make_scorer(balanced_accuracy_score))\n",
    "print(xgb_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63312743 0.64263686 0.63047409]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  2.7min finished\n"
     ]
    }
   ],
   "source": [
    "# Create an ensemble estimator of XGB and SVM:\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "ensemble = VotingClassifier([('svm', svm.SVC(class_weight='balanced')), \n",
    "                            ('xgb', xgb.XGBClassifier(random_state=42, learning_rate=0.5, n_estimators=300, max_depth=10))], \n",
    "                           weights=None)\n",
    "ensemble_estimators = [('standardization', preprocessing.StandardScaler()), \n",
    "              ('feature_selection', SelectFromModel(RandomForestClassifier(n_estimators=300, random_state=42))),\n",
    "              ('classifier', ensemble)\n",
    "             ]\n",
    "ensemble_pipe = Pipeline(ensemble_estimators)\n",
    "\n",
    "ensemble_score = cross_val_score(ensemble_pipe, X, y, cv=kf, scoring=make_scorer(balanced_accuracy_score), verbose=1)\n",
    "print(ensemble_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'smote' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8d5425ee0d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msvm_smote_pipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm_smote_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm_smote_pipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbalanced_accuracy_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    392\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 232\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    354\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ETH_Semester_1/Advanced_Machine_Learning/sdaml/task2/gabriel/utils.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmote_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mX_smote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_smote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmote_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_smote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_smote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'smote' is not defined"
     ]
    }
   ],
   "source": [
    "# Try a SVM Estimator with SMOTE\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "svm_smote_estimators = [\n",
    "              ('standardization', preprocessing.StandardScaler()), \n",
    "              ('feature_selection', SelectFromModel(RandomForestClassifier(n_estimators=300, random_state=42))),\n",
    "              ('classifier', SMOTEClassifier(SMOTE('minority'), svm.SVC()))\n",
    "             ]\n",
    "svm_smote_pipe = Pipeline(svm_smote_estimators)\n",
    "\n",
    "score = cross_val_score(svm_smote_pipe, X, y, cv=kf, scoring=make_scorer(balanced_accuracy_score), verbose=2)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "utils.SMOTEClassifier"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eval pipelines\n",
    "smote = SMOTE('minority')\n",
    "SMOTEClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardize data\n",
      "Feature Selection\n",
      "Fitting the model\n",
      "|   iter    |  target   | c0_weight | c1_weight | c2_weight |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8346  \u001b[0m | \u001b[0m 2.634   \u001b[0m | \u001b[0m 0.4472  \u001b[0m | \u001b[0m 2.602   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6256  \u001b[0m | \u001b[0m 1.776   \u001b[0m | \u001b[0m 4.354   \u001b[0m | \u001b[0m 1.654   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.9062  \u001b[0m | \u001b[95m 7.349   \u001b[0m | \u001b[95m 2.442   \u001b[0m | \u001b[95m 4.894   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8347  \u001b[0m | \u001b[0m 6.127   \u001b[0m | \u001b[0m 2.592   \u001b[0m | \u001b[0m 2.374   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6345  \u001b[0m | \u001b[0m 8.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 8.0     \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.6374  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 8.0     \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.9194  \u001b[0m | \u001b[95m 8.0     \u001b[0m | \u001b[95m 5.0     \u001b[0m | \u001b[95m 8.0     \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.3333  \u001b[0m | \u001b[0m 8.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.3333  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 8.0     \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8601  \u001b[0m | \u001b[0m 4.626   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 6.351   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.3333  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8715  \u001b[0m | \u001b[0m 8.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 3.837   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.6155  \u001b[0m | \u001b[0m 4.954   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 4.49    \u001b[0m |\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "BEST APPROACH SO FAR\n",
    "    \n",
    "Approach 2: \n",
    "\n",
    "model assessment via 5 fold CV \n",
    "class imbalance is taken care of by undersampling from class 1 \n",
    "'''\n",
    "\n",
    "#X and y are training x and y data \n",
    "#X_test_original corresponds to X_test.csv as given in the task \n",
    "\n",
    "X, X_test_original, y = load_data() \n",
    "y = y.ravel()\n",
    "scores = np.array([])\n",
    "\n",
    "kf = KFold(n_splits=2)\n",
    "BMAC_scores = np.array([])\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #define X_train and y_train as data in training folds (model is fitted here)\n",
    "    #similarly, X_test, y_test as data in test fold (model is evaluated here)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    \n",
    "    y_train.columns = ['y']\n",
    "    y_test.columns = ['y']\n",
    "    \n",
    "    #1. Zero Mean, Unit Variance\n",
    "    print(\"Standardize data\")\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = pd.DataFrame(scaler.transform(X_train))\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    #2. Feature selection \n",
    "    print(\"Feature Selection\")\n",
    "    select = SelectFromModel(RandomForestClassifier(n_estimators=300, random_state=42))\n",
    "    select.fit(X_train, y_train)\n",
    "    X_train = pd.DataFrame(select.transform(X_train))\n",
    "   \n",
    "    '''\n",
    "    #3. Outlier detection\n",
    "    print(\"Outlier Detection\")\n",
    "    isf = IsolationForest(n_estimators=300, contamination=0.38)\n",
    "    outliers = isf.fit_predict(X_train)\n",
    "    unique, counts = np.unique(outliers, return_counts=True)\n",
    "    count_dict = dict(zip(unique, counts))\n",
    "    X_train = X_train[outliers == 1]\n",
    "    y_train = y_train[outliers == 1]\n",
    "    #DBScan = DBSCAN(eps = .5, metric=”euclidean”,min_samples = 30, n_jobs = -1)    \n",
    "    #outliers = DBScan.fit_predict(X_train)\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    #4. Undersampling from class 1 to offset class imbalance\n",
    "    print('Undersampling')\n",
    "    X_concat = pd.concat([X_train, y_train], axis=1)\n",
    "    # separate minority and majority classes\n",
    "    class_0 = X_concat[X_concat.y==0]\n",
    "    class_1 = X_concat[X_concat.y==1]\n",
    "    class_2 = X_concat[X_concat.y==2]\n",
    "\n",
    "    #undersample majority class (1)\n",
    "    class_1_under = resample(class_1,\n",
    "                          replace=False, # sample with replacement\n",
    "                          n_samples=len(class_0), # match number in minority classes\n",
    "                          random_state=27) \n",
    "    undersampled = pd.concat([class_1_under, class_0, class_2])\n",
    "    y_train = undersampled.y\n",
    "    X_train = undersampled.drop('y', axis=1)\n",
    "    '''\n",
    "    #5. fitting model\n",
    "    print(\"Fitting the model\")\n",
    "    #clf = xgb.XGBClassifier(random_state=42, learning_rate=0.5, n_estimators=300, max_depth=10)\n",
    "    #clf = RandomForestClassifier(n_estimators=300, max_depth=10)\n",
    "    class_weight = y_train.shape[0] / (3 * np.bincount((y_train.iloc[:,0]).astype(int)))\n",
    "    class_weights0 = { \n",
    "    0 : class_weight[0],\n",
    "    1 : class_weight[1],\n",
    "    2 : class_weight[2]\n",
    "    }\n",
    "    \n",
    "    ########## BO\n",
    "    def classifier(c0_weight=class_weight[0], c1_weight=class_weight[1], c2_weight=class_weight[2], \n",
    "                   xtrain=X_train, ytrain=y_train, xtest=X_test, ytest=y_test):\n",
    "        class_weights1 = { \n",
    "        0 : c0_weight,\n",
    "        1 : c1_weight,\n",
    "        2 : c2_weight\n",
    "        }\n",
    "        clf = svm.SVC(class_weight=class_weights1)\n",
    "\n",
    "        clf.fit(xtrain, ytrain)\n",
    "\n",
    "        #6. prediction \n",
    "        #print(\"Predicting\")\n",
    "        #selecting features based on training results\n",
    "        #_test_selected = pd.DataFrame(select.transform(xtest))  #note: transform was previosuly fitted on training folds\n",
    "        pred = clf.predict(xtrain)\n",
    "\n",
    "        #scoring\n",
    "        score = balanced_accuracy_score(ytrain, pred)\n",
    "        #print(score)\n",
    "        #scores = np.append(scores,score)\n",
    "        return score\n",
    "\n",
    "\n",
    "    # specify parameters and distributions to sample from\n",
    "    param_dist = {\"c0_weight\": (0, 15), \"c1_weight\": (0, 12), \"c2_weight\": (0, 15)}\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=classifier,\n",
    "        pbounds=param_dist,\n",
    "        verbose=2,\n",
    "        random_state=5,\n",
    "    )\n",
    "\n",
    "    probe_params = {\"c0_weight\": class_weight[0], \"c1_weight\": class_weight[1], \"c2_weight\": class_weight[2]}\n",
    "    optimizer.probe(\n",
    "        params=probe_params,\n",
    "        lazy=True\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(\n",
    "        init_points=3,\n",
    "        n_iter=350,\n",
    "    )\n",
    "\n",
    "    print(optimizer.max)\n",
    "    \n",
    "\n",
    "    ########## BO\n",
    "    class_weights_test = { \n",
    "    0 : optimizer.max['params']['c0_weight'],\n",
    "    1 : optimizer.max['params']['c1_weight'],\n",
    "    2 : optimizer.max['params']['c2_weight']\n",
    "    }\n",
    "    clf2 = svm.SVC(class_weight=class_weights_test)\n",
    "    clf2.fit(X_train, y_train)\n",
    "    print(\"Predicting\")\n",
    "    #selecting features based on training results\n",
    "    X_test_selected = pd.DataFrame(select.transform(X_test))  #note: transform was previosuly fitted on training folds\n",
    "    pred2 = clf2.predict(X_test_selected)\n",
    "    #scoring\n",
    "    score2 = balanced_accuracy_score(y_test, pred2)\n",
    "    print('Test score:', score2)    \n",
    "    scores = np.append(scores,score2)\n",
    "    \n",
    "    \n",
    "##########################################################\n",
    "\n",
    "truth = np.mean(scores)\n",
    "std = np.std(scores)\n",
    "print(\"mean expected error: \", truth, \"std: \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardize data\n",
      "Feature Selection\n",
      "Outlier Detection\n",
      "Fitting the model\n",
      "Predicting\n",
      "0.5562480297533559\n",
      "Standardize data\n",
      "Feature Selection\n",
      "Outlier Detection\n",
      "Fitting the model\n",
      "Predicting\n",
      "0.5710455037919827\n",
      "Standardize data\n",
      "Feature Selection\n",
      "Outlier Detection\n",
      "Fitting the model\n",
      "Predicting\n",
      "0.5474442986943582\n",
      "Standardize data\n",
      "Feature Selection\n",
      "Outlier Detection\n",
      "Fitting the model\n",
      "Predicting\n",
      "0.5576211605644027\n",
      "Standardize data\n",
      "Feature Selection\n",
      "Outlier Detection\n",
      "Fitting the model\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Approach 4: \n",
    "\n",
    "model assessment via 5 fold CV \n",
    "class imbalance is taken care of by undersampling from class 1 \n",
    "'''\n",
    "\n",
    "#X and y are training x and y data \n",
    "#X_test_original corresponds to X_test.csv as given in the task \n",
    "\n",
    "X, X_test_original, y = load_data() \n",
    "y = y.ravel()\n",
    "scores = np.array([])\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "BMAC_scores = np.array([])\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #define X_train and y_train as data in training folds (model is fitted here)\n",
    "    #similarly, X_test, y_test as data in test fold (model is evaluated here)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    \n",
    "    y_train.columns = ['y']\n",
    "    y_test.columns = ['y']\n",
    "    \n",
    "    smote = SMOTE('minority')\n",
    "    X_train, y_train = smote.fit_sample(X_train, y_train)\n",
    "\n",
    "    '''\n",
    "    #undersampling from class 1 to offset class imbalance\n",
    "    X_concat = pd.concat([X_train, y_train], axis=1)\n",
    "    \n",
    "    # separate minority and majority classes\n",
    "    class_0 = X_concat[X_concat.y==0]\n",
    "    class_1 = X_concat[X_concat.y==1]\n",
    "    class_2 = X_concat[X_concat.y==2]\n",
    "\n",
    "    #upsample minority -- classes 0 and 2\n",
    "    class_1_under = resample(class_1,\n",
    "                          replace=False, # sample with replacement\n",
    "                          n_samples=len(class_0), # match number in minority classes\n",
    "                          random_state=27) \n",
    "\n",
    "    undersampled = pd.concat([class_1_under, class_0, class_2])\n",
    "   \n",
    "    y_train = undersampled.y\n",
    "    X_train = undersampled.drop('y', axis=1)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #1. Zero Mean, Unit Variance\n",
    "    print(\"Standardize data\")\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "#################################################################\n",
    "#begin fitting model to training folds -- X_train \n",
    "\n",
    "    #2. Feature selection \n",
    "    print(\"Feature Selection\")\n",
    "    select = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    select.fit(X_train, y_train)\n",
    "    X_train = select.transform(X_train)\n",
    "\n",
    "    #3. Outlier detection\n",
    "    print(\"Outlier Detection\")\n",
    "    isf = IsolationForest(n_estimators=100, contamination=0.35)\n",
    "    outliers = isf.fit_predict(X_train)\n",
    "    \n",
    "    #DBScan = DBSCAN(eps = .5, metric=”euclidean”,min_samples = 30, n_jobs = -1)    \n",
    "    #outliers = DBScan.fit_predict(X_train)\n",
    "\n",
    "    unique, counts = np.unique(outliers, return_counts=True)\n",
    "    count_dict = dict(zip(unique, counts))\n",
    "    X_train = X_train[outliers == 1]\n",
    "    y_train = y_train[outliers == 1]\n",
    "    \n",
    "    print(\"Fitting the model\")\n",
    "    clf = xgb.XGBClassifier(random_state=42, learning_rate=0.5, n_estimators=100, max_depth=10)\n",
    "    #clf = RandomForestClassifier(n_estimators=300, max_depth=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "#end model fitting on X_train\n",
    "############################################################\n",
    "        \n",
    "    #prediction \n",
    "    print(\"Predicting\")\n",
    "    #selecting features based on training results\n",
    "    X_test = select.transform(X_test)\n",
    "    pred = clf.predict(X_test)\n",
    "    \n",
    "    #scoring\n",
    "    score = balanced_accuracy_score(y_test, pred)\n",
    "    print(score)\n",
    "    scores = np.append(scores,score)\n",
    "\n",
    "##########################################################\n",
    "\n",
    "truth = np.mean(scores)\n",
    "std = np.std(scores)\n",
    "print(\"mean expected error: \", truth, \"std: \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
