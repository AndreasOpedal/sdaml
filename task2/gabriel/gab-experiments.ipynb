{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all necessary preprocessing, calling prepro.py\n",
    "import utils\n",
    "from utils import *\n",
    "importlib.reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X, X_test_original, y = load_data() \n",
    "y = y.ravel()\n",
    "scores = np.array([])\n",
    "xtrain = X  # For andreas cross validation\n",
    "ytrain = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we construct different pipelines and test them using cross validation\n",
    "\n",
    "The intermediate estimators have to have the fit and transform methods implemented, and the final estimator has to have the fit method implemented. If running cross validation, the models that should output a y (don't just transform data, like SVC or XGBClassifier need to have a predict method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with 'balanced' weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a SVM pipeline\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "svm_estimators = [('standardization', preprocessing.StandardScaler()), \n",
    "              ('feature_selection', SelectFromModel(RandomForestClassifier(n_estimators=300, random_state=42))),\n",
    "              ('classifier', svm.SVC(class_weight='balanced'))\n",
    "             ]\n",
    "svm_pipe = Pipeline(svm_estimators)\n",
    "\n",
    "score = cross_val_score(svm_pipe, X, y, cv=kf, scoring=make_scorer(balanced_accuracy_score))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check confusion matrix:\n",
    "svm_pipe.fit(X, y)\n",
    "y_pred = svm_pipe.predict(X)\n",
    "plot_confusion_matrix(y, y_pred, classes=[0, 1, 2],\n",
    "                      title='Confusion matrix', normalize=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM ('balanced') + XGBC Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an XGBoost pipeline\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "xgb_estimators = [('standardization', preprocessing.StandardScaler()), \n",
    "              ('feature_selection', SelectFromModel(RandomForestClassifier(n_estimators=300, random_state=42))),\n",
    "              ('classifier', xgb.XGBClassifier(random_state=42))\n",
    "             ]\n",
    "xgb_pipe = Pipeline(xgb_estimators)\n",
    "\n",
    "# xgb_score = cross_val_score(xgb_pipe, X, y, cv=kf, scoring=make_scorer(balanced_accuracy_score))\n",
    "# print(xgb_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check confusion matrix:\n",
    "xgb_pipe.fit(X, y)\n",
    "y_pred = xgb_pipe.predict(X)\n",
    "plot_confusion_matrix(y, y_pred, classes=[0, 1, 2],\n",
    "                      title='Confusion matrix', normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ensemble estimator of XGB and SVM:\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "ensemble = VotingClassifier([('svm', svm.SVC(class_weight='balanced')), \n",
    "                            ('xgb', xgb.XGBClassifier(random_state=42, learning_rate=0.5, n_estimators=300, max_depth=10))], \n",
    "                           weights=None)\n",
    "ensemble_estimators = [('standardization', preprocessing.StandardScaler()), \n",
    "              ('feature_selection', SelectFromModel(RandomForestClassifier(n_estimators=300, random_state=42))),\n",
    "              ('classifier', ensemble)\n",
    "             ]\n",
    "ensemble_pipe = Pipeline(ensemble_estimators)\n",
    "\n",
    "ensemble_score = cross_val_score(ensemble_pipe, X, y, cv=kf, scoring=make_scorer(balanced_accuracy_score), verbose=1)\n",
    "print(ensemble_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a SVM Estimator with SMOTE\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# svm_smote_estimators = [\n",
    "#               ('standardization', preprocessing.StandardScaler()), \n",
    "#               ('feature_selection', SelectFromModel(RandomForestClassifier(n_estimators=300, random_state=42))),\n",
    "#               ('classifier', SMOTEClassifier(SMOTE('minority'), svm.SVC()))\n",
    "#              ]\n",
    "\n",
    "smote2 = SMOTE(random_state=42)\n",
    "svm_pipe2 = Pipeline([('standardization', preprocessing.StandardScaler()), \n",
    "              ('classifier', svm.SVC())\n",
    "             ])\n",
    "svm_smote_estimators2 = [\n",
    "              ('classifier', SMOTEClassifier(smote2, svm_pipe2))\n",
    "             ]\n",
    "# svm_smote_estimators = [\n",
    "#               ('classifier', svm_pipe)\n",
    "#              ]\n",
    "svm_smote_pipe2 = Pipeline(svm_smote_estimators2)\n",
    "\n",
    "score = cross_val_score(svm_smote_pipe2, X, y, cv=kf, scoring=make_scorer(balanced_accuracy_score), verbose=2)\n",
    "print(score)\n",
    "# Score: [0.59077241 0.6063096  0.57785729]\n",
    "# Without smote, I get [0.55728494 0.54095411 0.57493051 0.57253056 0.55494866] which is same as andreas in mean\n",
    "# With current smote: [0.60208197 0.60274172 0.65079332 0.64553232 0.66089551]\n",
    "# SMOTE no deep copy: [0.66815703 0.58598228 0.62943226 0.6350562  0.61465314]\n",
    "# SMOTE no deep copy, sklearn.base.clone(classifier): [0.61372447 0.60251485 0.65401315 0.6055938  0.63318461]\n",
    "# SMOTE deep copy, sklearn.base.clone(classifier): [0.61372447 0.60251485 0.65401315 0.6055938  0.63318461]\n",
    "# SMOTE instantiated inside, sklearn.base.clone (classifier): [0.64730051 0.59922761 0.64895935 0.62417725 0.61648087]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check confusion matrix:\n",
    "svm_smote_pipe2.fit(X, y)\n",
    "y_pred = svm_smote_pipe2.predict(X)\n",
    "plot_confusion_matrix(y, y_pred, classes=[0, 1, 2],\n",
    "                      title='Confusion matrix', normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curve\n",
    "new_nn = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                       hidden_layer_sizes=(15,2), random_state=1,\n",
    "                      )\n",
    "plot_learning_curve(new_nn, 'NN (SMOTE) learning curve', X, y, ylim=(0.7, 1.01), cv=kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with SMOTE using Andreas cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM + SMOTE cross validation\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "\n",
    "BMAC_means = np.array([])\n",
    "BMAC_stds = np.array([])\n",
    "BMAC_scores = np.array([])\n",
    "for train_index, test_index in kf.split(xtrain):\n",
    "\n",
    "    x_train = xtrain[train_index]\n",
    "    x_test = xtrain[test_index]\n",
    "\n",
    "    y_train = ytrain[train_index]\n",
    "    y_test = ytrain[test_index]\n",
    "\n",
    "    sm = SMOTE(random_state=42)\n",
    "    x_train, y_train = sm.fit_resample(x_train, y_train) #x_train and y_train are now arrays\n",
    "\n",
    "    # Scale the data (should this be done for each bootstrap sample? in that case how)\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    scaler2 = preprocessing.StandardScaler()\n",
    "    x_test = scaler2.fit_transform(x_test)\n",
    "\n",
    "    # Model to fit\n",
    "    estimator = svm.SVC()\n",
    "    estimator.fit(x_train, y_train.ravel())\n",
    "    pred = estimator.predict(x_test)\n",
    "    BMAC = balanced_accuracy_score(y_test, pred)\n",
    "\n",
    "    print(\"BMAC Score: \", BMAC)\n",
    "    BMAC_scores = np.append(BMAC_scores, BMAC)\n",
    "\n",
    "BMAC_means = np.append(BMAC_means, np.mean(BMAC_scores))\n",
    "BMAC_stds = np.append(BMAC_stds, np.std(BMAC_scores))\n",
    "#print(np.mean(BMAC_scores))\n",
    "#print(np.std(BMAC_scores))\n",
    "\n",
    "print(BMAC_scores)\n",
    "print(BMAC_means)\n",
    "print(BMAC_stds)\n",
    "\n",
    "# WIth the sklearn pipeline: \n",
    "# BMAC Score:  0.6653723616089208\n",
    "# BMAC Score:  0.5817579772284974\n",
    "# BMAC Score:  0.6385067508557504\n",
    "# BMAC Score:  0.6052335588793922\n",
    "# BMAC Score:  0.6230010015609052\n",
    "# [0.66537236 0.58175798 0.63850675 0.60523356 0.623001  ]\n",
    "# [0.62277433]\n",
    "# [0.02846813]\n",
    "\n",
    "# With exactly like andreas did (no pipeline):\n",
    "# BMAC Score:  0.6742004135726839\n",
    "# BMAC Score:  0.6970752260765067\n",
    "# BMAC Score:  0.6703766780204368\n",
    "# BMAC Score:  0.6978427317254585\n",
    "# BMAC Score:  0.690524245525415\n",
    "# [0.67420041 0.69707523 0.67037668 0.69784273 0.69052425]\n",
    "# [0.68600386]\n",
    "# [0.01154728]\n",
    "\n",
    "# With like andreas but with 2 diff standardizers for xtrain and xtest (same thing)\n",
    "# BMAC Score:  0.6915209721130537\n",
    "# BMAC Score:  0.6502473263231936\n",
    "# BMAC Score:  0.683423031021961\n",
    "# BMAC Score:  0.6919825859146247\n",
    "# BMAC Score:  0.7100686782444291\n",
    "# [0.69152097 0.65024733 0.68342303 0.69198259 0.71006868]\n",
    "# [0.68544852]\n",
    "# [0.01964153]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBC with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a XGBC Estimator with SMOTE\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "xgb_smote_estimators = [\n",
    "              ('standardization', preprocessing.StandardScaler()), \n",
    "              ('feature_selection', SelectFromModel(RandomForestClassifier(n_estimators=300, random_state=42))),\n",
    "              ('classifier', SMOTEClassifier(SMOTE(random_state=42), xgb.XGBClassifier()))\n",
    "             ]\n",
    "xgb_smote_pipe = Pipeline(xgb_smote_estimators)\n",
    "\n",
    "# score = cross_val_score(xgb_smote_pipe, X, y, cv=kf, scoring=make_scorer(balanced_accuracy_score), verbose=2)\n",
    "# print(score)\n",
    "# Score: [0.56183764 0.53822128 0.61737798]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check confusion matrix:\n",
    "xgb_smote_pipe.fit(X, y)\n",
    "y_pred = xgb_smote_pipe.predict(X)\n",
    "plot_confusion_matrix(y, y_pred, classes=[0, 1, 2],\n",
    "                      title='Confusion matrix', normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BMAC Score:  0.6719996675437804\n",
      "BMAC Score:  0.6553210516180873\n",
      "BMAC Score:  0.6869477269541799\n",
      "BMAC Score:  0.6562721569618122\n",
      "BMAC Score:  0.6620788661635456\n",
      "Scores: [0.67199967 0.65532105 0.68694773 0.65627216 0.66207887]\n",
      "Mean Scores: [0.66652389]\n",
      "Std Scores: [0.01181082]\n"
     ]
    }
   ],
   "source": [
    "# Try a nn Estimator with SMOTE\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "\n",
    "BMAC_means = np.array([])\n",
    "BMAC_stds = np.array([])\n",
    "BMAC_scores = np.array([])\n",
    "for train_index, test_index in kf.split(xtrain):\n",
    "\n",
    "    x_train = xtrain[train_index]\n",
    "    x_test = xtrain[test_index]\n",
    "\n",
    "    y_train = ytrain[train_index]\n",
    "    y_test = ytrain[test_index]\n",
    "\n",
    "    sm = SMOTE(random_state=42)\n",
    "    x_train, y_train = sm.fit_resample(x_train, y_train) #x_train and y_train are now arrays\n",
    "\n",
    "    # Scale the data (should this be done for each bootstrap sample? in that case how)\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    scaler2 = preprocessing.StandardScaler()\n",
    "    x_test = scaler2.fit_transform(x_test)\n",
    "\n",
    "    # Model to fit\n",
    "    '''\n",
    "    MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n",
    "                  beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
    "                  epsilon=1e-08, hidden_layer_sizes=(15,),\n",
    "                  learning_rate='constant', learning_rate_init=0.001,\n",
    "                  max_iter=200, momentum=0.9, n_iter_no_change=10,\n",
    "                  nesterovs_momentum=True, power_t=0.5,  random_state=1,\n",
    "                  shuffle=True, solver='lbfgs', tol=0.0001,\n",
    "                  validation_fraction=0.1, verbose=False, warm_start=False)\n",
    "\n",
    "    '''\n",
    "    estimator = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                           hidden_layer_sizes=(150,150), random_state=1,\n",
    "                            activation='relu', learning_rate='invscaling', beta_1=0.9, \n",
    "                              beta_2=0.999, learning_rate_init=0.001, early_stopping=True,\n",
    "                              momentum=0.9, shuffle=True, epsilon=1e-08, \n",
    "                              \n",
    "                          )\n",
    "    estimator.fit(x_train, y_train.ravel())\n",
    "    pred = estimator.predict(x_test)\n",
    "    BMAC = balanced_accuracy_score(y_test, pred)\n",
    "\n",
    "    print(\"BMAC Score: \", BMAC)\n",
    "    BMAC_scores = np.append(BMAC_scores, BMAC)\n",
    "    \n",
    "#     train_sizes, train_scores, valid_scores = learning_curve(estimator, x_train, y_train, train_sizes=[50, 80, 110], cv=5)\n",
    "\n",
    "BMAC_means = np.append(BMAC_means, np.mean(BMAC_scores))\n",
    "BMAC_stds = np.append(BMAC_stds, np.std(BMAC_scores))\n",
    "#print(np.mean(BMAC_scores))\n",
    "#print(np.std(BMAC_scores))\n",
    "\n",
    "print(\"Scores:\", BMAC_scores)\n",
    "print(\"Mean Scores:\", BMAC_means)\n",
    "print(\"Std Scores:\", BMAC_stds)\n",
    "\n",
    "# Mean Scores: [0.67625076], cv=5:\n",
    "#    estimator = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "#                            hidden_layer_sizes=(150,150), random_state=1,\n",
    "#                             activation='relu', learning_rate='constant', beta_1=0.9, \n",
    "#                               beta_2=0.999, learning_rate_init=0.001\n",
    "#                           )\n",
    "\n",
    "#  Scores: [0.66639015 0.6848148  0.70958482 0.69875664 0.65585182]\n",
    "# Mean Scores: [0.68307965]\n",
    "# Std Scores: [0.01985546]\n",
    "    \n",
    "# estimator = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "#                        hidden_layer_sizes=(150,150), random_state=1,\n",
    "#                         activation='relu', learning_rate='invscaling', beta_1=0.9, \n",
    "#                           beta_2=0.999, learning_rate_init=0.001, early_stopping=True,\n",
    "#                           momentum=0.9, shuffle=True, epsilon=1e-08, \n",
    "\n",
    "#                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114f4fb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check confusion matrix:\n",
    "\n",
    "# Choose 1 split\n",
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "train_index, test_index = list(kf.split(X))[0]\n",
    "\n",
    "x_train = xtrain[train_index]\n",
    "x_test = xtrain[test_index]\n",
    "\n",
    "y_train = ytrain[train_index]\n",
    "y_test = ytrain[test_index]\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "x_train, y_train = sm.fit_resample(x_train, y_train) #x_train and y_train are now arrays\n",
    "\n",
    "# Scale the data (should this be done for each bootstrap sample? in that case how)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "scaler2 = preprocessing.StandardScaler()\n",
    "x_test = scaler2.fit_transform(x_test)\n",
    "\n",
    "# NN model\n",
    "estimator = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                       hidden_layer_sizes=(150,150), random_state=1,\n",
    "                        activation='relu', learning_rate='invscaling', beta_1=0.9, \n",
    "                          beta_2=0.999, learning_rate_init=0.001, early_stopping=True,\n",
    "                          momentum=0.9, shuffle=True, epsilon=1e-08, \n",
    "\n",
    "                      )\n",
    "\n",
    "estimator.fit(x_train, y_train)\n",
    "y_pred = estimator.predict(x_train)\n",
    "plot_confusion_matrix(y_train, y_pred, classes=[0, 1, 2],\n",
    "                      title='Confusion matrix', normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a submission\n",
    "\n",
    "'''\n",
    "produce submission file\n",
    "'''\n",
    "def produce_solution(y):\n",
    "    with open('out.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', lineterminator=\"\\n\")\n",
    "        writer.writerow(['id', 'y'])\n",
    "        for i in range(y.shape[0]):\n",
    "            writer.writerow([float(i), y[i]])\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "x_train, y_train = sm.fit_resample(X, y) #x_train and y_train are now arrays\n",
    "\n",
    "# Scale the data (should this be done for each bootstrap sample? in that case how)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "scaler2 = preprocessing.StandardScaler()\n",
    "x_test = scaler2.fit_transform(X_test_original)\n",
    "\n",
    "# NN model\n",
    "estimator = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                       hidden_layer_sizes=(150,150), random_state=1,\n",
    "                        activation='relu', learning_rate='invscaling', beta_1=0.9, \n",
    "                          beta_2=0.999, learning_rate_init=0.001, early_stopping=True,\n",
    "                          momentum=0.9, shuffle=True, epsilon=1e-08, \n",
    "\n",
    "                      )\n",
    "\n",
    "estimator.fit(x_train, y_train)\n",
    "y_pred = estimator.predict(x_test)\n",
    "            \n",
    "produce_solution(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.43299747\n",
      "Iteration 2, loss = 0.55793488\n",
      "Iteration 3, loss = 0.41909517\n",
      "Iteration 4, loss = 0.34979653\n",
      "Iteration 5, loss = 0.29351383\n",
      "Iteration 6, loss = 0.25624875\n",
      "Iteration 7, loss = 0.21420212\n",
      "Iteration 8, loss = 0.17716785\n",
      "Iteration 9, loss = 0.14479983\n",
      "Iteration 10, loss = 0.12097721\n",
      "Iteration 11, loss = 0.09684275\n",
      "Iteration 12, loss = 0.08094240\n",
      "Iteration 13, loss = 0.06489712\n",
      "Iteration 14, loss = 0.05351462\n",
      "Iteration 15, loss = 0.04358326\n",
      "Iteration 16, loss = 0.03558521\n",
      "Iteration 17, loss = 0.02981081\n",
      "Iteration 18, loss = 0.02449188\n",
      "Iteration 19, loss = 0.02062976\n",
      "Iteration 20, loss = 0.01671875\n",
      "Iteration 21, loss = 0.01431368\n",
      "Iteration 22, loss = 0.01220402\n",
      "Iteration 23, loss = 0.01049628\n",
      "Iteration 24, loss = 0.00914539\n",
      "Iteration 25, loss = 0.00802674\n",
      "Iteration 26, loss = 0.00715083\n",
      "Iteration 27, loss = 0.00641616\n",
      "Iteration 28, loss = 0.00577865\n",
      "Iteration 29, loss = 0.00528588\n",
      "Iteration 30, loss = 0.00480797\n",
      "Iteration 31, loss = 0.00440254\n",
      "Iteration 32, loss = 0.00410371\n",
      "Iteration 33, loss = 0.00381266\n",
      "Iteration 34, loss = 0.00356158\n",
      "Iteration 35, loss = 0.00335125\n",
      "Iteration 36, loss = 0.00317009\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.87640669\n",
      "Iteration 2, loss = 0.47249551\n",
      "Iteration 3, loss = 0.39053813\n",
      "Iteration 4, loss = 0.33009767\n",
      "Iteration 5, loss = 0.26938375\n",
      "Iteration 6, loss = 0.22175394\n",
      "Iteration 7, loss = 0.19637788\n",
      "Iteration 8, loss = 0.16051917\n",
      "Iteration 9, loss = 0.13986982\n",
      "Iteration 10, loss = 0.11170139\n",
      "Iteration 11, loss = 0.08954394\n",
      "Iteration 12, loss = 0.07339558\n",
      "Iteration 13, loss = 0.05803973\n",
      "Iteration 14, loss = 0.04848790\n",
      "Iteration 15, loss = 0.03968800\n",
      "Iteration 16, loss = 0.03513078\n",
      "Iteration 17, loss = 0.02840603\n",
      "Iteration 18, loss = 0.02179136\n",
      "Iteration 19, loss = 0.02298676\n",
      "Iteration 20, loss = 0.01883070\n",
      "Iteration 21, loss = 0.01223183\n",
      "Iteration 22, loss = 0.00926803\n",
      "Iteration 23, loss = 0.00788620\n",
      "Iteration 24, loss = 0.00685813\n",
      "Iteration 25, loss = 0.00592716\n",
      "Iteration 26, loss = 0.00519170\n",
      "Iteration 27, loss = 0.00460440\n",
      "Iteration 28, loss = 0.00420723\n",
      "Iteration 29, loss = 0.00383313\n",
      "Iteration 30, loss = 0.00351389\n",
      "Iteration 31, loss = 0.00316073\n",
      "Iteration 32, loss = 0.00290986\n",
      "Iteration 33, loss = 0.00271898\n",
      "Iteration 34, loss = 0.00255766\n",
      "Iteration 35, loss = 0.00235734\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76108127\n",
      "Iteration 2, loss = 0.46639636\n",
      "Iteration 3, loss = 0.38940788\n",
      "Iteration 4, loss = 0.31813574\n",
      "Iteration 5, loss = 0.26781896\n",
      "Iteration 6, loss = 0.21566193\n",
      "Iteration 7, loss = 0.18144478\n",
      "Iteration 8, loss = 0.17329887\n",
      "Iteration 9, loss = 0.15298111\n",
      "Iteration 10, loss = 0.10977609\n",
      "Iteration 11, loss = 0.08239741\n",
      "Iteration 12, loss = 0.06646311\n",
      "Iteration 13, loss = 0.05538121\n",
      "Iteration 14, loss = 0.04354388\n",
      "Iteration 15, loss = 0.03253156\n",
      "Iteration 16, loss = 0.02513724\n",
      "Iteration 17, loss = 0.02017473\n",
      "Iteration 18, loss = 0.01717816\n",
      "Iteration 19, loss = 0.01398247\n",
      "Iteration 20, loss = 0.01169968\n",
      "Iteration 21, loss = 0.00954853\n",
      "Iteration 22, loss = 0.00764492\n",
      "Iteration 23, loss = 0.00646735\n",
      "Iteration 24, loss = 0.00576701\n",
      "Iteration 25, loss = 0.00488066\n",
      "Iteration 26, loss = 0.00412608\n",
      "Iteration 27, loss = 0.00381714\n",
      "Iteration 28, loss = 0.00329361\n",
      "Iteration 29, loss = 0.00289255\n",
      "Iteration 30, loss = 0.00266598\n",
      "Iteration 31, loss = 0.00243679\n",
      "Iteration 32, loss = 0.00218761\n",
      "Iteration 33, loss = 0.00206029\n",
      "Iteration 34, loss = 0.00184161\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68759655\n",
      "Iteration 2, loss = 0.43121927\n",
      "Iteration 3, loss = 0.35812521\n",
      "Iteration 4, loss = 0.29325276\n",
      "Iteration 5, loss = 0.25979314\n",
      "Iteration 6, loss = 0.20927856\n",
      "Iteration 7, loss = 0.16191180\n",
      "Iteration 8, loss = 0.13597108\n",
      "Iteration 9, loss = 0.10354519\n",
      "Iteration 10, loss = 0.08196870\n",
      "Iteration 11, loss = 0.06026661\n",
      "Iteration 12, loss = 0.04647446\n",
      "Iteration 13, loss = 0.03651398\n",
      "Iteration 14, loss = 0.02925931\n",
      "Iteration 15, loss = 0.02013202\n",
      "Iteration 16, loss = 0.01369932\n",
      "Iteration 17, loss = 0.01082029\n",
      "Iteration 18, loss = 0.00801422\n",
      "Iteration 19, loss = 0.00643186\n",
      "Iteration 20, loss = 0.00557919\n",
      "Iteration 21, loss = 0.00450914\n",
      "Iteration 22, loss = 0.00392892\n",
      "Iteration 23, loss = 0.00337836\n",
      "Iteration 24, loss = 0.00291491\n",
      "Iteration 25, loss = 0.00244366\n",
      "Iteration 26, loss = 0.00220661\n",
      "Iteration 27, loss = 0.00196730\n",
      "Iteration 28, loss = 0.00183840\n",
      "Iteration 29, loss = 0.00161325\n",
      "Iteration 30, loss = 0.00146017\n",
      "Iteration 31, loss = 0.00136194\n",
      "Iteration 32, loss = 0.00122349\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65445068\n",
      "Iteration 2, loss = 0.42849121\n",
      "Iteration 3, loss = 0.35046541\n",
      "Iteration 4, loss = 0.29448434\n",
      "Iteration 5, loss = 0.24424899\n",
      "Iteration 6, loss = 0.20253722\n",
      "Iteration 7, loss = 0.16883982\n",
      "Iteration 8, loss = 0.13838084\n",
      "Iteration 9, loss = 0.11185186\n",
      "Iteration 10, loss = 0.08789986\n",
      "Iteration 11, loss = 0.07009831\n",
      "Iteration 12, loss = 0.04961298\n",
      "Iteration 13, loss = 0.03808459\n",
      "Iteration 14, loss = 0.02695259\n",
      "Iteration 15, loss = 0.01890126\n",
      "Iteration 16, loss = 0.01520684\n",
      "Iteration 17, loss = 0.01360818\n",
      "Iteration 18, loss = 0.00941450\n",
      "Iteration 19, loss = 0.00639541\n",
      "Iteration 20, loss = 0.00530013\n",
      "Iteration 21, loss = 0.00405599\n",
      "Iteration 22, loss = 0.00346635\n",
      "Iteration 23, loss = 0.00273400\n",
      "Iteration 24, loss = 0.00229458\n",
      "Iteration 25, loss = 0.00195626\n",
      "Iteration 26, loss = 0.00174716\n",
      "Iteration 27, loss = 0.00161712\n",
      "Iteration 28, loss = 0.00145712\n",
      "Iteration 29, loss = 0.00126123\n",
      "Iteration 30, loss = 0.00118895\n",
      "Iteration 31, loss = 0.00105884\n",
      "Iteration 32, loss = 0.00094890\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.43138101\n",
      "Iteration 2, loss = 0.61029341\n",
      "Iteration 3, loss = 0.47009454\n",
      "Iteration 4, loss = 0.38379657\n",
      "Iteration 5, loss = 0.32730414\n",
      "Iteration 6, loss = 0.26333000\n",
      "Iteration 7, loss = 0.22570335\n",
      "Iteration 8, loss = 0.18307328\n",
      "Iteration 9, loss = 0.15009351\n",
      "Iteration 10, loss = 0.11979079\n",
      "Iteration 11, loss = 0.09653865\n",
      "Iteration 12, loss = 0.07699188\n",
      "Iteration 13, loss = 0.05979379\n",
      "Iteration 14, loss = 0.04798749\n",
      "Iteration 15, loss = 0.03874035\n",
      "Iteration 16, loss = 0.03126398\n",
      "Iteration 17, loss = 0.02582493\n",
      "Iteration 18, loss = 0.02135033\n",
      "Iteration 19, loss = 0.01822617\n",
      "Iteration 20, loss = 0.01514344\n",
      "Iteration 21, loss = 0.01292659\n",
      "Iteration 22, loss = 0.01118307\n",
      "Iteration 23, loss = 0.00982524\n",
      "Iteration 24, loss = 0.00849805\n",
      "Iteration 25, loss = 0.00758051\n",
      "Iteration 26, loss = 0.00676677\n",
      "Iteration 27, loss = 0.00611639\n",
      "Iteration 28, loss = 0.00549169\n",
      "Iteration 29, loss = 0.00499339\n",
      "Iteration 30, loss = 0.00462176\n",
      "Iteration 31, loss = 0.00428932\n",
      "Iteration 32, loss = 0.00396010\n",
      "Iteration 33, loss = 0.00367677\n",
      "Iteration 34, loss = 0.00345967\n",
      "Iteration 35, loss = 0.00325661\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86827069\n",
      "Iteration 2, loss = 0.47677857\n",
      "Iteration 3, loss = 0.40054934\n",
      "Iteration 4, loss = 0.33787629\n",
      "Iteration 5, loss = 0.27554081\n",
      "Iteration 6, loss = 0.24149542\n",
      "Iteration 7, loss = 0.19669946\n",
      "Iteration 8, loss = 0.17656609\n",
      "Iteration 9, loss = 0.13735752\n",
      "Iteration 10, loss = 0.11386458\n",
      "Iteration 11, loss = 0.08985551\n",
      "Iteration 12, loss = 0.07073673\n",
      "Iteration 13, loss = 0.06081471\n",
      "Iteration 14, loss = 0.04380853\n",
      "Iteration 15, loss = 0.03433668\n",
      "Iteration 16, loss = 0.03303580\n",
      "Iteration 17, loss = 0.02514494\n",
      "Iteration 18, loss = 0.01965484\n",
      "Iteration 19, loss = 0.01538050\n",
      "Iteration 20, loss = 0.01335078\n",
      "Iteration 21, loss = 0.01095079\n",
      "Iteration 22, loss = 0.00979508\n",
      "Iteration 23, loss = 0.00772847\n",
      "Iteration 24, loss = 0.00700351\n",
      "Iteration 25, loss = 0.00687879\n",
      "Iteration 26, loss = 0.00602099\n",
      "Iteration 27, loss = 0.00464956\n",
      "Iteration 28, loss = 0.00428879\n",
      "Iteration 29, loss = 0.00383610\n",
      "Iteration 30, loss = 0.00344556\n",
      "Iteration 31, loss = 0.00304178\n",
      "Iteration 32, loss = 0.00278824\n",
      "Iteration 33, loss = 0.00251665\n",
      "Iteration 34, loss = 0.00233547\n",
      "Iteration 35, loss = 0.00217607\n",
      "Iteration 36, loss = 0.00204475\n",
      "Iteration 37, loss = 0.00187255\n",
      "Iteration 38, loss = 0.00172109\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73748235\n",
      "Iteration 2, loss = 0.46546269\n",
      "Iteration 3, loss = 0.39756256\n",
      "Iteration 4, loss = 0.32477196\n",
      "Iteration 5, loss = 0.26309892\n",
      "Iteration 6, loss = 0.23706209\n",
      "Iteration 7, loss = 0.20192007\n",
      "Iteration 8, loss = 0.16149018\n",
      "Iteration 9, loss = 0.12736498\n",
      "Iteration 10, loss = 0.10075602\n",
      "Iteration 11, loss = 0.07723186\n",
      "Iteration 12, loss = 0.06059568\n",
      "Iteration 13, loss = 0.04710190\n",
      "Iteration 14, loss = 0.03436469\n",
      "Iteration 15, loss = 0.02427314\n",
      "Iteration 16, loss = 0.01957600\n",
      "Iteration 17, loss = 0.01660619\n",
      "Iteration 18, loss = 0.01343021\n",
      "Iteration 19, loss = 0.01140897\n",
      "Iteration 20, loss = 0.00800373\n",
      "Iteration 21, loss = 0.00636081\n",
      "Iteration 22, loss = 0.00553686\n",
      "Iteration 23, loss = 0.00459706\n",
      "Iteration 24, loss = 0.00401827\n",
      "Iteration 25, loss = 0.00354152\n",
      "Iteration 26, loss = 0.00312296\n",
      "Iteration 27, loss = 0.00278812\n",
      "Iteration 28, loss = 0.00253599\n",
      "Iteration 29, loss = 0.00246148\n",
      "Iteration 30, loss = 0.00233879\n",
      "Iteration 31, loss = 0.00188755\n",
      "Iteration 32, loss = 0.00176062\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69470825\n",
      "Iteration 2, loss = 0.46395447\n",
      "Iteration 3, loss = 0.38232020\n",
      "Iteration 4, loss = 0.30095154\n",
      "Iteration 5, loss = 0.24762829\n",
      "Iteration 6, loss = 0.19853151\n",
      "Iteration 7, loss = 0.15902313\n",
      "Iteration 8, loss = 0.14754711\n",
      "Iteration 9, loss = 0.09888988\n",
      "Iteration 10, loss = 0.07195363\n",
      "Iteration 11, loss = 0.05493658\n",
      "Iteration 12, loss = 0.03963831\n",
      "Iteration 13, loss = 0.02889461\n",
      "Iteration 14, loss = 0.02289457\n",
      "Iteration 15, loss = 0.01839072\n",
      "Iteration 16, loss = 0.01261633\n",
      "Iteration 17, loss = 0.01074787\n",
      "Iteration 18, loss = 0.00900808\n",
      "Iteration 19, loss = 0.00568078\n",
      "Iteration 20, loss = 0.00453177\n",
      "Iteration 21, loss = 0.00378027\n",
      "Iteration 22, loss = 0.00338692\n",
      "Iteration 23, loss = 0.00287936\n",
      "Iteration 24, loss = 0.00252777\n",
      "Iteration 25, loss = 0.00226023\n",
      "Iteration 26, loss = 0.00208417\n",
      "Iteration 27, loss = 0.00184095\n",
      "Iteration 28, loss = 0.00162690\n",
      "Iteration 29, loss = 0.00148100\n",
      "Iteration 30, loss = 0.00136145\n",
      "Iteration 31, loss = 0.00125583\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65124645\n",
      "Iteration 2, loss = 0.43816613\n",
      "Iteration 3, loss = 0.35703322\n",
      "Iteration 4, loss = 0.32373242\n",
      "Iteration 5, loss = 0.26292653\n",
      "Iteration 6, loss = 0.22795535\n",
      "Iteration 7, loss = 0.18055046\n",
      "Iteration 8, loss = 0.16103926\n",
      "Iteration 9, loss = 0.12416967\n",
      "Iteration 10, loss = 0.09234875\n",
      "Iteration 11, loss = 0.07232503\n",
      "Iteration 12, loss = 0.05676441\n",
      "Iteration 13, loss = 0.04926789\n",
      "Iteration 14, loss = 0.08282580\n",
      "Iteration 15, loss = 0.03872437\n",
      "Iteration 16, loss = 0.02077947\n",
      "Iteration 17, loss = 0.01709512\n",
      "Iteration 18, loss = 0.01205627\n",
      "Iteration 19, loss = 0.00738320\n",
      "Iteration 20, loss = 0.00563986\n",
      "Iteration 21, loss = 0.00466538\n",
      "Iteration 22, loss = 0.00365921\n",
      "Iteration 23, loss = 0.00317613\n",
      "Iteration 24, loss = 0.00268877\n",
      "Iteration 25, loss = 0.00237029\n",
      "Iteration 26, loss = 0.00201061\n",
      "Iteration 27, loss = 0.00175190\n",
      "Iteration 28, loss = 0.00158097\n",
      "Iteration 29, loss = 0.00140908\n",
      "Iteration 30, loss = 0.00128149\n",
      "Iteration 31, loss = 0.00118347\n",
      "Iteration 32, loss = 0.00105728\n",
      "Iteration 33, loss = 0.00100113\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.44446922\n",
      "Iteration 2, loss = 0.56969882\n",
      "Iteration 3, loss = 0.45281923\n",
      "Iteration 4, loss = 0.36009863\n",
      "Iteration 5, loss = 0.31451464\n",
      "Iteration 6, loss = 0.25537683\n",
      "Iteration 7, loss = 0.21786056\n",
      "Iteration 8, loss = 0.17778436\n",
      "Iteration 9, loss = 0.15307482\n",
      "Iteration 10, loss = 0.12178768\n",
      "Iteration 11, loss = 0.10587737\n",
      "Iteration 12, loss = 0.08520908\n",
      "Iteration 13, loss = 0.07054245\n",
      "Iteration 14, loss = 0.06207078\n",
      "Iteration 15, loss = 0.04883193\n",
      "Iteration 16, loss = 0.03800514\n",
      "Iteration 17, loss = 0.03305842\n",
      "Iteration 18, loss = 0.02631617\n",
      "Iteration 19, loss = 0.02236317\n",
      "Iteration 20, loss = 0.01989086\n",
      "Iteration 21, loss = 0.01570330\n",
      "Iteration 22, loss = 0.01322512\n",
      "Iteration 23, loss = 0.01199892\n",
      "Iteration 24, loss = 0.01017811\n",
      "Iteration 25, loss = 0.00880901\n",
      "Iteration 26, loss = 0.00790402\n",
      "Iteration 27, loss = 0.00706673\n",
      "Iteration 28, loss = 0.00631307\n",
      "Iteration 29, loss = 0.00565967\n",
      "Iteration 30, loss = 0.00513446\n",
      "Iteration 31, loss = 0.00474174\n",
      "Iteration 32, loss = 0.00439590\n",
      "Iteration 33, loss = 0.00404334\n",
      "Iteration 34, loss = 0.00376528\n",
      "Iteration 35, loss = 0.00349107\n",
      "Iteration 36, loss = 0.00330010\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86626563\n",
      "Iteration 2, loss = 0.47651492\n",
      "Iteration 3, loss = 0.40362098\n",
      "Iteration 4, loss = 0.31904812\n",
      "Iteration 5, loss = 0.27107308\n",
      "Iteration 6, loss = 0.26153202\n",
      "Iteration 7, loss = 0.20425236\n",
      "Iteration 8, loss = 0.17005990\n",
      "Iteration 9, loss = 0.14600417\n",
      "Iteration 10, loss = 0.11644805\n",
      "Iteration 11, loss = 0.10213446\n",
      "Iteration 12, loss = 0.10129637\n",
      "Iteration 13, loss = 0.07610908\n",
      "Iteration 14, loss = 0.05957333\n",
      "Iteration 15, loss = 0.04502268\n",
      "Iteration 16, loss = 0.04008190\n",
      "Iteration 17, loss = 0.03112990\n",
      "Iteration 18, loss = 0.02552963\n",
      "Iteration 19, loss = 0.02041379\n",
      "Iteration 20, loss = 0.02150542\n",
      "Iteration 21, loss = 0.01443441\n",
      "Iteration 22, loss = 0.01024316\n",
      "Iteration 23, loss = 0.00873787\n",
      "Iteration 24, loss = 0.00751952\n",
      "Iteration 25, loss = 0.00678918\n",
      "Iteration 26, loss = 0.00597047\n",
      "Iteration 27, loss = 0.00533700\n",
      "Iteration 28, loss = 0.00478012\n",
      "Iteration 29, loss = 0.00472075\n",
      "Iteration 30, loss = 0.00411632\n",
      "Iteration 31, loss = 0.00372885\n",
      "Iteration 32, loss = 0.00340791\n",
      "Iteration 33, loss = 0.00299385\n",
      "Iteration 34, loss = 0.00275104\n",
      "Iteration 35, loss = 0.00255286\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74521265\n",
      "Iteration 2, loss = 0.45607814\n",
      "Iteration 3, loss = 0.37163829\n",
      "Iteration 4, loss = 0.31040977\n",
      "Iteration 5, loss = 0.24760022\n",
      "Iteration 6, loss = 0.20974852\n",
      "Iteration 7, loss = 0.17615649\n",
      "Iteration 8, loss = 0.14312927\n",
      "Iteration 9, loss = 0.12107291\n",
      "Iteration 10, loss = 0.09387609\n",
      "Iteration 11, loss = 0.06694906\n",
      "Iteration 12, loss = 0.05463814\n",
      "Iteration 13, loss = 0.04449740\n",
      "Iteration 14, loss = 0.03435417\n",
      "Iteration 15, loss = 0.02708484\n",
      "Iteration 16, loss = 0.02071591\n",
      "Iteration 17, loss = 0.01597692\n",
      "Iteration 18, loss = 0.01259729\n",
      "Iteration 19, loss = 0.01060017\n",
      "Iteration 20, loss = 0.00810261\n",
      "Iteration 21, loss = 0.00693023\n",
      "Iteration 22, loss = 0.00616529\n",
      "Iteration 23, loss = 0.00486010\n",
      "Iteration 24, loss = 0.00472314\n",
      "Iteration 25, loss = 0.00466741\n",
      "Iteration 26, loss = 0.00347877\n",
      "Iteration 27, loss = 0.00293067\n",
      "Iteration 28, loss = 0.00254785\n",
      "Iteration 29, loss = 0.00235638\n",
      "Iteration 30, loss = 0.00206527\n",
      "Iteration 31, loss = 0.00187536\n",
      "Iteration 32, loss = 0.00175258\n",
      "Iteration 33, loss = 0.00159909\n",
      "Iteration 34, loss = 0.00142187\n",
      "Iteration 35, loss = 0.00132306\n",
      "Iteration 36, loss = 0.00126099\n",
      "Iteration 37, loss = 0.00115313\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67352170\n",
      "Iteration 2, loss = 0.44701290\n",
      "Iteration 3, loss = 0.35126059\n",
      "Iteration 4, loss = 0.29090726\n",
      "Iteration 5, loss = 0.23256576\n",
      "Iteration 6, loss = 0.18723528\n",
      "Iteration 7, loss = 0.15725904\n",
      "Iteration 8, loss = 0.11918302\n",
      "Iteration 9, loss = 0.09770239\n",
      "Iteration 10, loss = 0.07119685\n",
      "Iteration 11, loss = 0.06319510\n",
      "Iteration 12, loss = 0.05325913\n",
      "Iteration 13, loss = 0.03260652\n",
      "Iteration 14, loss = 0.02254744\n",
      "Iteration 15, loss = 0.01766752\n",
      "Iteration 16, loss = 0.01398531\n",
      "Iteration 17, loss = 0.01074044\n",
      "Iteration 18, loss = 0.00800195\n",
      "Iteration 19, loss = 0.00593367\n",
      "Iteration 20, loss = 0.00499326\n",
      "Iteration 21, loss = 0.00413353\n",
      "Iteration 22, loss = 0.00348059\n",
      "Iteration 23, loss = 0.00302718\n",
      "Iteration 24, loss = 0.00272731\n",
      "Iteration 25, loss = 0.00248661\n",
      "Iteration 26, loss = 0.00203944\n",
      "Iteration 27, loss = 0.00180454\n",
      "Iteration 28, loss = 0.00157555\n",
      "Iteration 29, loss = 0.00141000\n",
      "Iteration 30, loss = 0.00129140\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64608067\n",
      "Iteration 2, loss = 0.43666140\n",
      "Iteration 3, loss = 0.36126283\n",
      "Iteration 4, loss = 0.29943706\n",
      "Iteration 5, loss = 0.24647010\n",
      "Iteration 6, loss = 0.20158983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.18099181\n",
      "Iteration 8, loss = 0.13897199\n",
      "Iteration 9, loss = 0.11135557\n",
      "Iteration 10, loss = 0.08224299\n",
      "Iteration 11, loss = 0.06628631\n",
      "Iteration 12, loss = 0.05707074\n",
      "Iteration 13, loss = 0.05060329\n",
      "Iteration 14, loss = 0.03841034\n",
      "Iteration 15, loss = 0.04861365\n",
      "Iteration 16, loss = 0.02363028\n",
      "Iteration 17, loss = 0.01282690\n",
      "Iteration 18, loss = 0.00877688\n",
      "Iteration 19, loss = 0.00915660\n",
      "Iteration 20, loss = 0.00610163\n",
      "Iteration 21, loss = 0.00413768\n",
      "Iteration 22, loss = 0.00364351\n",
      "Iteration 23, loss = 0.00337123\n",
      "Iteration 24, loss = 0.00230962\n",
      "Iteration 25, loss = 0.00201061\n",
      "Iteration 26, loss = 0.00180531\n",
      "Iteration 27, loss = 0.00162947\n",
      "Iteration 28, loss = 0.00150878\n",
      "Iteration 29, loss = 0.00135161\n",
      "Iteration 30, loss = 0.00121364\n",
      "Iteration 31, loss = 0.00112312\n",
      "Iteration 32, loss = 0.00105729\n",
      "Iteration 33, loss = 0.00099032\n",
      "Iteration 34, loss = 0.00088600\n",
      "Iteration 35, loss = 0.00080973\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.40039888\n",
      "Iteration 2, loss = 0.60046311\n",
      "Iteration 3, loss = 0.42516743\n",
      "Iteration 4, loss = 0.35744235\n",
      "Iteration 5, loss = 0.29030410\n",
      "Iteration 6, loss = 0.25693144\n",
      "Iteration 7, loss = 0.21488685\n",
      "Iteration 8, loss = 0.17697119\n",
      "Iteration 9, loss = 0.14199213\n",
      "Iteration 10, loss = 0.11968255\n",
      "Iteration 11, loss = 0.09496968\n",
      "Iteration 12, loss = 0.07718036\n",
      "Iteration 13, loss = 0.06608234\n",
      "Iteration 14, loss = 0.05180413\n",
      "Iteration 15, loss = 0.04354270\n",
      "Iteration 16, loss = 0.03533496\n",
      "Iteration 17, loss = 0.02933606\n",
      "Iteration 18, loss = 0.02460903\n",
      "Iteration 19, loss = 0.01993677\n",
      "Iteration 20, loss = 0.01645677\n",
      "Iteration 21, loss = 0.01414701\n",
      "Iteration 22, loss = 0.01209384\n",
      "Iteration 23, loss = 0.01037142\n",
      "Iteration 24, loss = 0.00921956\n",
      "Iteration 25, loss = 0.00804572\n",
      "Iteration 26, loss = 0.00717221\n",
      "Iteration 27, loss = 0.00641990\n",
      "Iteration 28, loss = 0.00597341\n",
      "Iteration 29, loss = 0.00538863\n",
      "Iteration 30, loss = 0.00485309\n",
      "Iteration 31, loss = 0.00451393\n",
      "Iteration 32, loss = 0.00424766\n",
      "Iteration 33, loss = 0.00395240\n",
      "Iteration 34, loss = 0.00365683\n",
      "Iteration 35, loss = 0.00341382\n",
      "Iteration 36, loss = 0.00323414\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.92956353\n",
      "Iteration 2, loss = 0.50966818\n",
      "Iteration 3, loss = 0.42418060\n",
      "Iteration 4, loss = 0.35372148\n",
      "Iteration 5, loss = 0.29269014\n",
      "Iteration 6, loss = 0.25078775\n",
      "Iteration 7, loss = 0.22615397\n",
      "Iteration 8, loss = 0.18370976\n",
      "Iteration 9, loss = 0.15661818\n",
      "Iteration 10, loss = 0.12840436\n",
      "Iteration 11, loss = 0.10056712\n",
      "Iteration 12, loss = 0.08568446\n",
      "Iteration 13, loss = 0.06590535\n",
      "Iteration 14, loss = 0.05724258\n",
      "Iteration 15, loss = 0.04304195\n",
      "Iteration 16, loss = 0.03611838\n",
      "Iteration 17, loss = 0.02993337\n",
      "Iteration 18, loss = 0.02339535\n",
      "Iteration 19, loss = 0.01773704\n",
      "Iteration 20, loss = 0.01482385\n",
      "Iteration 21, loss = 0.01298514\n",
      "Iteration 22, loss = 0.01252463\n",
      "Iteration 23, loss = 0.00891102\n",
      "Iteration 24, loss = 0.00754264\n",
      "Iteration 25, loss = 0.00624616\n",
      "Iteration 26, loss = 0.00574210\n",
      "Iteration 27, loss = 0.00505265\n",
      "Iteration 28, loss = 0.00440057\n",
      "Iteration 29, loss = 0.00438445\n",
      "Iteration 30, loss = 0.00394165\n",
      "Iteration 31, loss = 0.00338874\n",
      "Iteration 32, loss = 0.00312374\n",
      "Iteration 33, loss = 0.00269692\n",
      "Iteration 34, loss = 0.00246256\n",
      "Iteration 35, loss = 0.00227202\n",
      "Iteration 36, loss = 0.00209768\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73829233\n",
      "Iteration 2, loss = 0.47431792\n",
      "Iteration 3, loss = 0.38213046\n",
      "Iteration 4, loss = 0.30924059\n",
      "Iteration 5, loss = 0.25731492\n",
      "Iteration 6, loss = 0.21148239\n",
      "Iteration 7, loss = 0.17807838\n",
      "Iteration 8, loss = 0.13473983\n",
      "Iteration 9, loss = 0.10447018\n",
      "Iteration 10, loss = 0.08852173\n",
      "Iteration 11, loss = 0.06463727\n",
      "Iteration 12, loss = 0.04671563\n",
      "Iteration 13, loss = 0.03704741\n",
      "Iteration 14, loss = 0.03433027\n",
      "Iteration 15, loss = 0.02318455\n",
      "Iteration 16, loss = 0.02007506\n",
      "Iteration 17, loss = 0.01431234\n",
      "Iteration 18, loss = 0.01121849\n",
      "Iteration 19, loss = 0.00855730\n",
      "Iteration 20, loss = 0.00676561\n",
      "Iteration 21, loss = 0.00569323\n",
      "Iteration 22, loss = 0.00450451\n",
      "Iteration 23, loss = 0.00398271\n",
      "Iteration 24, loss = 0.00335237\n",
      "Iteration 25, loss = 0.00309518\n",
      "Iteration 26, loss = 0.00268836\n",
      "Iteration 27, loss = 0.00244886\n",
      "Iteration 28, loss = 0.00213841\n",
      "Iteration 29, loss = 0.00192652\n",
      "Iteration 30, loss = 0.00174572\n",
      "Iteration 31, loss = 0.00161403\n",
      "Iteration 32, loss = 0.00148453\n",
      "Iteration 33, loss = 0.00140241\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70472908\n",
      "Iteration 2, loss = 0.46489548\n",
      "Iteration 3, loss = 0.37890273\n",
      "Iteration 4, loss = 0.31088062\n",
      "Iteration 5, loss = 0.25848663\n",
      "Iteration 6, loss = 0.21235949\n",
      "Iteration 7, loss = 0.16844289\n",
      "Iteration 8, loss = 0.13420487\n",
      "Iteration 9, loss = 0.10350786\n",
      "Iteration 10, loss = 0.07859932\n",
      "Iteration 11, loss = 0.06671301\n",
      "Iteration 12, loss = 0.05231091\n",
      "Iteration 13, loss = 0.03614395\n",
      "Iteration 14, loss = 0.02219916\n",
      "Iteration 15, loss = 0.01746527\n",
      "Iteration 16, loss = 0.01410973\n",
      "Iteration 17, loss = 0.01044284\n",
      "Iteration 18, loss = 0.00804792\n",
      "Iteration 19, loss = 0.00685003\n",
      "Iteration 20, loss = 0.00588527\n",
      "Iteration 21, loss = 0.00438457\n",
      "Iteration 22, loss = 0.00366059\n",
      "Iteration 23, loss = 0.00330219\n",
      "Iteration 24, loss = 0.00275549\n",
      "Iteration 25, loss = 0.00258491\n",
      "Iteration 26, loss = 0.00213740\n",
      "Iteration 27, loss = 0.00195534\n",
      "Iteration 28, loss = 0.00177027\n",
      "Iteration 29, loss = 0.00160065\n",
      "Iteration 30, loss = 0.00149375\n",
      "Iteration 31, loss = 0.00135257\n",
      "Iteration 32, loss = 0.00127168\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66272263\n",
      "Iteration 2, loss = 0.43639964\n",
      "Iteration 3, loss = 0.36388374\n",
      "Iteration 4, loss = 0.31593127\n",
      "Iteration 5, loss = 0.25896373\n",
      "Iteration 6, loss = 0.21274130\n",
      "Iteration 7, loss = 0.19033362\n",
      "Iteration 8, loss = 0.15465195\n",
      "Iteration 9, loss = 0.11371021\n",
      "Iteration 10, loss = 0.09533656\n",
      "Iteration 11, loss = 0.07031262\n",
      "Iteration 12, loss = 0.06251410\n",
      "Iteration 13, loss = 0.05572968\n",
      "Iteration 14, loss = 0.05088626\n",
      "Iteration 15, loss = 0.02600128\n",
      "Iteration 16, loss = 0.01518307\n",
      "Iteration 17, loss = 0.01113949\n",
      "Iteration 18, loss = 0.00802684\n",
      "Iteration 19, loss = 0.00820592\n",
      "Iteration 20, loss = 0.00604370\n",
      "Iteration 21, loss = 0.00464651\n",
      "Iteration 22, loss = 0.00387157\n",
      "Iteration 23, loss = 0.00293221\n",
      "Iteration 24, loss = 0.00234135\n",
      "Iteration 25, loss = 0.00197767\n",
      "Iteration 26, loss = 0.00175295\n",
      "Iteration 27, loss = 0.00160629\n",
      "Iteration 28, loss = 0.00148333\n",
      "Iteration 29, loss = 0.00128372\n",
      "Iteration 30, loss = 0.00115806\n",
      "Iteration 31, loss = 0.00111200\n",
      "Iteration 32, loss = 0.00099336\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.40514508\n",
      "Iteration 2, loss = 0.58127305\n",
      "Iteration 3, loss = 0.43042092\n",
      "Iteration 4, loss = 0.36142348\n",
      "Iteration 5, loss = 0.30074560\n",
      "Iteration 6, loss = 0.25745766\n",
      "Iteration 7, loss = 0.20815123\n",
      "Iteration 8, loss = 0.17667906\n",
      "Iteration 9, loss = 0.13925974\n",
      "Iteration 10, loss = 0.11645487\n",
      "Iteration 11, loss = 0.08910706\n",
      "Iteration 12, loss = 0.07708291\n",
      "Iteration 13, loss = 0.05894592\n",
      "Iteration 14, loss = 0.05071494\n",
      "Iteration 15, loss = 0.03832017\n",
      "Iteration 16, loss = 0.03247957\n",
      "Iteration 17, loss = 0.02714834\n",
      "Iteration 18, loss = 0.02080622\n",
      "Iteration 19, loss = 0.01841239\n",
      "Iteration 20, loss = 0.01460958\n",
      "Iteration 21, loss = 0.01244793\n",
      "Iteration 22, loss = 0.01093862\n",
      "Iteration 23, loss = 0.00903544\n",
      "Iteration 24, loss = 0.00786531\n",
      "Iteration 25, loss = 0.00713322\n",
      "Iteration 26, loss = 0.00626111\n",
      "Iteration 27, loss = 0.00554547\n",
      "Iteration 28, loss = 0.00500167\n",
      "Iteration 29, loss = 0.00459692\n",
      "Iteration 30, loss = 0.00423550\n",
      "Iteration 31, loss = 0.00389295\n",
      "Iteration 32, loss = 0.00359215\n",
      "Iteration 33, loss = 0.00333637\n",
      "Iteration 34, loss = 0.00312052\n",
      "Iteration 35, loss = 0.00294037\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.88266594\n",
      "Iteration 2, loss = 0.49525564\n",
      "Iteration 3, loss = 0.40894592\n",
      "Iteration 4, loss = 0.34139608\n",
      "Iteration 5, loss = 0.28356117\n",
      "Iteration 6, loss = 0.26503538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.20875416\n",
      "Iteration 8, loss = 0.19565092\n",
      "Iteration 9, loss = 0.14922929\n",
      "Iteration 10, loss = 0.11824725\n",
      "Iteration 11, loss = 0.09326842\n",
      "Iteration 12, loss = 0.08009609\n",
      "Iteration 13, loss = 0.07001236\n",
      "Iteration 14, loss = 0.05502327\n",
      "Iteration 15, loss = 0.03949102\n",
      "Iteration 16, loss = 0.03080149\n",
      "Iteration 17, loss = 0.02643118\n",
      "Iteration 18, loss = 0.01988451\n",
      "Iteration 19, loss = 0.01660656\n",
      "Iteration 20, loss = 0.01614649\n",
      "Iteration 21, loss = 0.01395937\n",
      "Iteration 22, loss = 0.01119722\n",
      "Iteration 23, loss = 0.00948281\n",
      "Iteration 24, loss = 0.00752528\n",
      "Iteration 25, loss = 0.00648496\n",
      "Iteration 26, loss = 0.00565609\n",
      "Iteration 27, loss = 0.00457090\n",
      "Iteration 28, loss = 0.00409490\n",
      "Iteration 29, loss = 0.00369466\n",
      "Iteration 30, loss = 0.00330842\n",
      "Iteration 31, loss = 0.00303272\n",
      "Iteration 32, loss = 0.00275814\n",
      "Iteration 33, loss = 0.00259276\n",
      "Iteration 34, loss = 0.00242696\n",
      "Iteration 35, loss = 0.00228553\n",
      "Iteration 36, loss = 0.00209154\n",
      "Iteration 37, loss = 0.00191265\n",
      "Iteration 38, loss = 0.00178429\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76817605\n",
      "Iteration 2, loss = 0.49006530\n",
      "Iteration 3, loss = 0.39919148\n",
      "Iteration 4, loss = 0.32957316\n",
      "Iteration 5, loss = 0.27793441\n",
      "Iteration 6, loss = 0.23302175\n",
      "Iteration 7, loss = 0.18604013\n",
      "Iteration 8, loss = 0.14771631\n",
      "Iteration 9, loss = 0.12132853\n",
      "Iteration 10, loss = 0.09217266\n",
      "Iteration 11, loss = 0.07574366\n",
      "Iteration 12, loss = 0.06834984\n",
      "Iteration 13, loss = 0.05560879\n",
      "Iteration 14, loss = 0.03745569\n",
      "Iteration 15, loss = 0.03053101\n",
      "Iteration 16, loss = 0.02178443\n",
      "Iteration 17, loss = 0.01829286\n",
      "Iteration 18, loss = 0.01713417\n",
      "Iteration 19, loss = 0.01336610\n",
      "Iteration 20, loss = 0.00999358\n",
      "Iteration 21, loss = 0.00775266\n",
      "Iteration 22, loss = 0.00647632\n",
      "Iteration 23, loss = 0.00529390\n",
      "Iteration 24, loss = 0.00469226\n",
      "Iteration 25, loss = 0.00394185\n",
      "Iteration 26, loss = 0.00334410\n",
      "Iteration 27, loss = 0.00300689\n",
      "Iteration 28, loss = 0.00271564\n",
      "Iteration 29, loss = 0.00257621\n",
      "Iteration 30, loss = 0.00225290\n",
      "Iteration 31, loss = 0.00204482\n",
      "Iteration 32, loss = 0.00189672\n",
      "Iteration 33, loss = 0.00173826\n",
      "Iteration 34, loss = 0.00159649\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68533434\n",
      "Iteration 2, loss = 0.46042486\n",
      "Iteration 3, loss = 0.36925410\n",
      "Iteration 4, loss = 0.30265235\n",
      "Iteration 5, loss = 0.24742444\n",
      "Iteration 6, loss = 0.20951086\n",
      "Iteration 7, loss = 0.16659676\n",
      "Iteration 8, loss = 0.13412769\n",
      "Iteration 9, loss = 0.09983226\n",
      "Iteration 10, loss = 0.10028391\n",
      "Iteration 11, loss = 0.07077617\n",
      "Iteration 12, loss = 0.05085827\n",
      "Iteration 13, loss = 0.04231906\n",
      "Iteration 14, loss = 0.02914380\n",
      "Iteration 15, loss = 0.01985301\n",
      "Iteration 16, loss = 0.01382444\n",
      "Iteration 17, loss = 0.01050866\n",
      "Iteration 18, loss = 0.00825356\n",
      "Iteration 19, loss = 0.00693536\n",
      "Iteration 20, loss = 0.00563395\n",
      "Iteration 21, loss = 0.00484780\n",
      "Iteration 22, loss = 0.00435287\n",
      "Iteration 23, loss = 0.00367876\n",
      "Iteration 24, loss = 0.00316510\n",
      "Iteration 25, loss = 0.00277937\n",
      "Iteration 26, loss = 0.00249797\n",
      "Iteration 27, loss = 0.00226355\n",
      "Iteration 28, loss = 0.00206849\n",
      "Iteration 29, loss = 0.00188349\n",
      "Iteration 30, loss = 0.00165120\n",
      "Iteration 31, loss = 0.00159238\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64518078\n",
      "Iteration 2, loss = 0.43978139\n",
      "Iteration 3, loss = 0.35470816\n",
      "Iteration 4, loss = 0.30046554\n",
      "Iteration 5, loss = 0.25404721\n",
      "Iteration 6, loss = 0.20650216\n",
      "Iteration 7, loss = 0.16569440\n",
      "Iteration 8, loss = 0.12870112\n",
      "Iteration 9, loss = 0.11035147\n",
      "Iteration 10, loss = 0.08735811\n",
      "Iteration 11, loss = 0.06437242\n",
      "Iteration 12, loss = 0.04494931\n",
      "Iteration 13, loss = 0.03694953\n",
      "Iteration 14, loss = 0.03826037\n",
      "Iteration 15, loss = 0.01933430\n",
      "Iteration 16, loss = 0.01356753\n",
      "Iteration 17, loss = 0.00962155\n",
      "Iteration 18, loss = 0.00838938\n",
      "Iteration 19, loss = 0.00902758\n",
      "Iteration 20, loss = 0.00619583\n",
      "Iteration 21, loss = 0.00374679\n",
      "Iteration 22, loss = 0.00291875\n",
      "Iteration 23, loss = 0.00252000\n",
      "Iteration 24, loss = 0.00216192\n",
      "Iteration 25, loss = 0.00191939\n",
      "Iteration 26, loss = 0.00167729\n",
      "Iteration 27, loss = 0.00152002\n",
      "Iteration 28, loss = 0.00134425\n",
      "Iteration 29, loss = 0.00124866\n",
      "Iteration 30, loss = 0.00111269\n",
      "Iteration 31, loss = 0.00103421\n",
      "Iteration 32, loss = 0.00094706\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'matplotlib.pyplot' from '/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py'>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1147a0588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standardize the full data kek\n",
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "'''\n",
    "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n",
    "              beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
    "              epsilon=1e-08, hidden_layer_sizes=(15,),\n",
    "              learning_rate='constant', learning_rate_init=0.001,\n",
    "              max_iter=200, momentum=0.9, n_iter_no_change=10,\n",
    "              nesterovs_momentum=True, power_t=0.5,  random_state=1,\n",
    "              shuffle=True, solver='lbfgs', tol=0.0001,\n",
    "              validation_fraction=0.1, verbose=False, warm_start=False)\n",
    "\n",
    "'''\n",
    "estimator = MLPClassifier(solver='adam', alpha=1e-5,\n",
    "                       hidden_layer_sizes=(150,150), random_state=1,\n",
    "                        activation='relu', learning_rate='adaptive', beta_1=0.9, \n",
    "                          beta_2=0.999, learning_rate_init=0.001, verbose=True,\n",
    "                          power_t=0.5, tol=0.001\n",
    "                      )\n",
    "# train_sizes, train_scores, valid_scores = learning_curve(estimator, X_standardized, y, cv=kf)\n",
    "plot_learning_curve(estimator, \"NN Learning Curves\", X_standardized, y, ylim=None, cv=kf,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5))\n",
    "\n",
    "# Observations: 'sgd' too slow and not better results, 'adaptive' gradient seems best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63758719\n",
      "Iteration 2, loss = 0.44476393\n",
      "Iteration 3, loss = 0.36164996\n",
      "Iteration 4, loss = 0.30036180\n",
      "Iteration 5, loss = 0.25208929\n",
      "Iteration 6, loss = 0.22703049\n",
      "Iteration 7, loss = 0.19127241\n",
      "Iteration 8, loss = 0.14640694\n",
      "Iteration 9, loss = 0.15086179\n",
      "Iteration 10, loss = 0.09755705\n",
      "Iteration 11, loss = 0.07457444\n",
      "Iteration 12, loss = 0.06040634\n",
      "Iteration 13, loss = 0.04702093\n",
      "Iteration 14, loss = 0.04167514\n",
      "Iteration 15, loss = 0.02581442\n",
      "Iteration 16, loss = 0.02248644\n",
      "Iteration 17, loss = 0.01489569\n",
      "Iteration 18, loss = 0.01100080\n",
      "Iteration 19, loss = 0.00791699\n",
      "Iteration 20, loss = 0.00552660\n",
      "Iteration 21, loss = 0.00440576\n",
      "Iteration 22, loss = 0.00355139\n",
      "Iteration 23, loss = 0.00296610\n",
      "Iteration 24, loss = 0.00255770\n",
      "Iteration 25, loss = 0.00225298\n",
      "Iteration 26, loss = 0.00208030\n",
      "Iteration 27, loss = 0.00195597\n",
      "Iteration 28, loss = 0.00166280\n",
      "Iteration 29, loss = 0.00154138\n",
      "Iteration 30, loss = 0.00127129\n",
      "Iteration 31, loss = 0.00116857\n",
      "Iteration 32, loss = 0.00108090\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85509731\n",
      "Iteration 2, loss = 0.65979011\n",
      "Iteration 3, loss = 0.57367713\n",
      "Iteration 4, loss = 0.51108657\n",
      "Iteration 5, loss = 0.45958539\n",
      "Iteration 6, loss = 0.43235637\n",
      "Iteration 7, loss = 0.38995108\n",
      "Iteration 8, loss = 0.34112313\n",
      "Iteration 9, loss = 0.35012762\n",
      "Iteration 10, loss = 0.29303127\n",
      "Iteration 11, loss = 0.26490348\n",
      "Iteration 12, loss = 0.24606911\n",
      "Iteration 13, loss = 0.23263879\n",
      "Iteration 14, loss = 0.23277826\n",
      "Iteration 15, loss = 0.20967406\n",
      "Iteration 16, loss = 0.20061883\n",
      "Iteration 17, loss = 0.19035537\n",
      "Iteration 18, loss = 0.17981785\n",
      "Iteration 19, loss = 0.18334221\n",
      "Iteration 20, loss = 0.17077413\n",
      "Iteration 21, loss = 0.16147156\n",
      "Iteration 22, loss = 0.15622884\n",
      "Iteration 23, loss = 0.15165371\n",
      "Iteration 24, loss = 0.14730163\n",
      "Iteration 25, loss = 0.14318669\n",
      "Iteration 26, loss = 0.13976468\n",
      "Iteration 27, loss = 0.13592732\n",
      "Iteration 28, loss = 0.13243264\n",
      "Iteration 29, loss = 0.12857263\n",
      "Iteration 30, loss = 0.12460973\n",
      "Iteration 31, loss = 0.12120723\n",
      "Iteration 32, loss = 0.11774494\n",
      "Iteration 33, loss = 0.11448397\n",
      "Iteration 34, loss = 0.11147661\n",
      "Iteration 35, loss = 0.10849500\n",
      "Iteration 36, loss = 0.10555029\n",
      "Iteration 37, loss = 0.10284657\n",
      "Iteration 38, loss = 0.10010603\n",
      "Iteration 39, loss = 0.09783219\n",
      "Iteration 40, loss = 0.09487948\n",
      "Iteration 41, loss = 0.09268771\n",
      "Iteration 42, loss = 0.08998079\n",
      "Iteration 43, loss = 0.08786662\n",
      "Iteration 44, loss = 0.08626621\n",
      "Iteration 45, loss = 0.08314912\n",
      "Iteration 46, loss = 0.08096961\n",
      "Iteration 47, loss = 0.07914083\n",
      "Iteration 48, loss = 0.07732060\n",
      "Iteration 49, loss = 0.07577466\n",
      "Iteration 50, loss = 0.07377707\n",
      "Iteration 51, loss = 0.07172292\n",
      "Iteration 52, loss = 0.06975964\n",
      "Iteration 53, loss = 0.66438317\n",
      "Iteration 54, loss = 0.41195627\n",
      "Iteration 55, loss = 0.32160655\n",
      "Iteration 56, loss = 0.27729726\n",
      "Iteration 57, loss = 0.18951069\n",
      "Iteration 58, loss = 0.16630149\n",
      "Iteration 59, loss = 0.13840151\n",
      "Iteration 60, loss = 0.11278117\n",
      "Iteration 61, loss = 0.10149843\n",
      "Iteration 62, loss = 0.09528233\n",
      "Iteration 63, loss = 0.09192003\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.07015494\n",
      "Iteration 2, loss = 0.86785364\n",
      "Iteration 3, loss = 0.76818747\n",
      "Iteration 4, loss = 0.69284872\n",
      "Iteration 5, loss = 0.63113938\n",
      "Iteration 6, loss = 0.58965983\n",
      "Iteration 7, loss = 0.53936570\n",
      "Iteration 8, loss = 0.48114926\n",
      "Iteration 9, loss = 0.47375171\n",
      "Iteration 10, loss = 0.42372586\n",
      "Iteration 11, loss = 0.39067293\n",
      "Iteration 12, loss = 0.36074402\n",
      "Iteration 13, loss = 0.34685322\n",
      "Iteration 14, loss = 0.34065044\n",
      "Iteration 15, loss = 0.30816278\n",
      "Iteration 16, loss = 0.29327287\n",
      "Iteration 17, loss = 0.27026131\n",
      "Iteration 18, loss = 0.25605016\n",
      "Iteration 19, loss = 0.24893073\n",
      "Iteration 20, loss = 0.23614987\n",
      "Iteration 21, loss = 0.22264405\n",
      "Iteration 22, loss = 0.21316878\n",
      "Iteration 23, loss = 0.20424190\n",
      "Iteration 24, loss = 0.19621576\n",
      "Iteration 25, loss = 0.18892860\n",
      "Iteration 26, loss = 0.18271364\n",
      "Iteration 27, loss = 0.17457451\n",
      "Iteration 28, loss = 0.16824850\n",
      "Iteration 29, loss = 0.16156856\n",
      "Iteration 30, loss = 0.15546077\n",
      "Iteration 31, loss = 0.14986514\n",
      "Iteration 32, loss = 0.14451513\n",
      "Iteration 33, loss = 0.13954398\n",
      "Iteration 34, loss = 0.13624597\n",
      "Iteration 35, loss = 0.13311946\n",
      "Iteration 36, loss = 0.15081214\n",
      "Iteration 37, loss = 0.39682942\n",
      "Iteration 38, loss = 0.28724670\n",
      "Iteration 39, loss = 0.23001290\n",
      "Iteration 40, loss = 0.17780457\n",
      "Iteration 41, loss = 0.15861893\n",
      "Iteration 42, loss = 0.14702668\n",
      "Iteration 43, loss = 0.13748772\n",
      "Iteration 44, loss = 0.12963694\n",
      "Iteration 45, loss = 0.12483423\n",
      "Iteration 46, loss = 0.12063639\n",
      "Iteration 47, loss = 0.11705795\n",
      "Iteration 48, loss = 0.11394470\n",
      "Iteration 49, loss = 0.11058748\n",
      "Iteration 50, loss = 0.10689790\n",
      "Iteration 51, loss = 0.10372584\n",
      "Iteration 52, loss = 0.10103139\n",
      "Iteration 53, loss = 0.09837055\n",
      "Iteration 54, loss = 0.09571273\n",
      "Iteration 55, loss = 0.09340594\n",
      "Iteration 56, loss = 0.09100955\n",
      "Iteration 57, loss = 0.08926249\n",
      "Iteration 58, loss = 0.08687489\n",
      "Iteration 59, loss = 0.08561802\n",
      "Iteration 60, loss = 0.08364688\n",
      "Iteration 61, loss = 0.08154543\n",
      "Iteration 62, loss = 0.07948425\n",
      "Iteration 63, loss = 0.07794133\n",
      "Iteration 64, loss = 0.07627236\n",
      "Iteration 65, loss = 0.07450511\n",
      "Iteration 66, loss = 0.07323962\n",
      "Iteration 67, loss = 0.07192646\n",
      "Iteration 68, loss = 0.07262412\n",
      "Iteration 69, loss = 0.07006622\n",
      "Iteration 70, loss = 0.06815678\n",
      "Iteration 71, loss = 0.06696885\n",
      "Iteration 72, loss = 0.06647782\n",
      "Iteration 73, loss = 0.06575509\n",
      "Iteration 74, loss = 0.06550198\n",
      "Iteration 75, loss = 0.06641187\n",
      "Iteration 76, loss = 0.06345817\n",
      "Iteration 77, loss = 0.06154399\n",
      "Iteration 78, loss = 0.06125665\n",
      "Iteration 79, loss = 0.06720262\n",
      "Iteration 80, loss = 0.06422964\n",
      "Iteration 81, loss = 0.13232874\n",
      "Iteration 82, loss = 0.23752611\n",
      "Iteration 83, loss = 0.19409055\n",
      "Iteration 84, loss = 0.15891435\n",
      "Iteration 85, loss = 0.10725080\n",
      "Iteration 86, loss = 0.09021934\n",
      "Iteration 87, loss = 0.08304383\n",
      "Iteration 88, loss = 0.07824119\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.28329102\n",
      "Iteration 2, loss = 1.06193836\n",
      "Iteration 3, loss = 0.94857779\n",
      "Iteration 4, loss = 0.85555461\n",
      "Iteration 5, loss = 0.77689476\n",
      "Iteration 6, loss = 0.71893669\n",
      "Iteration 7, loss = 0.65908959\n",
      "Iteration 8, loss = 0.58874559\n",
      "Iteration 9, loss = 0.57578907\n",
      "Iteration 10, loss = 0.50986827\n",
      "Iteration 11, loss = 0.47740348\n",
      "Iteration 12, loss = 0.44710775\n",
      "Iteration 13, loss = 0.41375424\n",
      "Iteration 14, loss = 0.40855191\n",
      "Iteration 15, loss = 0.36837400\n",
      "Iteration 16, loss = 0.34158041\n",
      "Iteration 17, loss = 0.31592454\n",
      "Iteration 18, loss = 0.29684191\n",
      "Iteration 19, loss = 0.28726249\n",
      "Iteration 20, loss = 0.27488151\n",
      "Iteration 21, loss = 0.25976800\n",
      "Iteration 22, loss = 0.24248909\n",
      "Iteration 23, loss = 0.23162930\n",
      "Iteration 24, loss = 0.21961691\n",
      "Iteration 25, loss = 0.20937052\n",
      "Iteration 26, loss = 0.20219233\n",
      "Iteration 27, loss = 0.19489125\n",
      "Iteration 28, loss = 0.18960471\n",
      "Iteration 29, loss = 0.18245389\n",
      "Iteration 30, loss = 0.17420627\n",
      "Iteration 31, loss = 0.16765490\n",
      "Iteration 32, loss = 0.16306578\n",
      "Iteration 33, loss = 0.15477849\n",
      "Iteration 34, loss = 0.14909872\n",
      "Iteration 35, loss = 0.14407955\n",
      "Iteration 36, loss = 0.14282236\n",
      "Iteration 37, loss = 0.13834813\n",
      "Iteration 38, loss = 0.14681533\n",
      "Iteration 39, loss = 0.27443881\n",
      "Iteration 40, loss = 0.23726113\n",
      "Iteration 41, loss = 0.25368331\n",
      "Iteration 42, loss = 0.20659396\n",
      "Iteration 43, loss = 0.16255841\n",
      "Iteration 44, loss = 0.14591732\n",
      "Iteration 45, loss = 0.14200649\n",
      "Iteration 46, loss = 0.13106192\n",
      "Iteration 47, loss = 0.12626753\n",
      "Iteration 48, loss = 0.12116963\n",
      "Iteration 49, loss = 0.11651563\n",
      "Iteration 50, loss = 0.11195562\n",
      "Iteration 51, loss = 0.10817574\n",
      "Iteration 52, loss = 0.10507722\n",
      "Iteration 53, loss = 0.10234744\n",
      "Iteration 54, loss = 0.09965525\n",
      "Iteration 55, loss = 0.09724007\n",
      "Iteration 56, loss = 0.09532468\n",
      "Iteration 57, loss = 0.09443718\n",
      "Iteration 58, loss = 0.09301756\n",
      "Iteration 59, loss = 0.09057574\n",
      "Iteration 60, loss = 0.08827160\n",
      "Iteration 61, loss = 0.08686046\n",
      "Iteration 62, loss = 0.08435903\n",
      "Iteration 63, loss = 0.08473296\n",
      "Iteration 64, loss = 0.08244015\n",
      "Iteration 65, loss = 0.08094512\n",
      "Iteration 66, loss = 0.08003322\n",
      "Iteration 67, loss = 0.07860067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68, loss = 0.07869730\n",
      "Iteration 69, loss = 0.07682607\n",
      "Iteration 70, loss = 0.07616688\n",
      "Iteration 71, loss = 0.07438074\n",
      "Iteration 72, loss = 0.07396734\n",
      "Iteration 73, loss = 0.07319760\n",
      "Iteration 74, loss = 0.07324793\n",
      "Iteration 75, loss = 0.07338587\n",
      "Iteration 76, loss = 0.07659520\n",
      "Iteration 77, loss = 0.08244518\n",
      "Iteration 78, loss = 0.08291677\n",
      "Iteration 79, loss = 0.08575447\n",
      "Iteration 80, loss = 0.10149922\n",
      "Iteration 81, loss = 0.13745271\n",
      "Iteration 82, loss = 0.14355014\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.49454165\n",
      "Iteration 2, loss = 1.25234591\n",
      "Iteration 3, loss = 1.11086330\n",
      "Iteration 4, loss = 0.99631687\n",
      "Iteration 5, loss = 0.89898330\n",
      "Iteration 6, loss = 0.82925856\n",
      "Iteration 7, loss = 0.75385596\n",
      "Iteration 8, loss = 0.66993039\n",
      "Iteration 9, loss = 0.64087516\n",
      "Iteration 10, loss = 0.57845741\n",
      "Iteration 11, loss = 0.52926406\n",
      "Iteration 12, loss = 0.48724790\n",
      "Iteration 13, loss = 0.45671357\n",
      "Iteration 14, loss = 0.45018994\n",
      "Iteration 15, loss = 0.41644576\n",
      "Iteration 16, loss = 0.38435146\n",
      "Iteration 17, loss = 0.34723529\n",
      "Iteration 18, loss = 0.32895522\n",
      "Iteration 19, loss = 0.31964115\n",
      "Iteration 20, loss = 0.31470050\n",
      "Iteration 21, loss = 0.28672562\n",
      "Iteration 22, loss = 0.26399106\n",
      "Iteration 23, loss = 0.24993588\n",
      "Iteration 24, loss = 0.23609609\n",
      "Iteration 25, loss = 0.22375527\n",
      "Iteration 26, loss = 0.21763032\n",
      "Iteration 27, loss = 0.20734404\n",
      "Iteration 28, loss = 0.20365349\n",
      "Iteration 29, loss = 0.19805400\n",
      "Iteration 30, loss = 0.18996919\n",
      "Iteration 31, loss = 0.18058958\n",
      "Iteration 32, loss = 0.17265150\n",
      "Iteration 33, loss = 0.16315462\n",
      "Iteration 34, loss = 0.15917121\n",
      "Iteration 35, loss = 0.15693771\n",
      "Iteration 36, loss = 0.15588002\n",
      "Iteration 37, loss = 0.16149460\n",
      "Iteration 38, loss = 0.15899103\n",
      "Iteration 39, loss = 0.15978015\n",
      "Iteration 40, loss = 0.28117341\n",
      "Iteration 41, loss = 0.24106860\n",
      "Iteration 42, loss = 0.19179788\n",
      "Iteration 43, loss = 0.16334763\n",
      "Iteration 44, loss = 0.15234756\n",
      "Iteration 45, loss = 0.14040900\n",
      "Iteration 46, loss = 0.13319384\n",
      "Iteration 47, loss = 0.12857749\n",
      "Iteration 48, loss = 0.12388032\n",
      "Iteration 49, loss = 0.11991518\n",
      "Iteration 50, loss = 0.11611879\n",
      "Iteration 51, loss = 0.11263929\n",
      "Iteration 52, loss = 0.11021817\n",
      "Iteration 53, loss = 0.10909664\n",
      "Iteration 54, loss = 0.10706267\n",
      "Iteration 55, loss = 0.10512183\n",
      "Iteration 56, loss = 0.10473029\n",
      "Iteration 57, loss = 0.10690628\n",
      "Iteration 58, loss = 0.11014107\n",
      "Iteration 59, loss = 0.12649126\n",
      "Iteration 60, loss = 0.19594785\n",
      "Iteration 61, loss = 0.20529628\n",
      "Iteration 62, loss = 0.16454160\n",
      "Iteration 63, loss = 0.16798213\n",
      "Iteration 64, loss = 0.13035242\n",
      "Iteration 65, loss = 0.11925020\n",
      "Iteration 66, loss = 0.11116835\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.70395083\n",
      "Iteration 2, loss = 1.43144029\n",
      "Iteration 3, loss = 1.26271002\n",
      "Iteration 4, loss = 1.12333808\n",
      "Iteration 5, loss = 1.00142872\n",
      "Iteration 6, loss = 0.90936106\n",
      "Iteration 7, loss = 0.82047608\n",
      "Iteration 8, loss = 0.72650502\n",
      "Iteration 9, loss = 0.69783065\n",
      "Iteration 10, loss = 0.61867979\n",
      "Iteration 11, loss = 0.56637080\n",
      "Iteration 12, loss = 0.51519438\n",
      "Iteration 13, loss = 0.47555657\n",
      "Iteration 14, loss = 0.47420753\n",
      "Iteration 15, loss = 0.43935194\n",
      "Iteration 16, loss = 0.40883516\n",
      "Iteration 17, loss = 0.36694587\n",
      "Iteration 18, loss = 0.33806157\n",
      "Iteration 19, loss = 0.32639077\n",
      "Iteration 20, loss = 0.32118212\n",
      "Iteration 21, loss = 0.28797539\n",
      "Iteration 22, loss = 0.27100909\n",
      "Iteration 23, loss = 0.26016361\n",
      "Iteration 24, loss = 0.24339198\n",
      "Iteration 25, loss = 0.23121307\n",
      "Iteration 26, loss = 0.22045939\n",
      "Iteration 27, loss = 0.20990841\n",
      "Iteration 28, loss = 0.20322167\n",
      "Iteration 29, loss = 0.19861952\n",
      "Iteration 30, loss = 0.19817819\n",
      "Iteration 31, loss = 0.23378797\n",
      "Iteration 32, loss = 0.28476935\n",
      "Iteration 33, loss = 0.24466078\n",
      "Iteration 34, loss = 0.20917812\n",
      "Iteration 35, loss = 0.19231733\n",
      "Iteration 36, loss = 0.19500876\n",
      "Iteration 37, loss = 0.19888614\n",
      "Iteration 38, loss = 0.17363524\n",
      "Iteration 39, loss = 0.15983012\n",
      "Iteration 40, loss = 0.15162655\n",
      "Iteration 41, loss = 0.14752690\n",
      "Iteration 42, loss = 0.14286759\n",
      "Iteration 43, loss = 0.13939899\n",
      "Iteration 44, loss = 0.13778430\n",
      "Iteration 45, loss = 0.13131322\n",
      "Iteration 46, loss = 0.13039026\n",
      "Iteration 47, loss = 0.12918376\n",
      "Iteration 48, loss = 0.12597608\n",
      "Iteration 49, loss = 0.12360060\n",
      "Iteration 50, loss = 0.12167391\n",
      "Iteration 51, loss = 0.11870713\n",
      "Iteration 52, loss = 0.11863658\n",
      "Iteration 53, loss = 0.14807428\n",
      "Iteration 54, loss = 0.16379842\n",
      "Iteration 55, loss = 0.14646276\n",
      "Iteration 56, loss = 0.13814034\n",
      "Iteration 57, loss = 0.12766628\n",
      "Iteration 58, loss = 0.12041045\n",
      "Iteration 59, loss = 0.11555834\n",
      "Iteration 60, loss = 0.11246857\n",
      "Iteration 61, loss = 0.10985635\n",
      "Iteration 62, loss = 0.10798646\n",
      "Iteration 63, loss = 0.10711859\n",
      "Iteration 64, loss = 0.10846883\n",
      "Iteration 65, loss = 0.11040313\n",
      "Iteration 66, loss = 0.11306900\n",
      "Iteration 67, loss = 0.12084837\n",
      "Iteration 68, loss = 0.19184145\n",
      "Iteration 69, loss = 0.19838469\n",
      "Iteration 70, loss = 0.16371167\n",
      "Iteration 71, loss = 0.14771210\n",
      "Iteration 72, loss = 0.13224411\n",
      "Iteration 73, loss = 0.12601133\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.91195760\n",
      "Iteration 2, loss = 1.60819530\n",
      "Iteration 3, loss = 1.40307901\n",
      "Iteration 4, loss = 1.23576793\n",
      "Iteration 5, loss = 1.09414448\n",
      "Iteration 6, loss = 0.98783783\n",
      "Iteration 7, loss = 0.88652266\n",
      "Iteration 8, loss = 0.77887273\n",
      "Iteration 9, loss = 0.73733223\n",
      "Iteration 10, loss = 0.65223675\n",
      "Iteration 11, loss = 0.60548881\n",
      "Iteration 12, loss = 0.55066910\n",
      "Iteration 13, loss = 0.50960677\n",
      "Iteration 14, loss = 0.49877046\n",
      "Iteration 15, loss = 0.44893201\n",
      "Iteration 16, loss = 0.41225457\n",
      "Iteration 17, loss = 0.38095611\n",
      "Iteration 18, loss = 0.35817035\n",
      "Iteration 19, loss = 0.35261418\n",
      "Iteration 20, loss = 0.37133375\n",
      "Iteration 21, loss = 0.32043726\n",
      "Iteration 22, loss = 0.29115710\n",
      "Iteration 23, loss = 0.27490074\n",
      "Iteration 24, loss = 0.25977047\n",
      "Iteration 25, loss = 0.24384963\n",
      "Iteration 26, loss = 0.23427047\n",
      "Iteration 27, loss = 0.22162885\n",
      "Iteration 28, loss = 0.21770190\n",
      "Iteration 29, loss = 0.20964107\n",
      "Iteration 30, loss = 0.20524818\n",
      "Iteration 31, loss = 0.19534853\n",
      "Iteration 32, loss = 0.18796831\n",
      "Iteration 33, loss = 0.18697088\n",
      "Iteration 34, loss = 0.19244932\n",
      "Iteration 35, loss = 0.19581829\n",
      "Iteration 36, loss = 0.19579031\n",
      "Iteration 37, loss = 0.20301971\n",
      "Iteration 38, loss = 0.21548980\n",
      "Iteration 39, loss = 0.19578274\n",
      "Iteration 40, loss = 0.21383664\n",
      "Iteration 41, loss = 0.23276408\n",
      "Iteration 42, loss = 0.22582306\n",
      "Iteration 43, loss = 0.18732950\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11805867\n",
      "Iteration 2, loss = 1.77580108\n",
      "Iteration 3, loss = 1.53519950\n",
      "Iteration 4, loss = 1.33649543\n",
      "Iteration 5, loss = 1.17003607\n",
      "Iteration 6, loss = 1.04597718\n",
      "Iteration 7, loss = 0.93045381\n",
      "Iteration 8, loss = 0.81589940\n",
      "Iteration 9, loss = 0.75973353\n",
      "Iteration 10, loss = 0.67578332\n",
      "Iteration 11, loss = 0.61399157\n",
      "Iteration 12, loss = 0.55515900\n",
      "Iteration 13, loss = 0.50566865\n",
      "Iteration 14, loss = 0.48765638\n",
      "Iteration 15, loss = 0.46089017\n",
      "Iteration 16, loss = 0.43594026\n",
      "Iteration 17, loss = 0.39248038\n",
      "Iteration 18, loss = 0.36173831\n",
      "Iteration 19, loss = 0.36305114\n",
      "Iteration 20, loss = 0.36224089\n",
      "Iteration 21, loss = 0.31553111\n",
      "Iteration 22, loss = 0.28975207\n",
      "Iteration 23, loss = 0.27348031\n",
      "Iteration 24, loss = 0.25983983\n",
      "Iteration 25, loss = 0.24540489\n",
      "Iteration 26, loss = 0.23345780\n",
      "Iteration 27, loss = 0.22310123\n",
      "Iteration 28, loss = 0.21962997\n",
      "Iteration 29, loss = 0.21021117\n",
      "Iteration 30, loss = 0.20287404\n",
      "Iteration 31, loss = 0.19768032\n",
      "Iteration 32, loss = 0.19563703\n",
      "Iteration 33, loss = 0.18613438\n",
      "Iteration 34, loss = 0.18378413\n",
      "Iteration 35, loss = 0.18733022\n",
      "Iteration 36, loss = 0.21315388\n",
      "Iteration 37, loss = 0.20194386\n",
      "Iteration 38, loss = 0.18803806\n",
      "Iteration 39, loss = 0.18055078\n",
      "Iteration 40, loss = 0.19020112\n",
      "Iteration 41, loss = 0.24966933\n",
      "Iteration 42, loss = 0.22849309\n",
      "Iteration 43, loss = 0.20990503\n",
      "Iteration 44, loss = 0.19024293\n",
      "Iteration 45, loss = 0.20378886\n",
      "Iteration 46, loss = 0.17848323\n",
      "Iteration 47, loss = 0.16542255\n",
      "Iteration 48, loss = 0.15696944\n",
      "Iteration 49, loss = 0.15032084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, loss = 0.14956599\n",
      "Iteration 51, loss = 0.14456600\n",
      "Iteration 52, loss = 0.14085237\n",
      "Iteration 53, loss = 0.14895419\n",
      "Iteration 54, loss = 0.14339038\n",
      "Iteration 55, loss = 0.14002542\n",
      "Iteration 56, loss = 0.14349859\n",
      "Iteration 57, loss = 0.14785597\n",
      "Iteration 58, loss = 0.14148762\n",
      "Iteration 59, loss = 0.14519931\n",
      "Iteration 60, loss = 0.13595748\n",
      "Iteration 61, loss = 0.13254274\n",
      "Iteration 62, loss = 0.13092401\n",
      "Iteration 63, loss = 0.13563772\n",
      "Iteration 64, loss = 0.13457564\n",
      "Iteration 65, loss = 0.14011640\n",
      "Iteration 66, loss = 0.16538326\n",
      "Iteration 67, loss = 0.15498706\n",
      "Iteration 68, loss = 0.16919110\n",
      "Iteration 69, loss = 0.15320453\n",
      "Iteration 70, loss = 0.14792996\n",
      "Iteration 71, loss = 0.13795643\n",
      "Iteration 72, loss = 0.13413427\n",
      "Iteration 73, loss = 0.13577198\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32203461\n",
      "Iteration 2, loss = 1.93715513\n",
      "Iteration 3, loss = 1.65779755\n",
      "Iteration 4, loss = 1.42724902\n",
      "Iteration 5, loss = 1.23553214\n",
      "Iteration 6, loss = 1.09025975\n",
      "Iteration 7, loss = 0.96055121\n",
      "Iteration 8, loss = 0.83662367\n",
      "Iteration 9, loss = 0.78715059\n",
      "Iteration 10, loss = 0.69116545\n",
      "Iteration 11, loss = 0.63499021\n",
      "Iteration 12, loss = 0.57696648\n",
      "Iteration 13, loss = 0.53248222\n",
      "Iteration 14, loss = 0.52366899\n",
      "Iteration 15, loss = 0.47693037\n",
      "Iteration 16, loss = 0.43454422\n",
      "Iteration 17, loss = 0.39492226\n",
      "Iteration 18, loss = 0.37592800\n",
      "Iteration 19, loss = 0.35940990\n",
      "Iteration 20, loss = 0.36495584\n",
      "Iteration 21, loss = 0.32664105\n",
      "Iteration 22, loss = 0.29617954\n",
      "Iteration 23, loss = 0.28305052\n",
      "Iteration 24, loss = 0.28524843\n",
      "Iteration 25, loss = 0.27812427\n",
      "Iteration 26, loss = 0.25334447\n",
      "Iteration 27, loss = 0.24655143\n",
      "Iteration 28, loss = 0.24054636\n",
      "Iteration 29, loss = 0.22703511\n",
      "Iteration 30, loss = 0.21663322\n",
      "Iteration 31, loss = 0.20673502\n",
      "Iteration 32, loss = 0.20110172\n",
      "Iteration 33, loss = 0.19407173\n",
      "Iteration 34, loss = 0.19345123\n",
      "Iteration 35, loss = 0.19132724\n",
      "Iteration 36, loss = 0.19907421\n",
      "Iteration 37, loss = 0.20731309\n",
      "Iteration 38, loss = 0.20053004\n",
      "Iteration 39, loss = 0.18550545\n",
      "Iteration 40, loss = 0.19365416\n",
      "Iteration 41, loss = 0.28259900\n",
      "Iteration 42, loss = 0.24994508\n",
      "Iteration 43, loss = 0.19797619\n",
      "Iteration 44, loss = 0.18449955\n",
      "Iteration 45, loss = 0.17262676\n",
      "Iteration 46, loss = 0.16737462\n",
      "Iteration 47, loss = 0.17201860\n",
      "Iteration 48, loss = 0.16731362\n",
      "Iteration 49, loss = 0.16187959\n",
      "Iteration 50, loss = 0.15816382\n",
      "Iteration 51, loss = 0.15284547\n",
      "Iteration 52, loss = 0.15214575\n",
      "Iteration 53, loss = 0.17443174\n",
      "Iteration 54, loss = 0.16272078\n",
      "Iteration 55, loss = 0.16366253\n",
      "Iteration 56, loss = 0.20840630\n",
      "Iteration 57, loss = 0.18323735\n",
      "Iteration 58, loss = 0.16868454\n",
      "Iteration 59, loss = 0.16623121\n",
      "Iteration 60, loss = 0.15862938\n",
      "Iteration 61, loss = 0.14884778\n",
      "Iteration 62, loss = 0.14477647\n",
      "Iteration 63, loss = 0.14612473\n",
      "Iteration 64, loss = 0.14482992\n",
      "Iteration 65, loss = 0.14459851\n",
      "Iteration 66, loss = 0.16406026\n",
      "Iteration 67, loss = 0.15432722\n",
      "Iteration 68, loss = 0.14662216\n",
      "Iteration 69, loss = 0.14031584\n",
      "Iteration 70, loss = 0.13967487\n",
      "Iteration 71, loss = 0.13784356\n",
      "Iteration 72, loss = 0.13685011\n",
      "Iteration 73, loss = 0.13806751\n",
      "Iteration 74, loss = 0.13918253\n",
      "Iteration 75, loss = 0.14180060\n",
      "Iteration 76, loss = 0.13754864\n",
      "Iteration 77, loss = 0.13567514\n",
      "Iteration 78, loss = 0.13731689\n",
      "Iteration 79, loss = 0.13869201\n",
      "Iteration 80, loss = 0.14408798\n",
      "Iteration 81, loss = 0.16794334\n",
      "Iteration 82, loss = 0.16922832\n",
      "Iteration 83, loss = 0.21850144\n",
      "Iteration 84, loss = 0.17545018\n",
      "Iteration 85, loss = 0.15993038\n",
      "Iteration 86, loss = 0.15070283\n",
      "Iteration 87, loss = 0.14793495\n",
      "Iteration 88, loss = 0.14655646\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.52502097\n",
      "Iteration 2, loss = 2.09384016\n",
      "Iteration 3, loss = 1.77283436\n",
      "Iteration 4, loss = 1.51038863\n",
      "Iteration 5, loss = 1.29547340\n",
      "Iteration 6, loss = 1.13529536\n",
      "Iteration 7, loss = 0.99220225\n",
      "Iteration 8, loss = 0.86055475\n",
      "Iteration 9, loss = 0.80342613\n",
      "Iteration 10, loss = 0.70728187\n",
      "Iteration 11, loss = 0.63820905\n",
      "Iteration 12, loss = 0.57520515\n",
      "Iteration 13, loss = 0.52106511\n",
      "Iteration 14, loss = 0.50653047\n",
      "Iteration 15, loss = 0.46663088\n",
      "Iteration 16, loss = 0.44902729\n",
      "Iteration 17, loss = 0.40327850\n",
      "Iteration 18, loss = 0.37546225\n",
      "Iteration 19, loss = 0.36740069\n",
      "Iteration 20, loss = 0.38215991\n",
      "Iteration 21, loss = 0.33308882\n",
      "Iteration 22, loss = 0.30158973\n",
      "Iteration 23, loss = 0.28859057\n",
      "Iteration 24, loss = 0.29643048\n",
      "Iteration 25, loss = 0.30163822\n",
      "Iteration 26, loss = 0.27334083\n",
      "Iteration 27, loss = 0.26128223\n",
      "Iteration 28, loss = 0.25137110\n",
      "Iteration 29, loss = 0.23249638\n",
      "Iteration 30, loss = 0.22140594\n",
      "Iteration 31, loss = 0.21557821\n",
      "Iteration 32, loss = 0.21397604\n",
      "Iteration 33, loss = 0.20230781\n",
      "Iteration 34, loss = 0.20175703\n",
      "Iteration 35, loss = 0.20055862\n",
      "Iteration 36, loss = 0.20265774\n",
      "Iteration 37, loss = 0.20740398\n",
      "Iteration 38, loss = 0.20849403\n",
      "Iteration 39, loss = 0.19427667\n",
      "Iteration 40, loss = 0.18850850\n",
      "Iteration 41, loss = 0.20752600\n",
      "Iteration 42, loss = 0.20306653\n",
      "Iteration 43, loss = 0.18153856\n",
      "Iteration 44, loss = 0.17834326\n",
      "Iteration 45, loss = 0.18490721\n",
      "Iteration 46, loss = 0.18748074\n",
      "Iteration 47, loss = 0.20288242\n",
      "Iteration 48, loss = 0.18354439\n",
      "Iteration 49, loss = 0.17985104\n",
      "Iteration 50, loss = 0.17726522\n",
      "Iteration 51, loss = 0.17025101\n",
      "Iteration 52, loss = 0.16732425\n",
      "Iteration 53, loss = 0.17563637\n",
      "Iteration 54, loss = 0.16751134\n",
      "Iteration 55, loss = 0.17294404\n",
      "Iteration 56, loss = 0.19757772\n",
      "Iteration 57, loss = 0.18564329\n",
      "Iteration 58, loss = 0.17111384\n",
      "Iteration 59, loss = 0.17330886\n",
      "Iteration 60, loss = 0.16254784\n",
      "Iteration 61, loss = 0.15697049\n",
      "Iteration 62, loss = 0.15703740\n",
      "Iteration 63, loss = 0.16916808\n",
      "Iteration 64, loss = 0.17569611\n",
      "Iteration 65, loss = 0.22778261\n",
      "Iteration 66, loss = 0.21191235\n",
      "Iteration 67, loss = 0.18318315\n",
      "Iteration 68, loss = 0.19581413\n",
      "Iteration 69, loss = 0.17225710\n",
      "Iteration 70, loss = 0.16291076\n",
      "Iteration 71, loss = 0.15863503\n",
      "Iteration 72, loss = 0.15528584\n",
      "Iteration 73, loss = 0.15740553\n",
      "Iteration 74, loss = 0.15831487\n",
      "Iteration 75, loss = 0.16169732\n",
      "Iteration 76, loss = 0.15435224\n",
      "Iteration 77, loss = 0.15004138\n",
      "Iteration 78, loss = 0.15200131\n",
      "Iteration 79, loss = 0.16182935\n",
      "Iteration 80, loss = 0.16263133\n",
      "Iteration 81, loss = 0.15555202\n",
      "Iteration 82, loss = 0.16405420\n",
      "Iteration 83, loss = 0.15615151\n",
      "Iteration 84, loss = 0.15044429\n",
      "Iteration 85, loss = 0.14989245\n",
      "Iteration 86, loss = 0.14756241\n",
      "Iteration 87, loss = 0.15206156\n",
      "Iteration 88, loss = 0.14814820\n",
      "Iteration 89, loss = 0.14584575\n",
      "Iteration 90, loss = 0.14772677\n",
      "Iteration 91, loss = 0.14894272\n",
      "Iteration 92, loss = 0.15202157\n",
      "Iteration 93, loss = 0.15241539\n",
      "Iteration 94, loss = 0.14772799\n",
      "Iteration 95, loss = 0.14973511\n",
      "Iteration 96, loss = 0.14694187\n",
      "Iteration 97, loss = 0.15719714\n",
      "Iteration 98, loss = 0.15814000\n",
      "Iteration 99, loss = 0.16283188\n",
      "Iteration 100, loss = 0.16440824\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.72598418\n",
      "Iteration 2, loss = 2.24547534\n",
      "Iteration 3, loss = 1.87879676\n",
      "Iteration 4, loss = 1.58230855\n",
      "Iteration 5, loss = 1.34321274\n",
      "Iteration 6, loss = 1.16543744\n",
      "Iteration 7, loss = 1.01293393\n",
      "Iteration 8, loss = 0.87821305\n",
      "Iteration 9, loss = 0.81918740\n",
      "Iteration 10, loss = 0.71706267\n",
      "Iteration 11, loss = 0.64072794\n",
      "Iteration 12, loss = 0.58040805\n",
      "Iteration 13, loss = 0.53224361\n",
      "Iteration 14, loss = 0.51467633\n",
      "Iteration 15, loss = 0.48068699\n",
      "Iteration 16, loss = 0.45017692\n",
      "Iteration 17, loss = 0.40130738\n",
      "Iteration 18, loss = 0.37431295\n",
      "Iteration 19, loss = 0.37334351\n",
      "Iteration 20, loss = 0.39055115\n",
      "Iteration 21, loss = 0.34040186\n",
      "Iteration 22, loss = 0.31987934\n",
      "Iteration 23, loss = 0.30599733\n",
      "Iteration 24, loss = 0.31764177\n",
      "Iteration 25, loss = 0.32112009\n",
      "Iteration 26, loss = 0.28900346\n",
      "Iteration 27, loss = 0.27872135\n",
      "Iteration 28, loss = 0.27460239\n",
      "Iteration 29, loss = 0.24765307\n",
      "Iteration 30, loss = 0.23077653\n",
      "Iteration 31, loss = 0.22296183\n",
      "Iteration 32, loss = 0.21728282\n",
      "Iteration 33, loss = 0.20966343\n",
      "Iteration 34, loss = 0.21038247\n",
      "Iteration 35, loss = 0.22348347\n",
      "Iteration 36, loss = 0.23312720\n",
      "Iteration 37, loss = 0.22708877\n",
      "Iteration 38, loss = 0.21274421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39, loss = 0.20372324\n",
      "Iteration 40, loss = 0.19663369\n",
      "Iteration 41, loss = 0.22099637\n",
      "Iteration 42, loss = 0.22840666\n",
      "Iteration 43, loss = 0.20050686\n",
      "Iteration 44, loss = 0.18987086\n",
      "Iteration 45, loss = 0.19592111\n",
      "Iteration 46, loss = 0.20381006\n",
      "Iteration 47, loss = 0.21993035\n",
      "Iteration 48, loss = 0.19725832\n",
      "Iteration 49, loss = 0.18986535\n",
      "Iteration 50, loss = 0.18650233\n",
      "Iteration 51, loss = 0.18168788\n",
      "Iteration 52, loss = 0.18069724\n",
      "Iteration 53, loss = 0.19113955\n",
      "Iteration 54, loss = 0.17779925\n",
      "Iteration 55, loss = 0.17967459\n",
      "Iteration 56, loss = 0.21692964\n",
      "Iteration 57, loss = 0.20916837\n",
      "Iteration 58, loss = 0.18801474\n",
      "Iteration 59, loss = 0.21225937\n",
      "Iteration 60, loss = 0.19717266\n",
      "Iteration 61, loss = 0.19044313\n",
      "Iteration 62, loss = 0.18054239\n",
      "Iteration 63, loss = 0.18018416\n",
      "Iteration 64, loss = 0.17248537\n",
      "Iteration 65, loss = 0.17267292\n",
      "Iteration 66, loss = 0.17430372\n",
      "Iteration 67, loss = 0.16656361\n",
      "Iteration 68, loss = 0.17197182\n",
      "Iteration 69, loss = 0.16503055\n",
      "Iteration 70, loss = 0.16430791\n",
      "Iteration 71, loss = 0.16265927\n",
      "Iteration 72, loss = 0.16157136\n",
      "Iteration 73, loss = 0.16432421\n",
      "Iteration 74, loss = 0.16586112\n",
      "Iteration 75, loss = 0.16648163\n",
      "Iteration 76, loss = 0.16551287\n",
      "Iteration 77, loss = 0.16270779\n",
      "Iteration 78, loss = 0.16468833\n",
      "Iteration 79, loss = 0.16923586\n",
      "Iteration 80, loss = 0.17735559\n",
      "Iteration 81, loss = 0.18460756\n",
      "Iteration 82, loss = 0.17873636\n",
      "Iteration 83, loss = 0.18430192\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.92585774\n",
      "Iteration 2, loss = 2.39293442\n",
      "Iteration 3, loss = 1.98252425\n",
      "Iteration 4, loss = 1.65359364\n",
      "Iteration 5, loss = 1.39015627\n",
      "Iteration 6, loss = 1.19693267\n",
      "Iteration 7, loss = 1.03598421\n",
      "Iteration 8, loss = 0.89768263\n",
      "Iteration 9, loss = 0.83278969\n",
      "Iteration 10, loss = 0.72170270\n",
      "Iteration 11, loss = 0.64901000\n",
      "Iteration 12, loss = 0.58620039\n",
      "Iteration 13, loss = 0.53122210\n",
      "Iteration 14, loss = 0.51378802\n",
      "Iteration 15, loss = 0.47569014\n",
      "Iteration 16, loss = 0.46558977\n",
      "Iteration 17, loss = 0.41068703\n",
      "Iteration 18, loss = 0.38383295\n",
      "Iteration 19, loss = 0.37673918\n",
      "Iteration 20, loss = 0.39651001\n",
      "Iteration 21, loss = 0.34426259\n",
      "Iteration 22, loss = 0.31497490\n",
      "Iteration 23, loss = 0.30028039\n",
      "Iteration 24, loss = 0.30389346\n",
      "Iteration 25, loss = 0.32285907\n",
      "Iteration 26, loss = 0.28764184\n",
      "Iteration 27, loss = 0.27247805\n",
      "Iteration 28, loss = 0.26558247\n",
      "Iteration 29, loss = 0.25174315\n",
      "Iteration 30, loss = 0.23882363\n",
      "Iteration 31, loss = 0.23111821\n",
      "Iteration 32, loss = 0.22973380\n",
      "Iteration 33, loss = 0.22092411\n",
      "Iteration 34, loss = 0.22101984\n",
      "Iteration 35, loss = 0.22732673\n",
      "Iteration 36, loss = 0.24035188\n",
      "Iteration 37, loss = 0.24759684\n",
      "Iteration 38, loss = 0.24727828\n",
      "Iteration 39, loss = 0.23633422\n",
      "Iteration 40, loss = 0.21679295\n",
      "Iteration 41, loss = 0.23993913\n",
      "Iteration 42, loss = 0.25399360\n",
      "Iteration 43, loss = 0.22202704\n",
      "Iteration 44, loss = 0.20872690\n",
      "Iteration 45, loss = 0.20235617\n",
      "Iteration 46, loss = 0.20325731\n",
      "Iteration 47, loss = 0.20038860\n",
      "Iteration 48, loss = 0.19462429\n",
      "Iteration 49, loss = 0.19532924\n",
      "Iteration 50, loss = 0.20252074\n",
      "Iteration 51, loss = 0.19461152\n",
      "Iteration 52, loss = 0.19242287\n",
      "Iteration 53, loss = 0.20604559\n",
      "Iteration 54, loss = 0.19499944\n",
      "Iteration 55, loss = 0.19485908\n",
      "Iteration 56, loss = 0.23394499\n",
      "Iteration 57, loss = 0.22524721\n",
      "Iteration 58, loss = 0.20067872\n",
      "Iteration 59, loss = 0.20363272\n",
      "Iteration 60, loss = 0.19719520\n",
      "Iteration 61, loss = 0.18583209\n",
      "Iteration 62, loss = 0.18065710\n",
      "Iteration 63, loss = 0.18470799\n",
      "Iteration 64, loss = 0.18158090\n",
      "Iteration 65, loss = 0.18768391\n",
      "Iteration 66, loss = 0.20678013\n",
      "Iteration 67, loss = 0.18452465\n",
      "Iteration 68, loss = 0.18767087\n",
      "Iteration 69, loss = 0.18045680\n",
      "Iteration 70, loss = 0.18086304\n",
      "Iteration 71, loss = 0.17573644\n",
      "Iteration 72, loss = 0.17538381\n",
      "Iteration 73, loss = 0.17659377\n",
      "Iteration 74, loss = 0.17679806\n",
      "Iteration 75, loss = 0.17869523\n",
      "Iteration 76, loss = 0.17494365\n",
      "Iteration 77, loss = 0.17316490\n",
      "Iteration 78, loss = 0.17520760\n",
      "Iteration 79, loss = 0.18237303\n",
      "Iteration 80, loss = 0.18410287\n",
      "Iteration 81, loss = 0.19364323\n",
      "Iteration 82, loss = 0.18626806\n",
      "Iteration 83, loss = 0.18368707\n",
      "Iteration 84, loss = 0.17734398\n",
      "Iteration 85, loss = 0.17508065\n",
      "Iteration 86, loss = 0.17529845\n",
      "Iteration 87, loss = 0.18696055\n",
      "Iteration 88, loss = 0.18082367\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.12428316\n",
      "Iteration 2, loss = 2.53719378\n",
      "Iteration 3, loss = 2.07864792\n",
      "Iteration 4, loss = 1.71768516\n",
      "Iteration 5, loss = 1.43196226\n",
      "Iteration 6, loss = 1.22407312\n",
      "Iteration 7, loss = 1.05552165\n",
      "Iteration 8, loss = 0.90874193\n",
      "Iteration 9, loss = 0.82719671\n",
      "Iteration 10, loss = 0.72550730\n",
      "Iteration 11, loss = 0.65131978\n",
      "Iteration 12, loss = 0.58573689\n",
      "Iteration 13, loss = 0.53545111\n",
      "Iteration 14, loss = 0.52143432\n",
      "Iteration 15, loss = 0.47561464\n",
      "Iteration 16, loss = 0.46838416\n",
      "Iteration 17, loss = 0.41727810\n",
      "Iteration 18, loss = 0.38743377\n",
      "Iteration 19, loss = 0.39047348\n",
      "Iteration 20, loss = 0.39895151\n",
      "Iteration 21, loss = 0.35228878\n",
      "Iteration 22, loss = 0.33492764\n",
      "Iteration 23, loss = 0.30969055\n",
      "Iteration 24, loss = 0.30772414\n",
      "Iteration 25, loss = 0.31796670\n",
      "Iteration 26, loss = 0.29275517\n",
      "Iteration 27, loss = 0.27912570\n",
      "Iteration 28, loss = 0.27665329\n",
      "Iteration 29, loss = 0.26757721\n",
      "Iteration 30, loss = 0.24941837\n",
      "Iteration 31, loss = 0.24364080\n",
      "Iteration 32, loss = 0.23838474\n",
      "Iteration 33, loss = 0.22751431\n",
      "Iteration 34, loss = 0.22759356\n",
      "Iteration 35, loss = 0.23028448\n",
      "Iteration 36, loss = 0.23660662\n",
      "Iteration 37, loss = 0.23895869\n",
      "Iteration 38, loss = 0.23566477\n",
      "Iteration 39, loss = 0.23614110\n",
      "Iteration 40, loss = 0.22573344\n",
      "Iteration 41, loss = 0.25391047\n",
      "Iteration 42, loss = 0.25773093\n",
      "Iteration 43, loss = 0.22076532\n",
      "Iteration 44, loss = 0.21001733\n",
      "Iteration 45, loss = 0.22068976\n",
      "Iteration 46, loss = 0.22755804\n",
      "Iteration 47, loss = 0.24208974\n",
      "Iteration 48, loss = 0.22115974\n",
      "Iteration 49, loss = 0.21155697\n",
      "Iteration 50, loss = 0.21195511\n",
      "Iteration 51, loss = 0.20450311\n",
      "Iteration 52, loss = 0.20688369\n",
      "Iteration 53, loss = 0.22164556\n",
      "Iteration 54, loss = 0.20531450\n",
      "Iteration 55, loss = 0.20704642\n",
      "Iteration 56, loss = 0.22231571\n",
      "Iteration 57, loss = 0.24036802\n",
      "Iteration 58, loss = 0.21547741\n",
      "Iteration 59, loss = 0.22310472\n",
      "Iteration 60, loss = 0.20959468\n",
      "Iteration 61, loss = 0.19777762\n",
      "Iteration 62, loss = 0.19122106\n",
      "Iteration 63, loss = 0.19581889\n",
      "Iteration 64, loss = 0.19210167\n",
      "Iteration 65, loss = 0.19769953\n",
      "Iteration 66, loss = 0.20529866\n",
      "Iteration 67, loss = 0.19103248\n",
      "Iteration 68, loss = 0.19970497\n",
      "Iteration 69, loss = 0.19015152\n",
      "Iteration 70, loss = 0.19175566\n",
      "Iteration 71, loss = 0.18749378\n",
      "Iteration 72, loss = 0.18735952\n",
      "Iteration 73, loss = 0.18995354\n",
      "Iteration 74, loss = 0.19042072\n",
      "Iteration 75, loss = 0.19382254\n",
      "Iteration 76, loss = 0.18743492\n",
      "Iteration 77, loss = 0.18432636\n",
      "Iteration 78, loss = 0.18646148\n",
      "Iteration 79, loss = 0.19466051\n",
      "Iteration 80, loss = 0.19846184\n",
      "Iteration 81, loss = 0.19991198\n",
      "Iteration 82, loss = 0.20280129\n",
      "Iteration 83, loss = 0.20023648\n",
      "Iteration 84, loss = 0.19465610\n",
      "Iteration 85, loss = 0.18881590\n",
      "Iteration 86, loss = 0.18767987\n",
      "Iteration 87, loss = 0.19822573\n",
      "Iteration 88, loss = 0.19398803\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.32111633\n",
      "Iteration 2, loss = 2.67420019\n",
      "Iteration 3, loss = 2.16961906\n",
      "Iteration 4, loss = 1.77430024\n",
      "Iteration 5, loss = 1.46482695\n",
      "Iteration 6, loss = 1.24245694\n",
      "Iteration 7, loss = 1.06429294\n",
      "Iteration 8, loss = 0.91343073\n",
      "Iteration 9, loss = 0.83723960\n",
      "Iteration 10, loss = 0.73202118\n",
      "Iteration 11, loss = 0.65021776\n",
      "Iteration 12, loss = 0.58534545\n",
      "Iteration 13, loss = 0.53307267\n",
      "Iteration 14, loss = 0.51779587\n",
      "Iteration 15, loss = 0.47628435\n",
      "Iteration 16, loss = 0.47124502\n",
      "Iteration 17, loss = 0.41662249\n",
      "Iteration 18, loss = 0.39231761\n",
      "Iteration 19, loss = 0.39754406\n",
      "Iteration 20, loss = 0.38611696\n",
      "Iteration 21, loss = 0.34351133\n",
      "Iteration 22, loss = 0.33106942\n",
      "Iteration 23, loss = 0.31498467\n",
      "Iteration 24, loss = 0.31171920\n",
      "Iteration 25, loss = 0.33225229\n",
      "Iteration 26, loss = 0.29559593\n",
      "Iteration 27, loss = 0.28745212\n",
      "Iteration 28, loss = 0.28438035\n",
      "Iteration 29, loss = 0.27196086\n",
      "Iteration 30, loss = 0.25645617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31, loss = 0.25316168\n",
      "Iteration 32, loss = 0.24756769\n",
      "Iteration 33, loss = 0.23625158\n",
      "Iteration 34, loss = 0.23607942\n",
      "Iteration 35, loss = 0.24122846\n",
      "Iteration 36, loss = 0.24710380\n",
      "Iteration 37, loss = 0.24769301\n",
      "Iteration 38, loss = 0.25128065\n",
      "Iteration 39, loss = 0.25077857\n",
      "Iteration 40, loss = 0.24480995\n",
      "Iteration 41, loss = 0.27387053\n",
      "Iteration 42, loss = 0.27203407\n",
      "Iteration 43, loss = 0.23815750\n",
      "Iteration 44, loss = 0.22613593\n",
      "Iteration 45, loss = 0.22802596\n",
      "Iteration 46, loss = 0.23295859\n",
      "Iteration 47, loss = 0.22484067\n",
      "Iteration 48, loss = 0.21777363\n",
      "Iteration 49, loss = 0.21677854\n",
      "Iteration 50, loss = 0.22283023\n",
      "Iteration 51, loss = 0.21560135\n",
      "Iteration 52, loss = 0.21294721\n",
      "Iteration 53, loss = 0.23122756\n",
      "Iteration 54, loss = 0.21813909\n",
      "Iteration 55, loss = 0.22986271\n",
      "Iteration 56, loss = 0.25436718\n",
      "Iteration 57, loss = 0.24018758\n",
      "Iteration 58, loss = 0.22670309\n",
      "Iteration 59, loss = 0.23662050\n",
      "Iteration 60, loss = 0.22277472\n",
      "Iteration 61, loss = 0.21314753\n",
      "Iteration 62, loss = 0.20413483\n",
      "Iteration 63, loss = 0.20850435\n",
      "Iteration 64, loss = 0.20314043\n",
      "Iteration 65, loss = 0.20824400\n",
      "Iteration 66, loss = 0.21511886\n",
      "Iteration 67, loss = 0.20381219\n",
      "Iteration 68, loss = 0.20889973\n",
      "Iteration 69, loss = 0.20125137\n",
      "Iteration 70, loss = 0.20340129\n",
      "Iteration 71, loss = 0.19898683\n",
      "Iteration 72, loss = 0.19904973\n",
      "Iteration 73, loss = 0.20446478\n",
      "Iteration 74, loss = 0.20453935\n",
      "Iteration 75, loss = 0.20796250\n",
      "Iteration 76, loss = 0.19985581\n",
      "Iteration 77, loss = 0.19761436\n",
      "Iteration 78, loss = 0.20054130\n",
      "Iteration 79, loss = 0.20588918\n",
      "Iteration 80, loss = 0.21094367\n",
      "Iteration 81, loss = 0.20558825\n",
      "Iteration 82, loss = 0.21662324\n",
      "Iteration 83, loss = 0.20515241\n",
      "Iteration 84, loss = 0.20103112\n",
      "Iteration 85, loss = 0.20007173\n",
      "Iteration 86, loss = 0.19896946\n",
      "Iteration 87, loss = 0.21641195\n",
      "Iteration 88, loss = 0.21167431\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.51626353\n",
      "Iteration 2, loss = 2.80875742\n",
      "Iteration 3, loss = 2.25332705\n",
      "Iteration 4, loss = 1.82260081\n",
      "Iteration 5, loss = 1.49374090\n",
      "Iteration 6, loss = 1.26021508\n",
      "Iteration 7, loss = 1.07327440\n",
      "Iteration 8, loss = 0.91907576\n",
      "Iteration 9, loss = 0.84308296\n",
      "Iteration 10, loss = 0.73548016\n",
      "Iteration 11, loss = 0.65828463\n",
      "Iteration 12, loss = 0.59574268\n",
      "Iteration 13, loss = 0.54523625\n",
      "Iteration 14, loss = 0.52083763\n",
      "Iteration 15, loss = 0.47713096\n",
      "Iteration 16, loss = 0.46476866\n",
      "Iteration 17, loss = 0.42166573\n",
      "Iteration 18, loss = 0.40204994\n",
      "Iteration 19, loss = 0.39308882\n",
      "Iteration 20, loss = 0.40871638\n",
      "Iteration 21, loss = 0.35873788\n",
      "Iteration 22, loss = 0.34335313\n",
      "Iteration 23, loss = 0.32666552\n",
      "Iteration 24, loss = 0.32613648\n",
      "Iteration 25, loss = 0.34525971\n",
      "Iteration 26, loss = 0.31379926\n",
      "Iteration 27, loss = 0.30744337\n",
      "Iteration 28, loss = 0.29974066\n",
      "Iteration 29, loss = 0.28209694\n",
      "Iteration 30, loss = 0.26372871\n",
      "Iteration 31, loss = 0.26526299\n",
      "Iteration 32, loss = 0.25959867\n",
      "Iteration 33, loss = 0.24786008\n",
      "Iteration 34, loss = 0.25018651\n",
      "Iteration 35, loss = 0.25469140\n",
      "Iteration 36, loss = 0.26351742\n",
      "Iteration 37, loss = 0.25654025\n",
      "Iteration 38, loss = 0.25819585\n",
      "Iteration 39, loss = 0.24881737\n",
      "Iteration 40, loss = 0.23860825\n",
      "Iteration 41, loss = 0.27013234\n",
      "Iteration 42, loss = 0.27315375\n",
      "Iteration 43, loss = 0.24867209\n",
      "Iteration 44, loss = 0.23415954\n",
      "Iteration 45, loss = 0.24198007\n",
      "Iteration 46, loss = 0.25028989\n",
      "Iteration 47, loss = 0.25432232\n",
      "Iteration 48, loss = 0.23845553\n",
      "Iteration 49, loss = 0.23246079\n",
      "Iteration 50, loss = 0.23628565\n",
      "Iteration 51, loss = 0.22942152\n",
      "Iteration 52, loss = 0.23071508\n",
      "Iteration 53, loss = 0.24325988\n",
      "Iteration 54, loss = 0.22612909\n",
      "Iteration 55, loss = 0.22939264\n",
      "Iteration 56, loss = 0.24870735\n",
      "Iteration 57, loss = 0.26731735\n",
      "Iteration 58, loss = 0.24582049\n",
      "Iteration 59, loss = 0.25800455\n",
      "Iteration 60, loss = 0.24177047\n",
      "Iteration 61, loss = 0.22468044\n",
      "Iteration 62, loss = 0.21668703\n",
      "Iteration 63, loss = 0.22038621\n",
      "Iteration 64, loss = 0.21657659\n",
      "Iteration 65, loss = 0.21993706\n",
      "Iteration 66, loss = 0.22536855\n",
      "Iteration 67, loss = 0.21605138\n",
      "Iteration 68, loss = 0.22159586\n",
      "Iteration 69, loss = 0.21224671\n",
      "Iteration 70, loss = 0.21440307\n",
      "Iteration 71, loss = 0.21127907\n",
      "Iteration 72, loss = 0.21215825\n",
      "Iteration 73, loss = 0.21788323\n",
      "Iteration 74, loss = 0.21653056\n",
      "Iteration 75, loss = 0.22007289\n",
      "Iteration 76, loss = 0.21161152\n",
      "Iteration 77, loss = 0.20998274\n",
      "Iteration 78, loss = 0.21262915\n",
      "Iteration 79, loss = 0.22445237\n",
      "Iteration 80, loss = 0.21935494\n",
      "Iteration 81, loss = 0.21381507\n",
      "Iteration 82, loss = 0.22540156\n",
      "Iteration 83, loss = 0.21618172\n",
      "Iteration 84, loss = 0.21429696\n",
      "Iteration 85, loss = 0.21074729\n",
      "Iteration 86, loss = 0.21128412\n",
      "Iteration 87, loss = 0.22588287\n",
      "Iteration 88, loss = 0.22123733\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.71051626\n",
      "Iteration 2, loss = 2.93904967\n",
      "Iteration 3, loss = 2.33477259\n",
      "Iteration 4, loss = 1.87104735\n",
      "Iteration 5, loss = 1.52282300\n",
      "Iteration 6, loss = 1.27346192\n",
      "Iteration 7, loss = 1.08096665\n",
      "Iteration 8, loss = 0.92214012\n",
      "Iteration 9, loss = 0.84293767\n",
      "Iteration 10, loss = 0.73319383\n",
      "Iteration 11, loss = 0.65626187\n",
      "Iteration 12, loss = 0.59665396\n",
      "Iteration 13, loss = 0.54097509\n",
      "Iteration 14, loss = 0.52217688\n",
      "Iteration 15, loss = 0.48052762\n",
      "Iteration 16, loss = 0.47079805\n",
      "Iteration 17, loss = 0.42261099\n",
      "Iteration 18, loss = 0.39754746\n",
      "Iteration 19, loss = 0.39690527\n",
      "Iteration 20, loss = 0.41600281\n",
      "Iteration 21, loss = 0.36394299\n",
      "Iteration 22, loss = 0.34093616\n",
      "Iteration 23, loss = 0.33045790\n",
      "Iteration 24, loss = 0.34358543\n",
      "Iteration 25, loss = 0.35095431\n",
      "Iteration 26, loss = 0.32137081\n",
      "Iteration 27, loss = 0.30914039\n",
      "Iteration 28, loss = 0.30451088\n",
      "Iteration 29, loss = 0.28388911\n",
      "Iteration 30, loss = 0.26827556\n",
      "Iteration 31, loss = 0.26939393\n",
      "Iteration 32, loss = 0.27230516\n",
      "Iteration 33, loss = 0.26082401\n",
      "Iteration 34, loss = 0.26092014\n",
      "Iteration 35, loss = 0.26420530\n",
      "Iteration 36, loss = 0.27014835\n",
      "Iteration 37, loss = 0.26409297\n",
      "Iteration 38, loss = 0.27643583\n",
      "Iteration 39, loss = 0.26074646\n",
      "Iteration 40, loss = 0.24831159\n",
      "Iteration 41, loss = 0.27028650\n",
      "Iteration 42, loss = 0.27856147\n",
      "Iteration 43, loss = 0.25256209\n",
      "Iteration 44, loss = 0.23996409\n",
      "Iteration 45, loss = 0.25322867\n",
      "Iteration 46, loss = 0.27812554\n",
      "Iteration 47, loss = 0.25457708\n",
      "Iteration 48, loss = 0.24593893\n",
      "Iteration 49, loss = 0.23890090\n",
      "Iteration 50, loss = 0.24304643\n",
      "Iteration 51, loss = 0.23578700\n",
      "Iteration 52, loss = 0.23920047\n",
      "Iteration 53, loss = 0.25846129\n",
      "Iteration 54, loss = 0.24443025\n",
      "Iteration 55, loss = 0.25194174\n",
      "Iteration 56, loss = 0.24975769\n",
      "Iteration 57, loss = 0.24635159\n",
      "Iteration 58, loss = 0.23733765\n",
      "Iteration 59, loss = 0.24071167\n",
      "Iteration 60, loss = 0.23269608\n",
      "Iteration 61, loss = 0.22768552\n",
      "Iteration 62, loss = 0.22891673\n",
      "Iteration 63, loss = 0.23703952\n",
      "Iteration 64, loss = 0.22915043\n",
      "Iteration 65, loss = 0.23261943\n",
      "Iteration 66, loss = 0.23492604\n",
      "Iteration 67, loss = 0.22854043\n",
      "Iteration 68, loss = 0.23795079\n",
      "Iteration 69, loss = 0.22580445\n",
      "Iteration 70, loss = 0.22682010\n",
      "Iteration 71, loss = 0.22171243\n",
      "Iteration 72, loss = 0.22160973\n",
      "Iteration 73, loss = 0.22773756\n",
      "Iteration 74, loss = 0.22510663\n",
      "Iteration 75, loss = 0.22850434\n",
      "Iteration 76, loss = 0.22220947\n",
      "Iteration 77, loss = 0.21873442\n",
      "Iteration 78, loss = 0.22403211\n",
      "Iteration 79, loss = 0.23765338\n",
      "Iteration 80, loss = 0.23386381\n",
      "Iteration 81, loss = 0.22617342\n",
      "Iteration 82, loss = 0.23642967\n",
      "Iteration 83, loss = 0.22830813\n",
      "Iteration 84, loss = 0.22437114\n",
      "Iteration 85, loss = 0.22301653\n",
      "Iteration 86, loss = 0.22589207\n",
      "Iteration 87, loss = 0.24016991\n",
      "Iteration 88, loss = 0.22797235\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.90369036\n",
      "Iteration 2, loss = 3.06680752\n",
      "Iteration 3, loss = 2.40983272\n",
      "Iteration 4, loss = 1.91471517\n",
      "Iteration 5, loss = 1.54355631\n",
      "Iteration 6, loss = 1.28478719\n",
      "Iteration 7, loss = 1.08804627\n",
      "Iteration 8, loss = 0.92454079\n",
      "Iteration 9, loss = 0.84112793\n",
      "Iteration 10, loss = 0.72999065\n",
      "Iteration 11, loss = 0.65158509\n",
      "Iteration 12, loss = 0.59260861\n",
      "Iteration 13, loss = 0.54018320\n",
      "Iteration 14, loss = 0.52585990\n",
      "Iteration 15, loss = 0.48186765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.47684307\n",
      "Iteration 17, loss = 0.42207195\n",
      "Iteration 18, loss = 0.40152832\n",
      "Iteration 19, loss = 0.40271105\n",
      "Iteration 20, loss = 0.42596530\n",
      "Iteration 21, loss = 0.37136988\n",
      "Iteration 22, loss = 0.34814804\n",
      "Iteration 23, loss = 0.33341918\n",
      "Iteration 24, loss = 0.34279986\n",
      "Iteration 25, loss = 0.36822826\n",
      "Iteration 26, loss = 0.34297362\n",
      "Iteration 27, loss = 0.32015274\n",
      "Iteration 28, loss = 0.30908998\n",
      "Iteration 29, loss = 0.29435379\n",
      "Iteration 30, loss = 0.27717789\n",
      "Iteration 31, loss = 0.28331517\n",
      "Iteration 32, loss = 0.27997402\n",
      "Iteration 33, loss = 0.26718873\n",
      "Iteration 34, loss = 0.26871041\n",
      "Iteration 35, loss = 0.27509367\n",
      "Iteration 36, loss = 0.28781160\n",
      "Iteration 37, loss = 0.27640965\n",
      "Iteration 38, loss = 0.28966893\n",
      "Iteration 39, loss = 0.27111157\n",
      "Iteration 40, loss = 0.25941038\n",
      "Iteration 41, loss = 0.28797428\n",
      "Iteration 42, loss = 0.28506016\n",
      "Iteration 43, loss = 0.26380101\n",
      "Iteration 44, loss = 0.25180227\n",
      "Iteration 45, loss = 0.26511077\n",
      "Iteration 46, loss = 0.28071318\n",
      "Iteration 47, loss = 0.27809269\n",
      "Iteration 48, loss = 0.26734918\n",
      "Iteration 49, loss = 0.25678704\n",
      "Iteration 50, loss = 0.26004936\n",
      "Iteration 51, loss = 0.25168448\n",
      "Iteration 52, loss = 0.26222951\n",
      "Iteration 53, loss = 0.27858156\n",
      "Iteration 54, loss = 0.25765997\n",
      "Iteration 55, loss = 0.24902380\n",
      "Iteration 56, loss = 0.24696093\n",
      "Iteration 57, loss = 0.25709317\n",
      "Iteration 58, loss = 0.24888081\n",
      "Iteration 59, loss = 0.25552815\n",
      "Iteration 60, loss = 0.24083602\n",
      "Iteration 61, loss = 0.23704259\n",
      "Iteration 62, loss = 0.23936386\n",
      "Iteration 63, loss = 0.25035546\n",
      "Iteration 64, loss = 0.24301614\n",
      "Iteration 65, loss = 0.24916693\n",
      "Iteration 66, loss = 0.25523147\n",
      "Iteration 67, loss = 0.24140865\n",
      "Iteration 68, loss = 0.24811362\n",
      "Iteration 69, loss = 0.23758928\n",
      "Iteration 70, loss = 0.23954620\n",
      "Iteration 71, loss = 0.23408384\n",
      "Iteration 72, loss = 0.23432560\n",
      "Iteration 73, loss = 0.24202836\n",
      "Iteration 74, loss = 0.23845087\n",
      "Iteration 75, loss = 0.24068268\n",
      "Iteration 76, loss = 0.23385669\n",
      "Iteration 77, loss = 0.23163718\n",
      "Iteration 78, loss = 0.23624458\n",
      "Iteration 79, loss = 0.25073762\n",
      "Iteration 80, loss = 0.25028846\n",
      "Iteration 81, loss = 0.23724855\n",
      "Iteration 82, loss = 0.24474647\n",
      "Iteration 83, loss = 0.23701683\n",
      "Iteration 84, loss = 0.23698054\n",
      "Iteration 85, loss = 0.23311647\n",
      "Iteration 86, loss = 0.23479232\n",
      "Iteration 87, loss = 0.25029459\n",
      "Iteration 88, loss = 0.24736260\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.09498752\n",
      "Iteration 2, loss = 3.19226354\n",
      "Iteration 3, loss = 2.48801604\n",
      "Iteration 4, loss = 1.95844456\n",
      "Iteration 5, loss = 1.57106329\n",
      "Iteration 6, loss = 1.30081432\n",
      "Iteration 7, loss = 1.09275893\n",
      "Iteration 8, loss = 0.92806811\n",
      "Iteration 9, loss = 0.83962657\n",
      "Iteration 10, loss = 0.73506866\n",
      "Iteration 11, loss = 0.65848801\n",
      "Iteration 12, loss = 0.60372623\n",
      "Iteration 13, loss = 0.55336733\n",
      "Iteration 14, loss = 0.53169808\n",
      "Iteration 15, loss = 0.49138212\n",
      "Iteration 16, loss = 0.46513482\n",
      "Iteration 17, loss = 0.43316937\n",
      "Iteration 18, loss = 0.41810011\n",
      "Iteration 19, loss = 0.41427031\n",
      "Iteration 20, loss = 0.40973541\n",
      "Iteration 21, loss = 0.37596770\n",
      "Iteration 22, loss = 0.36347234\n",
      "Iteration 23, loss = 0.34318759\n",
      "Iteration 24, loss = 0.34739110\n",
      "Iteration 25, loss = 0.35177209\n",
      "Iteration 26, loss = 0.33628600\n",
      "Iteration 27, loss = 0.32450280\n",
      "Iteration 28, loss = 0.32819353\n",
      "Iteration 29, loss = 0.31188256\n",
      "Iteration 30, loss = 0.29299934\n",
      "Iteration 31, loss = 0.29574312\n",
      "Iteration 32, loss = 0.28859077\n",
      "Iteration 33, loss = 0.27575428\n",
      "Iteration 34, loss = 0.27617997\n",
      "Iteration 35, loss = 0.28403443\n",
      "Iteration 36, loss = 0.28965009\n",
      "Iteration 37, loss = 0.28693116\n",
      "Iteration 38, loss = 0.29953749\n",
      "Iteration 39, loss = 0.28174162\n",
      "Iteration 40, loss = 0.27141304\n",
      "Iteration 41, loss = 0.30605061\n",
      "Iteration 42, loss = 0.30143620\n",
      "Iteration 43, loss = 0.27783685\n",
      "Iteration 44, loss = 0.26509045\n",
      "Iteration 45, loss = 0.27315970\n",
      "Iteration 46, loss = 0.28738438\n",
      "Iteration 47, loss = 0.27652762\n",
      "Iteration 48, loss = 0.26564735\n",
      "Iteration 49, loss = 0.26635974\n",
      "Iteration 50, loss = 0.27312580\n",
      "Iteration 51, loss = 0.26001746\n",
      "Iteration 52, loss = 0.26864479\n",
      "Iteration 53, loss = 0.28295619\n",
      "Iteration 54, loss = 0.27053157\n",
      "Iteration 55, loss = 0.26565457\n",
      "Iteration 56, loss = 0.26233196\n",
      "Iteration 57, loss = 0.27494750\n",
      "Iteration 58, loss = 0.26733510\n",
      "Iteration 59, loss = 0.27058998\n",
      "Iteration 60, loss = 0.25561441\n",
      "Iteration 61, loss = 0.25060076\n",
      "Iteration 62, loss = 0.24664541\n",
      "Iteration 63, loss = 0.25502000\n",
      "Iteration 64, loss = 0.25022237\n",
      "Iteration 65, loss = 0.25867278\n",
      "Iteration 66, loss = 0.25608581\n",
      "Iteration 67, loss = 0.25016247\n",
      "Iteration 68, loss = 0.25758390\n",
      "Iteration 69, loss = 0.24724971\n",
      "Iteration 70, loss = 0.24910459\n",
      "Iteration 71, loss = 0.24378543\n",
      "Iteration 72, loss = 0.24415419\n",
      "Iteration 73, loss = 0.25149324\n",
      "Iteration 74, loss = 0.25005058\n",
      "Iteration 75, loss = 0.25341508\n",
      "Iteration 76, loss = 0.24443209\n",
      "Iteration 77, loss = 0.24227204\n",
      "Iteration 78, loss = 0.24716253\n",
      "Iteration 79, loss = 0.26004047\n",
      "Iteration 80, loss = 0.25818254\n",
      "Iteration 81, loss = 0.25232458\n",
      "Iteration 82, loss = 0.26360940\n",
      "Iteration 83, loss = 0.25229160\n",
      "Iteration 84, loss = 0.24795673\n",
      "Iteration 85, loss = 0.24594833\n",
      "Iteration 86, loss = 0.24507063\n",
      "Iteration 87, loss = 0.26105693\n",
      "Iteration 88, loss = 0.25717934\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.28621371\n",
      "Iteration 2, loss = 3.31463160\n",
      "Iteration 3, loss = 2.55946733\n",
      "Iteration 4, loss = 1.99902798\n",
      "Iteration 5, loss = 1.59133064\n",
      "Iteration 6, loss = 1.31004798\n",
      "Iteration 7, loss = 1.09611009\n",
      "Iteration 8, loss = 0.92921199\n",
      "Iteration 9, loss = 0.83458865\n",
      "Iteration 10, loss = 0.73006518\n",
      "Iteration 11, loss = 0.65185067\n",
      "Iteration 12, loss = 0.59221426\n",
      "Iteration 13, loss = 0.54181903\n",
      "Iteration 14, loss = 0.52628748\n",
      "Iteration 15, loss = 0.48638319\n",
      "Iteration 16, loss = 0.47907842\n",
      "Iteration 17, loss = 0.43119355\n",
      "Iteration 18, loss = 0.41299196\n",
      "Iteration 19, loss = 0.41114427\n",
      "Iteration 20, loss = 0.40530776\n",
      "Iteration 21, loss = 0.37403735\n",
      "Iteration 22, loss = 0.35943340\n",
      "Iteration 23, loss = 0.34648252\n",
      "Iteration 24, loss = 0.35451613\n",
      "Iteration 25, loss = 0.37128917\n",
      "Iteration 26, loss = 0.35037459\n",
      "Iteration 27, loss = 0.33532996\n",
      "Iteration 28, loss = 0.32533911\n",
      "Iteration 29, loss = 0.31294451\n",
      "Iteration 30, loss = 0.29567019\n",
      "Iteration 31, loss = 0.30213840\n",
      "Iteration 32, loss = 0.29806288\n",
      "Iteration 33, loss = 0.28585400\n",
      "Iteration 34, loss = 0.28703810\n",
      "Iteration 35, loss = 0.28922267\n",
      "Iteration 36, loss = 0.29999803\n",
      "Iteration 37, loss = 0.29567473\n",
      "Iteration 38, loss = 0.31532580\n",
      "Iteration 39, loss = 0.29963628\n",
      "Iteration 40, loss = 0.28767700\n",
      "Iteration 41, loss = 0.31565722\n",
      "Iteration 42, loss = 0.29749947\n",
      "Iteration 43, loss = 0.28665392\n",
      "Iteration 44, loss = 0.27680636\n",
      "Iteration 45, loss = 0.28311179\n",
      "Iteration 46, loss = 0.29446591\n",
      "Iteration 47, loss = 0.29182782\n",
      "Iteration 48, loss = 0.28790788\n",
      "Iteration 49, loss = 0.29010035\n",
      "Iteration 50, loss = 0.29323470\n",
      "Iteration 51, loss = 0.27417887\n",
      "Iteration 52, loss = 0.28381376\n",
      "Iteration 53, loss = 0.30410196\n",
      "Iteration 54, loss = 0.27916938\n",
      "Iteration 55, loss = 0.26854352\n",
      "Iteration 56, loss = 0.27180173\n",
      "Iteration 57, loss = 0.28151789\n",
      "Iteration 58, loss = 0.27370934\n",
      "Iteration 59, loss = 0.28006616\n",
      "Iteration 60, loss = 0.26459335\n",
      "Iteration 61, loss = 0.25929371\n",
      "Iteration 62, loss = 0.25870874\n",
      "Iteration 63, loss = 0.26747803\n",
      "Iteration 64, loss = 0.26267050\n",
      "Iteration 65, loss = 0.26946839\n",
      "Iteration 66, loss = 0.26255822\n",
      "Iteration 67, loss = 0.25981346\n",
      "Iteration 68, loss = 0.26545956\n",
      "Iteration 69, loss = 0.25572618\n",
      "Iteration 70, loss = 0.26180397\n",
      "Iteration 71, loss = 0.25509809\n",
      "Iteration 72, loss = 0.25674448\n",
      "Iteration 73, loss = 0.26606639\n",
      "Iteration 74, loss = 0.25889063\n",
      "Iteration 75, loss = 0.26214905\n",
      "Iteration 76, loss = 0.25531855\n",
      "Iteration 77, loss = 0.25298689\n",
      "Iteration 78, loss = 0.25719598\n",
      "Iteration 79, loss = 0.27511265\n",
      "Iteration 80, loss = 0.28096817\n",
      "Iteration 81, loss = 0.27138671\n",
      "Iteration 82, loss = 0.28442655\n",
      "Iteration 83, loss = 0.26860216\n",
      "Iteration 84, loss = 0.25830629\n",
      "Iteration 85, loss = 0.25722118\n",
      "Iteration 86, loss = 0.25446501\n",
      "Iteration 87, loss = 0.26586153\n",
      "Iteration 88, loss = 0.26458650\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.47559685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 3.43082727\n",
      "Iteration 3, loss = 2.62217306\n",
      "Iteration 4, loss = 2.03079360\n",
      "Iteration 5, loss = 1.60522250\n",
      "Iteration 6, loss = 1.31348917\n",
      "Iteration 7, loss = 1.09787754\n",
      "Iteration 8, loss = 0.93130537\n",
      "Iteration 9, loss = 0.83553543\n",
      "Iteration 10, loss = 0.73047409\n",
      "Iteration 11, loss = 0.65130997\n",
      "Iteration 12, loss = 0.59537610\n",
      "Iteration 13, loss = 0.54382376\n",
      "Iteration 14, loss = 0.52642431\n",
      "Iteration 15, loss = 0.48817596\n",
      "Iteration 16, loss = 0.48104245\n",
      "Iteration 17, loss = 0.43363343\n",
      "Iteration 18, loss = 0.42229961\n",
      "Iteration 19, loss = 0.41604031\n",
      "Iteration 20, loss = 0.41441602\n",
      "Iteration 21, loss = 0.38021694\n",
      "Iteration 22, loss = 0.36822826\n",
      "Iteration 23, loss = 0.35279846\n",
      "Iteration 24, loss = 0.36064979\n",
      "Iteration 25, loss = 0.38188150\n",
      "Iteration 26, loss = 0.36153995\n",
      "Iteration 27, loss = 0.33823727\n",
      "Iteration 28, loss = 0.33842853\n",
      "Iteration 29, loss = 0.33417804\n",
      "Iteration 30, loss = 0.31104140\n",
      "Iteration 31, loss = 0.30978946\n",
      "Iteration 32, loss = 0.30632734\n",
      "Iteration 33, loss = 0.29373874\n",
      "Iteration 34, loss = 0.29831726\n",
      "Iteration 35, loss = 0.30348914\n",
      "Iteration 36, loss = 0.30568464\n",
      "Iteration 37, loss = 0.30490950\n",
      "Iteration 38, loss = 0.31902252\n",
      "Iteration 39, loss = 0.30412594\n",
      "Iteration 40, loss = 0.29740355\n",
      "Iteration 41, loss = 0.31862207\n",
      "Iteration 42, loss = 0.30294461\n",
      "Iteration 43, loss = 0.29838535\n",
      "Iteration 44, loss = 0.28484913\n",
      "Iteration 45, loss = 0.30336593\n",
      "Iteration 46, loss = 0.31847055\n",
      "Iteration 47, loss = 0.30460965\n",
      "Iteration 48, loss = 0.29998727\n",
      "Iteration 49, loss = 0.29726310\n",
      "Iteration 50, loss = 0.30220273\n",
      "Iteration 51, loss = 0.28440942\n",
      "Iteration 52, loss = 0.29787285\n",
      "Iteration 53, loss = 0.32339097\n",
      "Iteration 54, loss = 0.29663313\n",
      "Iteration 55, loss = 0.28214100\n",
      "Iteration 56, loss = 0.28403713\n",
      "Iteration 57, loss = 0.28459553\n",
      "Iteration 58, loss = 0.27870436\n",
      "Iteration 59, loss = 0.29080499\n",
      "Iteration 60, loss = 0.27520771\n",
      "Iteration 61, loss = 0.26957156\n",
      "Iteration 62, loss = 0.26913398\n",
      "Iteration 63, loss = 0.27891839\n",
      "Iteration 64, loss = 0.27278742\n",
      "Iteration 65, loss = 0.28224156\n",
      "Iteration 66, loss = 0.27390027\n",
      "Iteration 67, loss = 0.26985402\n",
      "Iteration 68, loss = 0.27572594\n",
      "Iteration 69, loss = 0.26614265\n",
      "Iteration 70, loss = 0.26722812\n",
      "Iteration 71, loss = 0.26465789\n",
      "Iteration 72, loss = 0.26378494\n",
      "Iteration 73, loss = 0.27499715\n",
      "Iteration 74, loss = 0.26844791\n",
      "Iteration 75, loss = 0.27161735\n",
      "Iteration 76, loss = 0.26481713\n",
      "Iteration 77, loss = 0.26225621\n",
      "Iteration 78, loss = 0.26813706\n",
      "Iteration 79, loss = 0.28666234\n",
      "Iteration 80, loss = 0.29121473\n",
      "Iteration 81, loss = 0.28793561\n",
      "Iteration 82, loss = 0.29632669\n",
      "Iteration 83, loss = 0.27989966\n",
      "Iteration 84, loss = 0.26660192\n",
      "Iteration 85, loss = 0.26472565\n",
      "Iteration 86, loss = 0.26293755\n",
      "Iteration 87, loss = 0.27695051\n",
      "Iteration 88, loss = 0.27744897\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.66443078\n",
      "Iteration 2, loss = 3.55092872\n",
      "Iteration 3, loss = 2.69245931\n",
      "Iteration 4, loss = 2.07039050\n",
      "Iteration 5, loss = 1.62459583\n",
      "Iteration 6, loss = 1.32523169\n",
      "Iteration 7, loss = 1.10291961\n",
      "Iteration 8, loss = 0.92997578\n",
      "Iteration 9, loss = 0.83728221\n",
      "Iteration 10, loss = 0.73289207\n",
      "Iteration 11, loss = 0.65475191\n",
      "Iteration 12, loss = 0.60378373\n",
      "Iteration 13, loss = 0.55932895\n",
      "Iteration 14, loss = 0.52407703\n",
      "Iteration 15, loss = 0.49033880\n",
      "Iteration 16, loss = 0.47335087\n",
      "Iteration 17, loss = 0.43893456\n",
      "Iteration 18, loss = 0.42826363\n",
      "Iteration 19, loss = 0.42626375\n",
      "Iteration 20, loss = 0.41960078\n",
      "Iteration 21, loss = 0.39023164\n",
      "Iteration 22, loss = 0.37962467\n",
      "Iteration 23, loss = 0.36189804\n",
      "Iteration 24, loss = 0.37480568\n",
      "Iteration 25, loss = 0.38741539\n",
      "Iteration 26, loss = 0.36891990\n",
      "Iteration 27, loss = 0.35069665\n",
      "Iteration 28, loss = 0.34121634\n",
      "Iteration 29, loss = 0.33982681\n",
      "Iteration 30, loss = 0.31859273\n",
      "Iteration 31, loss = 0.31853193\n",
      "Iteration 32, loss = 0.31884134\n",
      "Iteration 33, loss = 0.30546269\n",
      "Iteration 34, loss = 0.30861995\n",
      "Iteration 35, loss = 0.31068339\n",
      "Iteration 36, loss = 0.31972750\n",
      "Iteration 37, loss = 0.31512933\n",
      "Iteration 38, loss = 0.33118775\n",
      "Iteration 39, loss = 0.31810202\n",
      "Iteration 40, loss = 0.31020401\n",
      "Iteration 41, loss = 0.32348373\n",
      "Iteration 42, loss = 0.31290512\n",
      "Iteration 43, loss = 0.31013279\n",
      "Iteration 44, loss = 0.30140398\n",
      "Iteration 45, loss = 0.30612760\n",
      "Iteration 46, loss = 0.31887621\n",
      "Iteration 47, loss = 0.31533673\n",
      "Iteration 48, loss = 0.30999806\n",
      "Iteration 49, loss = 0.31172607\n",
      "Iteration 50, loss = 0.31011180\n",
      "Iteration 51, loss = 0.29198261\n",
      "Iteration 52, loss = 0.30203375\n",
      "Iteration 53, loss = 0.32724651\n",
      "Iteration 54, loss = 0.30725351\n",
      "Iteration 55, loss = 0.29386511\n",
      "Iteration 56, loss = 0.29635876\n",
      "Iteration 57, loss = 0.29494149\n",
      "Iteration 58, loss = 0.29017973\n",
      "Iteration 59, loss = 0.30041986\n",
      "Iteration 60, loss = 0.28464480\n",
      "Iteration 61, loss = 0.28028871\n",
      "Iteration 62, loss = 0.27867362\n",
      "Iteration 63, loss = 0.28861154\n",
      "Iteration 64, loss = 0.28314365\n",
      "Iteration 65, loss = 0.29596817\n",
      "Iteration 66, loss = 0.28071659\n",
      "Iteration 67, loss = 0.27908618\n",
      "Iteration 68, loss = 0.28261883\n",
      "Iteration 69, loss = 0.27797278\n",
      "Iteration 70, loss = 0.27682467\n",
      "Iteration 71, loss = 0.27401390\n",
      "Iteration 72, loss = 0.27411183\n",
      "Iteration 73, loss = 0.28460805\n",
      "Iteration 74, loss = 0.27766005\n",
      "Iteration 75, loss = 0.28464809\n",
      "Iteration 76, loss = 0.27805852\n",
      "Iteration 77, loss = 0.27467671\n",
      "Iteration 78, loss = 0.27854914\n",
      "Iteration 79, loss = 0.30185470\n",
      "Iteration 80, loss = 0.31945118\n",
      "Iteration 81, loss = 0.31088764\n",
      "Iteration 82, loss = 0.28175538\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.85162658\n",
      "Iteration 2, loss = 3.66470896\n",
      "Iteration 3, loss = 2.75611296\n",
      "Iteration 4, loss = 2.10385830\n",
      "Iteration 5, loss = 1.64395750\n",
      "Iteration 6, loss = 1.33644745\n",
      "Iteration 7, loss = 1.11105088\n",
      "Iteration 8, loss = 0.93482309\n",
      "Iteration 9, loss = 0.83888262\n",
      "Iteration 10, loss = 0.73267210\n",
      "Iteration 11, loss = 0.65657924\n",
      "Iteration 12, loss = 0.60514602\n",
      "Iteration 13, loss = 0.56333564\n",
      "Iteration 14, loss = 0.52457212\n",
      "Iteration 15, loss = 0.49416329\n",
      "Iteration 16, loss = 0.47241101\n",
      "Iteration 17, loss = 0.44096360\n",
      "Iteration 18, loss = 0.43094815\n",
      "Iteration 19, loss = 0.42998977\n",
      "Iteration 20, loss = 0.42929349\n",
      "Iteration 21, loss = 0.39486572\n",
      "Iteration 22, loss = 0.38580644\n",
      "Iteration 23, loss = 0.36780477\n",
      "Iteration 24, loss = 0.37636966\n",
      "Iteration 25, loss = 0.38624900\n",
      "Iteration 26, loss = 0.37266309\n",
      "Iteration 27, loss = 0.35286475\n",
      "Iteration 28, loss = 0.35106589\n",
      "Iteration 29, loss = 0.34557736\n",
      "Iteration 30, loss = 0.32622247\n",
      "Iteration 31, loss = 0.33154611\n",
      "Iteration 32, loss = 0.32781580\n",
      "Iteration 33, loss = 0.31296101\n",
      "Iteration 34, loss = 0.31439173\n",
      "Iteration 35, loss = 0.31881379\n",
      "Iteration 36, loss = 0.32701252\n",
      "Iteration 37, loss = 0.32194784\n",
      "Iteration 38, loss = 0.33882883\n",
      "Iteration 39, loss = 0.32583285\n",
      "Iteration 40, loss = 0.31862570\n",
      "Iteration 41, loss = 0.33696286\n",
      "Iteration 42, loss = 0.32329048\n",
      "Iteration 43, loss = 0.31759795\n",
      "Iteration 44, loss = 0.31594590\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.03855792\n",
      "Iteration 2, loss = 3.77625702\n",
      "Iteration 3, loss = 2.81665108\n",
      "Iteration 4, loss = 2.13274288\n",
      "Iteration 5, loss = 1.65647316\n",
      "Iteration 6, loss = 1.34077185\n",
      "Iteration 7, loss = 1.11075365\n",
      "Iteration 8, loss = 0.93302432\n",
      "Iteration 9, loss = 0.84035709\n",
      "Iteration 10, loss = 0.73049295\n",
      "Iteration 11, loss = 0.65554847\n",
      "Iteration 12, loss = 0.60331320\n",
      "Iteration 13, loss = 0.56125508\n",
      "Iteration 14, loss = 0.52723733\n",
      "Iteration 15, loss = 0.49224822\n",
      "Iteration 16, loss = 0.47998095\n",
      "Iteration 17, loss = 0.45010509\n",
      "Iteration 18, loss = 0.43641595\n",
      "Iteration 19, loss = 0.43273281\n",
      "Iteration 20, loss = 0.42373945\n",
      "Iteration 21, loss = 0.39549163\n",
      "Iteration 22, loss = 0.39103443\n",
      "Iteration 23, loss = 0.37432775\n",
      "Iteration 24, loss = 0.38839886\n",
      "Iteration 25, loss = 0.38819599\n",
      "Iteration 26, loss = 0.37613784\n",
      "Iteration 27, loss = 0.36335950\n",
      "Iteration 28, loss = 0.36015003\n",
      "Iteration 29, loss = 0.35372003\n",
      "Iteration 30, loss = 0.33272624\n",
      "Iteration 31, loss = 0.33862470\n",
      "Iteration 32, loss = 0.33649480\n",
      "Iteration 33, loss = 0.32338554\n",
      "Iteration 34, loss = 0.32819945\n",
      "Iteration 35, loss = 0.32771073\n",
      "Iteration 36, loss = 0.33465697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37, loss = 0.33099159\n",
      "Iteration 38, loss = 0.34338846\n",
      "Iteration 39, loss = 0.33110770\n",
      "Iteration 40, loss = 0.32260298\n",
      "Iteration 41, loss = 0.33674255\n",
      "Iteration 42, loss = 0.33262604\n",
      "Iteration 43, loss = 0.32136132\n",
      "Iteration 44, loss = 0.31516279\n",
      "Iteration 45, loss = 0.32600495\n",
      "Iteration 46, loss = 0.34190913\n",
      "Iteration 47, loss = 0.33307804\n",
      "Iteration 48, loss = 0.32604643\n",
      "Iteration 49, loss = 0.32816272\n",
      "Iteration 50, loss = 0.32476127\n",
      "Iteration 51, loss = 0.32033022\n",
      "Iteration 52, loss = 0.32446165\n",
      "Iteration 53, loss = 0.34261882\n",
      "Iteration 54, loss = 0.32218058\n",
      "Iteration 55, loss = 0.31145255\n",
      "Iteration 56, loss = 0.31363033\n",
      "Iteration 57, loss = 0.30547759\n",
      "Iteration 58, loss = 0.30586338\n",
      "Iteration 59, loss = 0.31844627\n",
      "Iteration 60, loss = 0.30838409\n",
      "Iteration 61, loss = 0.30254433\n",
      "Iteration 62, loss = 0.29957644\n",
      "Iteration 63, loss = 0.30755892\n",
      "Iteration 64, loss = 0.30203590\n",
      "Iteration 65, loss = 0.31278370\n",
      "Iteration 66, loss = 0.29703536\n",
      "Iteration 67, loss = 0.29868431\n",
      "Iteration 68, loss = 0.30555855\n",
      "Iteration 69, loss = 0.29838304\n",
      "Iteration 70, loss = 0.29821938\n",
      "Iteration 71, loss = 0.29467226\n",
      "Iteration 72, loss = 0.29309344\n",
      "Iteration 73, loss = 0.30528033\n",
      "Iteration 74, loss = 0.30190296\n",
      "Iteration 75, loss = 0.30803422\n",
      "Iteration 76, loss = 0.29954761\n",
      "Iteration 77, loss = 0.29713478\n",
      "Iteration 78, loss = 0.30852948\n",
      "Iteration 79, loss = 0.34525646\n",
      "Iteration 80, loss = 0.31786023\n",
      "Iteration 81, loss = 0.31040429\n",
      "Iteration 82, loss = 0.29809811\n",
      "Iteration 83, loss = 0.29231598\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.22414775\n",
      "Iteration 2, loss = 3.88384329\n",
      "Iteration 3, loss = 2.87209669\n",
      "Iteration 4, loss = 2.15991903\n",
      "Iteration 5, loss = 1.66830449\n",
      "Iteration 6, loss = 1.34384202\n",
      "Iteration 7, loss = 1.10898419\n",
      "Iteration 8, loss = 0.93315790\n",
      "Iteration 9, loss = 0.83384056\n",
      "Iteration 10, loss = 0.72872978\n",
      "Iteration 11, loss = 0.65502729\n",
      "Iteration 12, loss = 0.60410661\n",
      "Iteration 13, loss = 0.56389025\n",
      "Iteration 14, loss = 0.52624302\n",
      "Iteration 15, loss = 0.49883190\n",
      "Iteration 16, loss = 0.47801496\n",
      "Iteration 17, loss = 0.44874740\n",
      "Iteration 18, loss = 0.43940028\n",
      "Iteration 19, loss = 0.43996407\n",
      "Iteration 20, loss = 0.42879066\n",
      "Iteration 21, loss = 0.39973043\n",
      "Iteration 22, loss = 0.39885008\n",
      "Iteration 23, loss = 0.37889292\n",
      "Iteration 24, loss = 0.38410361\n",
      "Iteration 25, loss = 0.39194466\n",
      "Iteration 26, loss = 0.38130308\n",
      "Iteration 27, loss = 0.36873611\n",
      "Iteration 28, loss = 0.36829964\n",
      "Iteration 29, loss = 0.36469041\n",
      "Iteration 30, loss = 0.34419419\n",
      "Iteration 31, loss = 0.34855954\n",
      "Iteration 32, loss = 0.34503693\n",
      "Iteration 33, loss = 0.33001638\n",
      "Iteration 34, loss = 0.33346484\n",
      "Iteration 35, loss = 0.33373138\n",
      "Iteration 36, loss = 0.34008672\n",
      "Iteration 37, loss = 0.33967262\n",
      "Iteration 38, loss = 0.35217390\n",
      "Iteration 39, loss = 0.34553191\n",
      "Iteration 40, loss = 0.33291578\n",
      "Iteration 41, loss = 0.34546602\n",
      "Iteration 42, loss = 0.33596668\n",
      "Iteration 43, loss = 0.33016521\n",
      "Iteration 44, loss = 0.32971338\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.40883286\n",
      "Iteration 2, loss = 3.99201532\n",
      "Iteration 3, loss = 2.92936089\n",
      "Iteration 4, loss = 2.18991187\n",
      "Iteration 5, loss = 1.68418171\n",
      "Iteration 6, loss = 1.35047792\n",
      "Iteration 7, loss = 1.11046678\n",
      "Iteration 8, loss = 0.93269096\n",
      "Iteration 9, loss = 0.83630565\n",
      "Iteration 10, loss = 0.72917118\n",
      "Iteration 11, loss = 0.65202999\n",
      "Iteration 12, loss = 0.60480129\n",
      "Iteration 13, loss = 0.56572928\n",
      "Iteration 14, loss = 0.53331631\n",
      "Iteration 15, loss = 0.50438277\n",
      "Iteration 16, loss = 0.48140005\n",
      "Iteration 17, loss = 0.45590839\n",
      "Iteration 18, loss = 0.44377332\n",
      "Iteration 19, loss = 0.44737392\n",
      "Iteration 20, loss = 0.43274749\n",
      "Iteration 21, loss = 0.40513528\n",
      "Iteration 22, loss = 0.40409004\n",
      "Iteration 23, loss = 0.38822191\n",
      "Iteration 24, loss = 0.40079327\n",
      "Iteration 25, loss = 0.39503470\n",
      "Iteration 26, loss = 0.38738732\n",
      "Iteration 27, loss = 0.37870729\n",
      "Iteration 28, loss = 0.37673484\n",
      "Iteration 29, loss = 0.37883840\n",
      "Iteration 30, loss = 0.35665673\n",
      "Iteration 31, loss = 0.35828805\n",
      "Iteration 32, loss = 0.35496116\n",
      "Iteration 33, loss = 0.34039189\n",
      "Iteration 34, loss = 0.34818794\n",
      "Iteration 35, loss = 0.34223564\n",
      "Iteration 36, loss = 0.35071725\n",
      "Iteration 37, loss = 0.34754645\n",
      "Iteration 38, loss = 0.36774824\n",
      "Iteration 39, loss = 0.35502921\n",
      "Iteration 40, loss = 0.33611014\n",
      "Iteration 41, loss = 0.35419122\n",
      "Iteration 42, loss = 0.35108031\n",
      "Iteration 43, loss = 0.34357723\n",
      "Iteration 44, loss = 0.34718617\n",
      "Iteration 45, loss = 0.33289864\n",
      "Iteration 46, loss = 0.34233873\n",
      "Iteration 47, loss = 0.34234245\n",
      "Iteration 48, loss = 0.34147407\n",
      "Iteration 49, loss = 0.34989395\n",
      "Iteration 50, loss = 0.34577107\n",
      "Iteration 51, loss = 0.33860249\n",
      "Iteration 52, loss = 0.34830688\n",
      "Iteration 53, loss = 0.35386964\n",
      "Iteration 54, loss = 0.33622945\n",
      "Iteration 55, loss = 0.32910417\n",
      "Iteration 56, loss = 0.33480194\n",
      "Iteration 57, loss = 0.32359360\n",
      "Iteration 58, loss = 0.32477830\n",
      "Iteration 59, loss = 0.34442555\n",
      "Iteration 60, loss = 0.32579601\n",
      "Iteration 61, loss = 0.31913111\n",
      "Iteration 62, loss = 0.31776302\n",
      "Iteration 63, loss = 0.32879949\n",
      "Iteration 64, loss = 0.32404333\n",
      "Iteration 65, loss = 0.33084125\n",
      "Iteration 66, loss = 0.31471906\n",
      "Iteration 67, loss = 0.31545656\n",
      "Iteration 68, loss = 0.32862478\n",
      "Iteration 69, loss = 0.31996737\n",
      "Iteration 70, loss = 0.31833689\n",
      "Iteration 71, loss = 0.31233099\n",
      "Iteration 72, loss = 0.31386435\n",
      "Iteration 73, loss = 0.33402291\n",
      "Iteration 74, loss = 0.32691259\n",
      "Iteration 75, loss = 0.32112689\n",
      "Iteration 76, loss = 0.31408518\n",
      "Iteration 77, loss = 0.31220344\n",
      "Iteration 78, loss = 0.32332948\n",
      "Iteration 79, loss = 0.36245857\n",
      "Iteration 80, loss = 0.33298525\n",
      "Iteration 81, loss = 0.33707909\n",
      "Iteration 82, loss = 0.31654076\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65041533\n",
      "Iteration 2, loss = 0.44314215\n",
      "Iteration 3, loss = 0.36910489\n",
      "Iteration 4, loss = 0.31438861\n",
      "Iteration 5, loss = 0.26172380\n",
      "Iteration 6, loss = 0.21812871\n",
      "Iteration 7, loss = 0.17647562\n",
      "Iteration 8, loss = 0.14078720\n",
      "Iteration 9, loss = 0.11049524\n",
      "Iteration 10, loss = 0.08853522\n",
      "Iteration 11, loss = 0.08467021\n",
      "Iteration 12, loss = 0.05909047\n",
      "Iteration 13, loss = 0.04348672\n",
      "Iteration 14, loss = 0.02928940\n",
      "Iteration 15, loss = 0.02209414\n",
      "Iteration 16, loss = 0.01699066\n",
      "Iteration 17, loss = 0.02019090\n",
      "Iteration 18, loss = 0.01286844\n",
      "Iteration 19, loss = 0.00741434\n",
      "Iteration 20, loss = 0.00591122\n",
      "Iteration 21, loss = 0.00410120\n",
      "Iteration 22, loss = 0.00325132\n",
      "Iteration 23, loss = 0.00252282\n",
      "Iteration 24, loss = 0.00215077\n",
      "Iteration 25, loss = 0.00191772\n",
      "Iteration 26, loss = 0.00187619\n",
      "Iteration 27, loss = 0.00173162\n",
      "Iteration 28, loss = 0.00146203\n",
      "Iteration 29, loss = 0.00126664\n",
      "Iteration 30, loss = 0.00109995\n",
      "Iteration 31, loss = 0.00100145\n",
      "Iteration 32, loss = 0.00091554\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86756277\n",
      "Iteration 2, loss = 0.65812370\n",
      "Iteration 3, loss = 0.58536288\n",
      "Iteration 4, loss = 0.53150425\n",
      "Iteration 5, loss = 0.47622502\n",
      "Iteration 6, loss = 0.42399221\n",
      "Iteration 7, loss = 0.38489532\n",
      "Iteration 8, loss = 0.34755677\n",
      "Iteration 9, loss = 0.31310594\n",
      "Iteration 10, loss = 0.29180804\n",
      "Iteration 11, loss = 0.28622618\n",
      "Iteration 12, loss = 0.25146838\n",
      "Iteration 13, loss = 0.23276015\n",
      "Iteration 14, loss = 0.21507991\n",
      "Iteration 15, loss = 0.20067582\n",
      "Iteration 16, loss = 0.19471212\n",
      "Iteration 17, loss = 0.19976200\n",
      "Iteration 18, loss = 0.22476753\n",
      "Iteration 19, loss = 0.19251665\n",
      "Iteration 20, loss = 0.17745759\n",
      "Iteration 21, loss = 0.16412889\n",
      "Iteration 22, loss = 0.15670180\n",
      "Iteration 23, loss = 0.15102139\n",
      "Iteration 24, loss = 0.14705054\n",
      "Iteration 25, loss = 0.14230646\n",
      "Iteration 26, loss = 0.13889746\n",
      "Iteration 27, loss = 0.13510306\n",
      "Iteration 28, loss = 0.13102644\n",
      "Iteration 29, loss = 0.12764736\n",
      "Iteration 30, loss = 0.12398560\n",
      "Iteration 31, loss = 0.12054826\n",
      "Iteration 32, loss = 0.11736227\n",
      "Iteration 33, loss = 0.11426586\n",
      "Iteration 34, loss = 0.11122059\n",
      "Iteration 35, loss = 0.10817149\n",
      "Iteration 36, loss = 0.10531295\n",
      "Iteration 37, loss = 0.10276549\n",
      "Iteration 38, loss = 0.09999093\n",
      "Iteration 39, loss = 0.09729964\n",
      "Iteration 40, loss = 0.09466991\n",
      "Iteration 41, loss = 0.09265472\n",
      "Iteration 42, loss = 0.09038061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43, loss = 0.08786295\n",
      "Iteration 44, loss = 0.08586485\n",
      "Iteration 45, loss = 0.08362964\n",
      "Iteration 46, loss = 0.08194278\n",
      "Iteration 47, loss = 0.07959831\n",
      "Iteration 48, loss = 0.07735690\n",
      "Iteration 49, loss = 0.07540498\n",
      "Iteration 50, loss = 0.07365676\n",
      "Iteration 51, loss = 0.07202830\n",
      "Iteration 52, loss = 0.07038601\n",
      "Iteration 53, loss = 0.06941959\n",
      "Iteration 54, loss = 0.29373708\n",
      "Iteration 55, loss = 0.36824262\n",
      "Iteration 56, loss = 0.31434309\n",
      "Iteration 57, loss = 0.23308600\n",
      "Iteration 58, loss = 0.16394399\n",
      "Iteration 59, loss = 0.15043323\n",
      "Iteration 60, loss = 0.12043361\n",
      "Iteration 61, loss = 0.10359333\n",
      "Iteration 62, loss = 0.09446540\n",
      "Iteration 63, loss = 0.08742495\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08257686\n",
      "Iteration 2, loss = 0.86466386\n",
      "Iteration 3, loss = 0.77605405\n",
      "Iteration 4, loss = 0.70512305\n",
      "Iteration 5, loss = 0.64025993\n",
      "Iteration 6, loss = 0.58747601\n",
      "Iteration 7, loss = 0.53141034\n",
      "Iteration 8, loss = 0.48338914\n",
      "Iteration 9, loss = 0.44581065\n",
      "Iteration 10, loss = 0.42052578\n",
      "Iteration 11, loss = 0.41848721\n",
      "Iteration 12, loss = 0.37566491\n",
      "Iteration 13, loss = 0.33822570\n",
      "Iteration 14, loss = 0.31304932\n",
      "Iteration 15, loss = 0.30510330\n",
      "Iteration 16, loss = 0.28660047\n",
      "Iteration 17, loss = 0.29340095\n",
      "Iteration 18, loss = 0.26782154\n",
      "Iteration 19, loss = 0.25334463\n",
      "Iteration 20, loss = 0.25081598\n",
      "Iteration 21, loss = 0.30101311\n",
      "Iteration 22, loss = 0.25377814\n",
      "Iteration 23, loss = 0.23426942\n",
      "Iteration 24, loss = 0.23729875\n",
      "Iteration 25, loss = 0.21905848\n",
      "Iteration 26, loss = 0.20778777\n",
      "Iteration 27, loss = 0.19752053\n",
      "Iteration 28, loss = 0.18623699\n",
      "Iteration 29, loss = 0.17857850\n",
      "Iteration 30, loss = 0.17131273\n",
      "Iteration 31, loss = 0.16486017\n",
      "Iteration 32, loss = 0.15909577\n",
      "Iteration 33, loss = 0.15360532\n",
      "Iteration 34, loss = 0.14827762\n",
      "Iteration 35, loss = 0.14328582\n",
      "Iteration 36, loss = 0.13854071\n",
      "Iteration 37, loss = 0.13494144\n",
      "Iteration 38, loss = 0.12985150\n",
      "Iteration 39, loss = 0.12584903\n",
      "Iteration 40, loss = 0.12184018\n",
      "Iteration 41, loss = 0.11878696\n",
      "Iteration 42, loss = 0.11577083\n",
      "Iteration 43, loss = 0.11253464\n",
      "Iteration 44, loss = 0.10987146\n",
      "Iteration 45, loss = 0.10615872\n",
      "Iteration 46, loss = 0.10341388\n",
      "Iteration 47, loss = 0.10005020\n",
      "Iteration 48, loss = 0.09703666\n",
      "Iteration 49, loss = 0.09447825\n",
      "Iteration 50, loss = 0.09201012\n",
      "Iteration 51, loss = 0.09759827\n",
      "Iteration 52, loss = 0.16734626\n",
      "Iteration 53, loss = 0.33094444\n",
      "Iteration 54, loss = 0.19920120\n",
      "Iteration 55, loss = 0.18150636\n",
      "Iteration 56, loss = 0.14908011\n",
      "Iteration 57, loss = 0.12320012\n",
      "Iteration 58, loss = 0.11132086\n",
      "Iteration 59, loss = 0.10490029\n",
      "Iteration 60, loss = 0.10027699\n",
      "Iteration 61, loss = 0.09669491\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.29631631\n",
      "Iteration 2, loss = 1.06079700\n",
      "Iteration 3, loss = 0.95638721\n",
      "Iteration 4, loss = 0.87284749\n",
      "Iteration 5, loss = 0.78983574\n",
      "Iteration 6, loss = 0.71520163\n",
      "Iteration 7, loss = 0.64982021\n",
      "Iteration 8, loss = 0.59077862\n",
      "Iteration 9, loss = 0.53927367\n",
      "Iteration 10, loss = 0.50225163\n",
      "Iteration 11, loss = 0.48281327\n",
      "Iteration 12, loss = 0.45007976\n",
      "Iteration 13, loss = 0.40948199\n",
      "Iteration 14, loss = 0.37734257\n",
      "Iteration 15, loss = 0.35086867\n",
      "Iteration 16, loss = 0.33485765\n",
      "Iteration 17, loss = 0.32452358\n",
      "Iteration 18, loss = 0.31728721\n",
      "Iteration 19, loss = 0.28734900\n",
      "Iteration 20, loss = 0.28533295\n",
      "Iteration 21, loss = 0.27504948\n",
      "Iteration 22, loss = 0.24655350\n",
      "Iteration 23, loss = 0.23462570\n",
      "Iteration 24, loss = 0.22395317\n",
      "Iteration 25, loss = 0.21136073\n",
      "Iteration 26, loss = 0.20138867\n",
      "Iteration 27, loss = 0.19481824\n",
      "Iteration 28, loss = 0.18574544\n",
      "Iteration 29, loss = 0.18073683\n",
      "Iteration 30, loss = 0.17240916\n",
      "Iteration 31, loss = 0.16644670\n",
      "Iteration 32, loss = 0.15973684\n",
      "Iteration 33, loss = 0.15497242\n",
      "Iteration 34, loss = 0.14773852\n",
      "Iteration 35, loss = 0.14455739\n",
      "Iteration 36, loss = 0.14105898\n",
      "Iteration 37, loss = 0.13557118\n",
      "Iteration 38, loss = 0.13102969\n",
      "Iteration 39, loss = 0.12661801\n",
      "Iteration 40, loss = 0.12429590\n",
      "Iteration 41, loss = 0.13845964\n",
      "Iteration 42, loss = 0.23949980\n",
      "Iteration 43, loss = 0.21380045\n",
      "Iteration 44, loss = 0.17986545\n",
      "Iteration 45, loss = 0.20691973\n",
      "Iteration 46, loss = 0.16965906\n",
      "Iteration 47, loss = 0.14319264\n",
      "Iteration 48, loss = 0.12820020\n",
      "Iteration 49, loss = 0.12138015\n",
      "Iteration 50, loss = 0.11640041\n",
      "Iteration 51, loss = 0.11174308\n",
      "Iteration 52, loss = 0.10797879\n",
      "Iteration 53, loss = 0.10459657\n",
      "Iteration 54, loss = 0.10131692\n",
      "Iteration 55, loss = 0.09872427\n",
      "Iteration 56, loss = 0.09606419\n",
      "Iteration 57, loss = 0.09416319\n",
      "Iteration 58, loss = 0.09267097\n",
      "Iteration 59, loss = 0.09006920\n",
      "Iteration 60, loss = 0.08883784\n",
      "Iteration 61, loss = 0.08644680\n",
      "Iteration 62, loss = 0.08450836\n",
      "Iteration 63, loss = 0.08384807\n",
      "Iteration 64, loss = 0.08194338\n",
      "Iteration 65, loss = 0.08093435\n",
      "Iteration 66, loss = 0.08056117\n",
      "Iteration 67, loss = 0.07900470\n",
      "Iteration 68, loss = 0.07741120\n",
      "Iteration 69, loss = 0.07798210\n",
      "Iteration 70, loss = 0.07662566\n",
      "Iteration 71, loss = 0.07479951\n",
      "Iteration 72, loss = 0.07551231\n",
      "Iteration 73, loss = 0.07433157\n",
      "Iteration 74, loss = 0.07412815\n",
      "Iteration 75, loss = 0.07357389\n",
      "Iteration 76, loss = 0.07445178\n",
      "Iteration 77, loss = 0.07373689\n",
      "Iteration 78, loss = 0.07300073\n",
      "Iteration 79, loss = 0.07199214\n",
      "Iteration 80, loss = 0.06962558\n",
      "Iteration 81, loss = 0.06872479\n",
      "Iteration 82, loss = 0.06884598\n",
      "Iteration 83, loss = 0.06999411\n",
      "Iteration 84, loss = 0.07005638\n",
      "Iteration 85, loss = 0.14854475\n",
      "Iteration 86, loss = 0.21468914\n",
      "Iteration 87, loss = 0.25242023\n",
      "Iteration 88, loss = 0.16490305\n",
      "Iteration 89, loss = 0.13766489\n",
      "Iteration 90, loss = 0.11903469\n",
      "Iteration 91, loss = 0.10711904\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50774251\n",
      "Iteration 2, loss = 1.25006200\n",
      "Iteration 3, loss = 1.11715099\n",
      "Iteration 4, loss = 1.00887430\n",
      "Iteration 5, loss = 0.90619981\n",
      "Iteration 6, loss = 0.82187815\n",
      "Iteration 7, loss = 0.73763518\n",
      "Iteration 8, loss = 0.66587738\n",
      "Iteration 9, loss = 0.60769655\n",
      "Iteration 10, loss = 0.56088467\n",
      "Iteration 11, loss = 0.52767881\n",
      "Iteration 12, loss = 0.48970295\n",
      "Iteration 13, loss = 0.44975928\n",
      "Iteration 14, loss = 0.41535221\n",
      "Iteration 15, loss = 0.37714518\n",
      "Iteration 16, loss = 0.35354616\n",
      "Iteration 17, loss = 0.35199571\n",
      "Iteration 18, loss = 0.33218500\n",
      "Iteration 19, loss = 0.31192104\n",
      "Iteration 20, loss = 0.32207163\n",
      "Iteration 21, loss = 0.36177129\n",
      "Iteration 22, loss = 0.28954596\n",
      "Iteration 23, loss = 0.26295843\n",
      "Iteration 24, loss = 0.24551234\n",
      "Iteration 25, loss = 0.23134179\n",
      "Iteration 26, loss = 0.22156138\n",
      "Iteration 27, loss = 0.21344602\n",
      "Iteration 28, loss = 0.20239629\n",
      "Iteration 29, loss = 0.19552763\n",
      "Iteration 30, loss = 0.18515030\n",
      "Iteration 31, loss = 0.17891090\n",
      "Iteration 32, loss = 0.17105605\n",
      "Iteration 33, loss = 0.16611755\n",
      "Iteration 34, loss = 0.15942805\n",
      "Iteration 35, loss = 0.15495968\n",
      "Iteration 36, loss = 0.15188374\n",
      "Iteration 37, loss = 0.14977047\n",
      "Iteration 38, loss = 0.14431012\n",
      "Iteration 39, loss = 0.13952030\n",
      "Iteration 40, loss = 0.13741691\n",
      "Iteration 41, loss = 0.26368016\n",
      "Iteration 42, loss = 0.23305061\n",
      "Iteration 43, loss = 0.20329855\n",
      "Iteration 44, loss = 0.18810179\n",
      "Iteration 45, loss = 0.16612472\n",
      "Iteration 46, loss = 0.15476497\n",
      "Iteration 47, loss = 0.14117878\n",
      "Iteration 48, loss = 0.13175702\n",
      "Iteration 49, loss = 0.12601188\n",
      "Iteration 50, loss = 0.12241891\n",
      "Iteration 51, loss = 0.11789317\n",
      "Iteration 52, loss = 0.11401901\n",
      "Iteration 53, loss = 0.11272100\n",
      "Iteration 54, loss = 0.10869151\n",
      "Iteration 55, loss = 0.10614435\n",
      "Iteration 56, loss = 0.10414641\n",
      "Iteration 57, loss = 0.10208554\n",
      "Iteration 58, loss = 0.10072143\n",
      "Iteration 59, loss = 0.09974995\n",
      "Iteration 60, loss = 0.09838571\n",
      "Iteration 61, loss = 0.09699002\n",
      "Iteration 62, loss = 0.11468797\n",
      "Iteration 63, loss = 0.11605551\n",
      "Iteration 64, loss = 0.10860024\n",
      "Iteration 65, loss = 0.14279994\n",
      "Iteration 66, loss = 0.15159506\n",
      "Iteration 67, loss = 0.12210543\n",
      "Iteration 68, loss = 0.11100866\n",
      "Iteration 69, loss = 0.10269192\n",
      "Iteration 70, loss = 0.09993773\n",
      "Iteration 71, loss = 0.10196003\n",
      "Iteration 72, loss = 0.09418423\n",
      "Iteration 73, loss = 0.09176728\n",
      "Iteration 74, loss = 0.08924573\n",
      "Iteration 75, loss = 0.08782895\n",
      "Iteration 76, loss = 0.08678725\n",
      "Iteration 77, loss = 0.08523057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 78, loss = 0.08464633\n",
      "Iteration 79, loss = 0.08428607\n",
      "Iteration 80, loss = 0.08434652\n",
      "Iteration 81, loss = 0.08292519\n",
      "Iteration 82, loss = 0.08264640\n",
      "Iteration 83, loss = 0.08500661\n",
      "Iteration 84, loss = 0.08248890\n",
      "Iteration 85, loss = 0.08780601\n",
      "Iteration 86, loss = 0.08747846\n",
      "Iteration 87, loss = 0.09269077\n",
      "Iteration 88, loss = 0.10589666\n",
      "Iteration 89, loss = 0.13248583\n",
      "Iteration 90, loss = 0.15590560\n",
      "Iteration 91, loss = 0.13010754\n",
      "Iteration 92, loss = 0.11851510\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.71746720\n",
      "Iteration 2, loss = 1.43269814\n",
      "Iteration 3, loss = 1.27030728\n",
      "Iteration 4, loss = 1.13539701\n",
      "Iteration 5, loss = 1.01149509\n",
      "Iteration 6, loss = 0.90789325\n",
      "Iteration 7, loss = 0.80768669\n",
      "Iteration 8, loss = 0.72558978\n",
      "Iteration 9, loss = 0.65885749\n",
      "Iteration 10, loss = 0.60752633\n",
      "Iteration 11, loss = 0.56837869\n",
      "Iteration 12, loss = 0.52584930\n",
      "Iteration 13, loss = 0.47873108\n",
      "Iteration 14, loss = 0.43459755\n",
      "Iteration 15, loss = 0.39561154\n",
      "Iteration 16, loss = 0.36656399\n",
      "Iteration 17, loss = 0.36798507\n",
      "Iteration 18, loss = 0.34014425\n",
      "Iteration 19, loss = 0.31789666\n",
      "Iteration 20, loss = 0.29968196\n",
      "Iteration 21, loss = 0.31814093\n",
      "Iteration 22, loss = 0.35892628\n",
      "Iteration 23, loss = 0.29914981\n",
      "Iteration 24, loss = 0.26782701\n",
      "Iteration 25, loss = 0.25100829\n",
      "Iteration 26, loss = 0.23748621\n",
      "Iteration 27, loss = 0.22672132\n",
      "Iteration 28, loss = 0.21669946\n",
      "Iteration 29, loss = 0.20645773\n",
      "Iteration 30, loss = 0.19386745\n",
      "Iteration 31, loss = 0.18664084\n",
      "Iteration 32, loss = 0.17820178\n",
      "Iteration 33, loss = 0.17249993\n",
      "Iteration 34, loss = 0.16640848\n",
      "Iteration 35, loss = 0.16210186\n",
      "Iteration 36, loss = 0.15921225\n",
      "Iteration 37, loss = 0.15736492\n",
      "Iteration 38, loss = 0.15210267\n",
      "Iteration 39, loss = 0.14670233\n",
      "Iteration 40, loss = 0.14331171\n",
      "Iteration 41, loss = 0.18003529\n",
      "Iteration 42, loss = 0.19203446\n",
      "Iteration 43, loss = 0.17880470\n",
      "Iteration 44, loss = 0.18139208\n",
      "Iteration 45, loss = 0.16393302\n",
      "Iteration 46, loss = 0.14922440\n",
      "Iteration 47, loss = 0.14214975\n",
      "Iteration 48, loss = 0.13272705\n",
      "Iteration 49, loss = 0.12764075\n",
      "Iteration 50, loss = 0.12450016\n",
      "Iteration 51, loss = 0.12155434\n",
      "Iteration 52, loss = 0.11825927\n",
      "Iteration 53, loss = 0.11727524\n",
      "Iteration 54, loss = 0.11930453\n",
      "Iteration 55, loss = 0.12045795\n",
      "Iteration 56, loss = 0.11388876\n",
      "Iteration 57, loss = 0.11062076\n",
      "Iteration 58, loss = 0.10886156\n",
      "Iteration 59, loss = 0.10891402\n",
      "Iteration 60, loss = 0.10800994\n",
      "Iteration 61, loss = 0.10515444\n",
      "Iteration 62, loss = 0.11630249\n",
      "Iteration 63, loss = 0.11044150\n",
      "Iteration 64, loss = 0.10500895\n",
      "Iteration 65, loss = 0.10520876\n",
      "Iteration 66, loss = 0.10321014\n",
      "Iteration 67, loss = 0.10160815\n",
      "Iteration 68, loss = 0.11007350\n",
      "Iteration 69, loss = 0.11900882\n",
      "Iteration 70, loss = 0.13631432\n",
      "Iteration 71, loss = 0.19091595\n",
      "Iteration 72, loss = 0.15898756\n",
      "Iteration 73, loss = 0.13639475\n",
      "Iteration 74, loss = 0.15232069\n",
      "Iteration 75, loss = 0.13026654\n",
      "Iteration 76, loss = 0.12327777\n",
      "Iteration 77, loss = 0.11478635\n",
      "Iteration 78, loss = 0.10869433\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92545875\n",
      "Iteration 2, loss = 1.60610268\n",
      "Iteration 3, loss = 1.40952631\n",
      "Iteration 4, loss = 1.24748020\n",
      "Iteration 5, loss = 1.10018705\n",
      "Iteration 6, loss = 0.97559087\n",
      "Iteration 7, loss = 0.86385807\n",
      "Iteration 8, loss = 0.76824221\n",
      "Iteration 9, loss = 0.69413296\n",
      "Iteration 10, loss = 0.62893192\n",
      "Iteration 11, loss = 0.57928835\n",
      "Iteration 12, loss = 0.52756317\n",
      "Iteration 13, loss = 0.49579746\n",
      "Iteration 14, loss = 0.44678704\n",
      "Iteration 15, loss = 0.40470085\n",
      "Iteration 16, loss = 0.37187175\n",
      "Iteration 17, loss = 0.37342301\n",
      "Iteration 18, loss = 0.34014497\n",
      "Iteration 19, loss = 0.34571458\n",
      "Iteration 20, loss = 0.31914810\n",
      "Iteration 21, loss = 0.33024783\n",
      "Iteration 22, loss = 0.32853838\n",
      "Iteration 23, loss = 0.27698437\n",
      "Iteration 24, loss = 0.25831267\n",
      "Iteration 25, loss = 0.24092795\n",
      "Iteration 26, loss = 0.23447958\n",
      "Iteration 27, loss = 0.22342658\n",
      "Iteration 28, loss = 0.21439149\n",
      "Iteration 29, loss = 0.21757831\n",
      "Iteration 30, loss = 0.20539057\n",
      "Iteration 31, loss = 0.19285893\n",
      "Iteration 32, loss = 0.18216962\n",
      "Iteration 33, loss = 0.17524180\n",
      "Iteration 34, loss = 0.16903129\n",
      "Iteration 35, loss = 0.16649093\n",
      "Iteration 36, loss = 0.16438931\n",
      "Iteration 37, loss = 0.16096838\n",
      "Iteration 38, loss = 0.15892116\n",
      "Iteration 39, loss = 0.15952245\n",
      "Iteration 40, loss = 0.16711389\n",
      "Iteration 41, loss = 0.18849972\n",
      "Iteration 42, loss = 0.17213572\n",
      "Iteration 43, loss = 0.15995697\n",
      "Iteration 44, loss = 0.15459137\n",
      "Iteration 45, loss = 0.15586864\n",
      "Iteration 46, loss = 0.14854153\n",
      "Iteration 47, loss = 0.15807646\n",
      "Iteration 48, loss = 0.16995710\n",
      "Iteration 49, loss = 0.15770850\n",
      "Iteration 50, loss = 0.15074651\n",
      "Iteration 51, loss = 0.16167427\n",
      "Iteration 52, loss = 0.15002685\n",
      "Iteration 53, loss = 0.14872338\n",
      "Iteration 54, loss = 0.14220975\n",
      "Iteration 55, loss = 0.13407873\n",
      "Iteration 56, loss = 0.13231450\n",
      "Iteration 57, loss = 0.12447136\n",
      "Iteration 58, loss = 0.12304627\n",
      "Iteration 59, loss = 0.11970667\n",
      "Iteration 60, loss = 0.11872241\n",
      "Iteration 61, loss = 0.11654355\n",
      "Iteration 62, loss = 0.12087602\n",
      "Iteration 63, loss = 0.13650125\n",
      "Iteration 64, loss = 0.12800328\n",
      "Iteration 65, loss = 0.12390614\n",
      "Iteration 66, loss = 0.11945564\n",
      "Iteration 67, loss = 0.11628104\n",
      "Iteration 68, loss = 0.11673202\n",
      "Iteration 69, loss = 0.11698515\n",
      "Iteration 70, loss = 0.11639256\n",
      "Iteration 71, loss = 0.11551312\n",
      "Iteration 72, loss = 0.11445907\n",
      "Iteration 73, loss = 0.12237580\n",
      "Iteration 74, loss = 0.16039581\n",
      "Iteration 75, loss = 0.16561720\n",
      "Iteration 76, loss = 0.17509066\n",
      "Iteration 77, loss = 0.14138269\n",
      "Iteration 78, loss = 0.12702560\n",
      "Iteration 79, loss = 0.12478954\n",
      "Iteration 80, loss = 0.12138001\n",
      "Iteration 81, loss = 0.11857428\n",
      "Iteration 82, loss = 0.11636617\n",
      "Iteration 83, loss = 0.11644926\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13139723\n",
      "Iteration 2, loss = 1.77417948\n",
      "Iteration 3, loss = 1.54038880\n",
      "Iteration 4, loss = 1.35155458\n",
      "Iteration 5, loss = 1.17928567\n",
      "Iteration 6, loss = 1.03679255\n",
      "Iteration 7, loss = 0.91309662\n",
      "Iteration 8, loss = 0.80871642\n",
      "Iteration 9, loss = 0.72836556\n",
      "Iteration 10, loss = 0.65597365\n",
      "Iteration 11, loss = 0.60589669\n",
      "Iteration 12, loss = 0.54917505\n",
      "Iteration 13, loss = 0.50230881\n",
      "Iteration 14, loss = 0.46590928\n",
      "Iteration 15, loss = 0.42048164\n",
      "Iteration 16, loss = 0.38988561\n",
      "Iteration 17, loss = 0.39638978\n",
      "Iteration 18, loss = 0.35918897\n",
      "Iteration 19, loss = 0.34127312\n",
      "Iteration 20, loss = 0.33296419\n",
      "Iteration 21, loss = 0.30979285\n",
      "Iteration 22, loss = 0.34418751\n",
      "Iteration 23, loss = 0.28664940\n",
      "Iteration 24, loss = 0.27442727\n",
      "Iteration 25, loss = 0.24974396\n",
      "Iteration 26, loss = 0.24676916\n",
      "Iteration 27, loss = 0.23630600\n",
      "Iteration 28, loss = 0.24688130\n",
      "Iteration 29, loss = 0.24977369\n",
      "Iteration 30, loss = 0.22582535\n",
      "Iteration 31, loss = 0.21351683\n",
      "Iteration 32, loss = 0.19487458\n",
      "Iteration 33, loss = 0.18825770\n",
      "Iteration 34, loss = 0.18113056\n",
      "Iteration 35, loss = 0.17875946\n",
      "Iteration 36, loss = 0.17611065\n",
      "Iteration 37, loss = 0.17554545\n",
      "Iteration 38, loss = 0.16665617\n",
      "Iteration 39, loss = 0.16245366\n",
      "Iteration 40, loss = 0.16078718\n",
      "Iteration 41, loss = 0.17916815\n",
      "Iteration 42, loss = 0.17040082\n",
      "Iteration 43, loss = 0.16218170\n",
      "Iteration 44, loss = 0.15791276\n",
      "Iteration 45, loss = 0.15165481\n",
      "Iteration 46, loss = 0.14961900\n",
      "Iteration 47, loss = 0.16506463\n",
      "Iteration 48, loss = 0.17401836\n",
      "Iteration 49, loss = 0.17848232\n",
      "Iteration 50, loss = 0.16245642\n",
      "Iteration 51, loss = 0.15105204\n",
      "Iteration 52, loss = 0.14366857\n",
      "Iteration 53, loss = 0.14112340\n",
      "Iteration 54, loss = 0.14044265\n",
      "Iteration 55, loss = 0.16309544\n",
      "Iteration 56, loss = 0.18624573\n",
      "Iteration 57, loss = 0.18734647\n",
      "Iteration 58, loss = 0.20272059\n",
      "Iteration 59, loss = 0.18605084\n",
      "Iteration 60, loss = 0.17101503\n",
      "Iteration 61, loss = 0.14746936\n",
      "Iteration 62, loss = 0.14000596\n",
      "Iteration 63, loss = 0.13826359\n",
      "Iteration 64, loss = 0.13192763\n",
      "Iteration 65, loss = 0.13076089\n",
      "Iteration 66, loss = 0.13952439\n",
      "Iteration 67, loss = 0.13456664\n",
      "Iteration 68, loss = 0.12850102\n",
      "Iteration 69, loss = 0.12785526\n",
      "Iteration 70, loss = 0.13077170\n",
      "Iteration 71, loss = 0.12773794\n",
      "Iteration 72, loss = 0.12625338\n",
      "Iteration 73, loss = 0.12876795\n",
      "Iteration 74, loss = 0.12752894\n",
      "Iteration 75, loss = 0.12744767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76, loss = 0.12507753\n",
      "Iteration 77, loss = 0.12220561\n",
      "Iteration 78, loss = 0.12257209\n",
      "Iteration 79, loss = 0.12418464\n",
      "Iteration 80, loss = 0.12534590\n",
      "Iteration 81, loss = 0.12307739\n",
      "Iteration 82, loss = 0.12538716\n",
      "Iteration 83, loss = 0.12490241\n",
      "Iteration 84, loss = 0.12387375\n",
      "Iteration 85, loss = 0.12251303\n",
      "Iteration 86, loss = 0.12351772\n",
      "Iteration 87, loss = 0.12065020\n",
      "Iteration 88, loss = 0.12166516\n",
      "Iteration 89, loss = 0.12364460\n",
      "Iteration 90, loss = 0.12291222\n",
      "Iteration 91, loss = 0.12912828\n",
      "Iteration 92, loss = 0.13128925\n",
      "Iteration 93, loss = 0.13948118\n",
      "Iteration 94, loss = 0.13775563\n",
      "Iteration 95, loss = 0.13808689\n",
      "Iteration 96, loss = 0.13183157\n",
      "Iteration 97, loss = 0.12755092\n",
      "Iteration 98, loss = 0.12278706\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.33578054\n",
      "Iteration 2, loss = 1.93802313\n",
      "Iteration 3, loss = 1.66271965\n",
      "Iteration 4, loss = 1.43804957\n",
      "Iteration 5, loss = 1.23997415\n",
      "Iteration 6, loss = 1.08244614\n",
      "Iteration 7, loss = 0.94675617\n",
      "Iteration 8, loss = 0.83330736\n",
      "Iteration 9, loss = 0.74750645\n",
      "Iteration 10, loss = 0.67712454\n",
      "Iteration 11, loss = 0.62392734\n",
      "Iteration 12, loss = 0.56572780\n",
      "Iteration 13, loss = 0.51006604\n",
      "Iteration 14, loss = 0.47045077\n",
      "Iteration 15, loss = 0.42491270\n",
      "Iteration 16, loss = 0.38975622\n",
      "Iteration 17, loss = 0.39120738\n",
      "Iteration 18, loss = 0.36463238\n",
      "Iteration 19, loss = 0.36919417\n",
      "Iteration 20, loss = 0.34365845\n",
      "Iteration 21, loss = 0.31445578\n",
      "Iteration 22, loss = 0.33601848\n",
      "Iteration 23, loss = 0.28710881\n",
      "Iteration 24, loss = 0.26690187\n",
      "Iteration 25, loss = 0.24950247\n",
      "Iteration 26, loss = 0.25394476\n",
      "Iteration 27, loss = 0.24090519\n",
      "Iteration 28, loss = 0.24952331\n",
      "Iteration 29, loss = 0.24578776\n",
      "Iteration 30, loss = 0.22415788\n",
      "Iteration 31, loss = 0.22177226\n",
      "Iteration 32, loss = 0.20628940\n",
      "Iteration 33, loss = 0.19641152\n",
      "Iteration 34, loss = 0.18756143\n",
      "Iteration 35, loss = 0.18703068\n",
      "Iteration 36, loss = 0.18345314\n",
      "Iteration 37, loss = 0.18105340\n",
      "Iteration 38, loss = 0.17353513\n",
      "Iteration 39, loss = 0.16987452\n",
      "Iteration 40, loss = 0.16985512\n",
      "Iteration 41, loss = 0.18378055\n",
      "Iteration 42, loss = 0.17965756\n",
      "Iteration 43, loss = 0.16649469\n",
      "Iteration 44, loss = 0.16397107\n",
      "Iteration 45, loss = 0.16376073\n",
      "Iteration 46, loss = 0.16727418\n",
      "Iteration 47, loss = 0.19079000\n",
      "Iteration 48, loss = 0.19333946\n",
      "Iteration 49, loss = 0.20003929\n",
      "Iteration 50, loss = 0.17530318\n",
      "Iteration 51, loss = 0.17912650\n",
      "Iteration 52, loss = 0.16120746\n",
      "Iteration 53, loss = 0.15569006\n",
      "Iteration 54, loss = 0.15892481\n",
      "Iteration 55, loss = 0.15258834\n",
      "Iteration 56, loss = 0.15065727\n",
      "Iteration 57, loss = 0.16391651\n",
      "Iteration 58, loss = 0.15689672\n",
      "Iteration 59, loss = 0.14914444\n",
      "Iteration 60, loss = 0.14701535\n",
      "Iteration 61, loss = 0.14181232\n",
      "Iteration 62, loss = 0.15674309\n",
      "Iteration 63, loss = 0.17447503\n",
      "Iteration 64, loss = 0.15550844\n",
      "Iteration 65, loss = 0.15230628\n",
      "Iteration 66, loss = 0.15324507\n",
      "Iteration 67, loss = 0.14784384\n",
      "Iteration 68, loss = 0.14568463\n",
      "Iteration 69, loss = 0.14787632\n",
      "Iteration 70, loss = 0.15526268\n",
      "Iteration 71, loss = 0.14750921\n",
      "Iteration 72, loss = 0.14375726\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.53857371\n",
      "Iteration 2, loss = 2.09536773\n",
      "Iteration 3, loss = 1.77666749\n",
      "Iteration 4, loss = 1.51728639\n",
      "Iteration 5, loss = 1.29785801\n",
      "Iteration 6, loss = 1.12282209\n",
      "Iteration 7, loss = 0.97061364\n",
      "Iteration 8, loss = 0.85058478\n",
      "Iteration 9, loss = 0.76201755\n",
      "Iteration 10, loss = 0.68303283\n",
      "Iteration 11, loss = 0.63042404\n",
      "Iteration 12, loss = 0.56495506\n",
      "Iteration 13, loss = 0.51434283\n",
      "Iteration 14, loss = 0.47071008\n",
      "Iteration 15, loss = 0.43119758\n",
      "Iteration 16, loss = 0.39513814\n",
      "Iteration 17, loss = 0.39586706\n",
      "Iteration 18, loss = 0.35934616\n",
      "Iteration 19, loss = 0.35446715\n",
      "Iteration 20, loss = 0.33780009\n",
      "Iteration 21, loss = 0.32461939\n",
      "Iteration 22, loss = 0.32813701\n",
      "Iteration 23, loss = 0.29125159\n",
      "Iteration 24, loss = 0.28248675\n",
      "Iteration 25, loss = 0.25538206\n",
      "Iteration 26, loss = 0.24790930\n",
      "Iteration 27, loss = 0.24857145\n",
      "Iteration 28, loss = 0.25242911\n",
      "Iteration 29, loss = 0.26999926\n",
      "Iteration 30, loss = 0.24093184\n",
      "Iteration 31, loss = 0.21779372\n",
      "Iteration 32, loss = 0.20558487\n",
      "Iteration 33, loss = 0.20312808\n",
      "Iteration 34, loss = 0.19578322\n",
      "Iteration 35, loss = 0.19899103\n",
      "Iteration 36, loss = 0.19506771\n",
      "Iteration 37, loss = 0.19311349\n",
      "Iteration 38, loss = 0.18319322\n",
      "Iteration 39, loss = 0.17878588\n",
      "Iteration 40, loss = 0.17765290\n",
      "Iteration 41, loss = 0.19317391\n",
      "Iteration 42, loss = 0.18533402\n",
      "Iteration 43, loss = 0.17510177\n",
      "Iteration 44, loss = 0.17458360\n",
      "Iteration 45, loss = 0.18081081\n",
      "Iteration 46, loss = 0.19340755\n",
      "Iteration 47, loss = 0.21602781\n",
      "Iteration 48, loss = 0.20230518\n",
      "Iteration 49, loss = 0.19497547\n",
      "Iteration 50, loss = 0.18725192\n",
      "Iteration 51, loss = 0.19817115\n",
      "Iteration 52, loss = 0.17520115\n",
      "Iteration 53, loss = 0.17504789\n",
      "Iteration 54, loss = 0.17454630\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.73985100\n",
      "Iteration 2, loss = 2.24767068\n",
      "Iteration 3, loss = 1.88364855\n",
      "Iteration 4, loss = 1.59000457\n",
      "Iteration 5, loss = 1.34649884\n",
      "Iteration 6, loss = 1.15710127\n",
      "Iteration 7, loss = 0.99480073\n",
      "Iteration 8, loss = 0.86922273\n",
      "Iteration 9, loss = 0.77408118\n",
      "Iteration 10, loss = 0.70111686\n",
      "Iteration 11, loss = 0.64031427\n",
      "Iteration 12, loss = 0.58884731\n",
      "Iteration 13, loss = 0.53044446\n",
      "Iteration 14, loss = 0.48239709\n",
      "Iteration 15, loss = 0.43772131\n",
      "Iteration 16, loss = 0.40180118\n",
      "Iteration 17, loss = 0.39409578\n",
      "Iteration 18, loss = 0.36084200\n",
      "Iteration 19, loss = 0.36451901\n",
      "Iteration 20, loss = 0.35782954\n",
      "Iteration 21, loss = 0.36310723\n",
      "Iteration 22, loss = 0.34977813\n",
      "Iteration 23, loss = 0.32852086\n",
      "Iteration 24, loss = 0.31469470\n",
      "Iteration 25, loss = 0.27853062\n",
      "Iteration 26, loss = 0.26420339\n",
      "Iteration 27, loss = 0.25630631\n",
      "Iteration 28, loss = 0.24914144\n",
      "Iteration 29, loss = 0.27334782\n",
      "Iteration 30, loss = 0.24724072\n",
      "Iteration 31, loss = 0.22633799\n",
      "Iteration 32, loss = 0.21708044\n",
      "Iteration 33, loss = 0.21436837\n",
      "Iteration 34, loss = 0.20416035\n",
      "Iteration 35, loss = 0.20571181\n",
      "Iteration 36, loss = 0.20547071\n",
      "Iteration 37, loss = 0.20073971\n",
      "Iteration 38, loss = 0.19574761\n",
      "Iteration 39, loss = 0.19325530\n",
      "Iteration 40, loss = 0.19699825\n",
      "Iteration 41, loss = 0.20733582\n",
      "Iteration 42, loss = 0.20766809\n",
      "Iteration 43, loss = 0.18773351\n",
      "Iteration 44, loss = 0.19042406\n",
      "Iteration 45, loss = 0.19761975\n",
      "Iteration 46, loss = 0.19678800\n",
      "Iteration 47, loss = 0.20442470\n",
      "Iteration 48, loss = 0.19032648\n",
      "Iteration 49, loss = 0.19507979\n",
      "Iteration 50, loss = 0.18272558\n",
      "Iteration 51, loss = 0.19181308\n",
      "Iteration 52, loss = 0.17749129\n",
      "Iteration 53, loss = 0.17750378\n",
      "Iteration 54, loss = 0.17489311\n",
      "Iteration 55, loss = 0.18413106\n",
      "Iteration 56, loss = 0.17829625\n",
      "Iteration 57, loss = 0.17826813\n",
      "Iteration 58, loss = 0.18801035\n",
      "Iteration 59, loss = 0.19376618\n",
      "Iteration 60, loss = 0.20177724\n",
      "Iteration 61, loss = 0.18050864\n",
      "Iteration 62, loss = 0.18005962\n",
      "Iteration 63, loss = 0.17969543\n",
      "Iteration 64, loss = 0.17011870\n",
      "Iteration 65, loss = 0.16820334\n",
      "Iteration 66, loss = 0.16634973\n",
      "Iteration 67, loss = 0.16238481\n",
      "Iteration 68, loss = 0.16982643\n",
      "Iteration 69, loss = 0.16880087\n",
      "Iteration 70, loss = 0.17071272\n",
      "Iteration 71, loss = 0.16603739\n",
      "Iteration 72, loss = 0.16685541\n",
      "Iteration 73, loss = 0.17442444\n",
      "Iteration 74, loss = 0.18271198\n",
      "Iteration 75, loss = 0.17689674\n",
      "Iteration 76, loss = 0.17665342\n",
      "Iteration 77, loss = 0.16617257\n",
      "Iteration 78, loss = 0.16576587\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.93980670\n",
      "Iteration 2, loss = 2.39543325\n",
      "Iteration 3, loss = 1.98490330\n",
      "Iteration 4, loss = 1.65816056\n",
      "Iteration 5, loss = 1.39644010\n",
      "Iteration 6, loss = 1.18743094\n",
      "Iteration 7, loss = 1.01845627\n",
      "Iteration 8, loss = 0.88285512\n",
      "Iteration 9, loss = 0.78352758\n",
      "Iteration 10, loss = 0.71420727\n",
      "Iteration 11, loss = 0.63990877\n",
      "Iteration 12, loss = 0.58597705\n",
      "Iteration 13, loss = 0.54487985\n",
      "Iteration 14, loss = 0.48338730\n",
      "Iteration 15, loss = 0.45883897\n",
      "Iteration 16, loss = 0.41410847\n",
      "Iteration 17, loss = 0.41165083\n",
      "Iteration 18, loss = 0.37948283\n",
      "Iteration 19, loss = 0.36547786\n",
      "Iteration 20, loss = 0.36706102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.37941914\n",
      "Iteration 22, loss = 0.32366639\n",
      "Iteration 23, loss = 0.31328549\n",
      "Iteration 24, loss = 0.30706596\n",
      "Iteration 25, loss = 0.27665864\n",
      "Iteration 26, loss = 0.26973308\n",
      "Iteration 27, loss = 0.26570904\n",
      "Iteration 28, loss = 0.27191723\n",
      "Iteration 29, loss = 0.29668114\n",
      "Iteration 30, loss = 0.25733533\n",
      "Iteration 31, loss = 0.23454925\n",
      "Iteration 32, loss = 0.22461473\n",
      "Iteration 33, loss = 0.22298743\n",
      "Iteration 34, loss = 0.21296480\n",
      "Iteration 35, loss = 0.21718225\n",
      "Iteration 36, loss = 0.21690727\n",
      "Iteration 37, loss = 0.21708555\n",
      "Iteration 38, loss = 0.20435323\n",
      "Iteration 39, loss = 0.20191784\n",
      "Iteration 40, loss = 0.20262138\n",
      "Iteration 41, loss = 0.21898981\n",
      "Iteration 42, loss = 0.21276729\n",
      "Iteration 43, loss = 0.19630926\n",
      "Iteration 44, loss = 0.20178707\n",
      "Iteration 45, loss = 0.20218991\n",
      "Iteration 46, loss = 0.21339838\n",
      "Iteration 47, loss = 0.24077717\n",
      "Iteration 48, loss = 0.21885854\n",
      "Iteration 49, loss = 0.22323662\n",
      "Iteration 50, loss = 0.20251115\n",
      "Iteration 51, loss = 0.19707918\n",
      "Iteration 52, loss = 0.18790936\n",
      "Iteration 53, loss = 0.18887962\n",
      "Iteration 54, loss = 0.19152368\n",
      "Iteration 55, loss = 0.19723577\n",
      "Iteration 56, loss = 0.18383218\n",
      "Iteration 57, loss = 0.18823280\n",
      "Iteration 58, loss = 0.18645887\n",
      "Iteration 59, loss = 0.18659616\n",
      "Iteration 60, loss = 0.18249721\n",
      "Iteration 61, loss = 0.17849204\n",
      "Iteration 62, loss = 0.19528359\n",
      "Iteration 63, loss = 0.19605384\n",
      "Iteration 64, loss = 0.18048419\n",
      "Iteration 65, loss = 0.18309405\n",
      "Iteration 66, loss = 0.17934181\n",
      "Iteration 67, loss = 0.17732284\n",
      "Iteration 68, loss = 0.18811045\n",
      "Iteration 69, loss = 0.18347889\n",
      "Iteration 70, loss = 0.18350643\n",
      "Iteration 71, loss = 0.18113724\n",
      "Iteration 72, loss = 0.17862132\n",
      "Iteration 73, loss = 0.18632070\n",
      "Iteration 74, loss = 0.20191136\n",
      "Iteration 75, loss = 0.19392327\n",
      "Iteration 76, loss = 0.18707447\n",
      "Iteration 77, loss = 0.17753452\n",
      "Iteration 78, loss = 0.17767471\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.13811252\n",
      "Iteration 2, loss = 2.53870319\n",
      "Iteration 3, loss = 2.08030149\n",
      "Iteration 4, loss = 1.72003647\n",
      "Iteration 5, loss = 1.43359852\n",
      "Iteration 6, loss = 1.21305620\n",
      "Iteration 7, loss = 1.03367399\n",
      "Iteration 8, loss = 0.89575982\n",
      "Iteration 9, loss = 0.79247970\n",
      "Iteration 10, loss = 0.71978672\n",
      "Iteration 11, loss = 0.64935026\n",
      "Iteration 12, loss = 0.59891459\n",
      "Iteration 13, loss = 0.55038781\n",
      "Iteration 14, loss = 0.48853461\n",
      "Iteration 15, loss = 0.45272636\n",
      "Iteration 16, loss = 0.41264098\n",
      "Iteration 17, loss = 0.40767512\n",
      "Iteration 18, loss = 0.37677354\n",
      "Iteration 19, loss = 0.37622281\n",
      "Iteration 20, loss = 0.38071066\n",
      "Iteration 21, loss = 0.38650856\n",
      "Iteration 22, loss = 0.32784795\n",
      "Iteration 23, loss = 0.31372551\n",
      "Iteration 24, loss = 0.30676334\n",
      "Iteration 25, loss = 0.28151622\n",
      "Iteration 26, loss = 0.26925966\n",
      "Iteration 27, loss = 0.26499046\n",
      "Iteration 28, loss = 0.26298065\n",
      "Iteration 29, loss = 0.27099032\n",
      "Iteration 30, loss = 0.24994528\n",
      "Iteration 31, loss = 0.25834117\n",
      "Iteration 32, loss = 0.24678493\n",
      "Iteration 33, loss = 0.24062669\n",
      "Iteration 34, loss = 0.22603479\n",
      "Iteration 35, loss = 0.22744387\n",
      "Iteration 36, loss = 0.22762860\n",
      "Iteration 37, loss = 0.23006500\n",
      "Iteration 38, loss = 0.21715987\n",
      "Iteration 39, loss = 0.21068426\n",
      "Iteration 40, loss = 0.21152594\n",
      "Iteration 41, loss = 0.23412860\n",
      "Iteration 42, loss = 0.22449195\n",
      "Iteration 43, loss = 0.20797828\n",
      "Iteration 44, loss = 0.21100651\n",
      "Iteration 45, loss = 0.20783398\n",
      "Iteration 46, loss = 0.21543554\n",
      "Iteration 47, loss = 0.24771880\n",
      "Iteration 48, loss = 0.23443628\n",
      "Iteration 49, loss = 0.23408160\n",
      "Iteration 50, loss = 0.21147849\n",
      "Iteration 51, loss = 0.21384597\n",
      "Iteration 52, loss = 0.19962228\n",
      "Iteration 53, loss = 0.19899999\n",
      "Iteration 54, loss = 0.19847538\n",
      "Iteration 55, loss = 0.20010421\n",
      "Iteration 56, loss = 0.19383352\n",
      "Iteration 57, loss = 0.19721261\n",
      "Iteration 58, loss = 0.20026280\n",
      "Iteration 59, loss = 0.20176094\n",
      "Iteration 60, loss = 0.20202606\n",
      "Iteration 61, loss = 0.19211708\n",
      "Iteration 62, loss = 0.21403325\n",
      "Iteration 63, loss = 0.24928972\n",
      "Iteration 64, loss = 0.20469694\n",
      "Iteration 65, loss = 0.20048808\n",
      "Iteration 66, loss = 0.20020087\n",
      "Iteration 67, loss = 0.18982085\n",
      "Iteration 68, loss = 0.19393365\n",
      "Iteration 69, loss = 0.19591045\n",
      "Iteration 70, loss = 0.19593325\n",
      "Iteration 71, loss = 0.19174124\n",
      "Iteration 72, loss = 0.19016875\n",
      "Iteration 73, loss = 0.19671509\n",
      "Iteration 74, loss = 0.19483740\n",
      "Iteration 75, loss = 0.18960039\n",
      "Iteration 76, loss = 0.18976558\n",
      "Iteration 77, loss = 0.18498081\n",
      "Iteration 78, loss = 0.18705671\n",
      "Iteration 79, loss = 0.18844107\n",
      "Iteration 80, loss = 0.18793675\n",
      "Iteration 81, loss = 0.18485947\n",
      "Iteration 82, loss = 0.19023329\n",
      "Iteration 83, loss = 0.19058020\n",
      "Iteration 84, loss = 0.18808994\n",
      "Iteration 85, loss = 0.18874824\n",
      "Iteration 86, loss = 0.19292888\n",
      "Iteration 87, loss = 0.19145481\n",
      "Iteration 88, loss = 0.18894258\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.33529141\n",
      "Iteration 2, loss = 2.67993485\n",
      "Iteration 3, loss = 2.17296221\n",
      "Iteration 4, loss = 1.78082055\n",
      "Iteration 5, loss = 1.47146389\n",
      "Iteration 6, loss = 1.23661065\n",
      "Iteration 7, loss = 1.05029275\n",
      "Iteration 8, loss = 0.90393765\n",
      "Iteration 9, loss = 0.80006318\n",
      "Iteration 10, loss = 0.72582947\n",
      "Iteration 11, loss = 0.65364586\n",
      "Iteration 12, loss = 0.60073136\n",
      "Iteration 13, loss = 0.54723515\n",
      "Iteration 14, loss = 0.49263993\n",
      "Iteration 15, loss = 0.46067335\n",
      "Iteration 16, loss = 0.41726069\n",
      "Iteration 17, loss = 0.41431604\n",
      "Iteration 18, loss = 0.38448376\n",
      "Iteration 19, loss = 0.38108702\n",
      "Iteration 20, loss = 0.39745238\n",
      "Iteration 21, loss = 0.39799506\n",
      "Iteration 22, loss = 0.36210596\n",
      "Iteration 23, loss = 0.33680151\n",
      "Iteration 24, loss = 0.31145726\n",
      "Iteration 25, loss = 0.30064301\n",
      "Iteration 26, loss = 0.28273447\n",
      "Iteration 27, loss = 0.27971388\n",
      "Iteration 28, loss = 0.27047148\n",
      "Iteration 29, loss = 0.31083275\n",
      "Iteration 30, loss = 0.27087959\n",
      "Iteration 31, loss = 0.25285287\n",
      "Iteration 32, loss = 0.24507197\n",
      "Iteration 33, loss = 0.24281274\n",
      "Iteration 34, loss = 0.23296003\n",
      "Iteration 35, loss = 0.24290966\n",
      "Iteration 36, loss = 0.23798430\n",
      "Iteration 37, loss = 0.24226386\n",
      "Iteration 38, loss = 0.22674168\n",
      "Iteration 39, loss = 0.22438081\n",
      "Iteration 40, loss = 0.22474071\n",
      "Iteration 41, loss = 0.24334078\n",
      "Iteration 42, loss = 0.24083786\n",
      "Iteration 43, loss = 0.21772612\n",
      "Iteration 44, loss = 0.22834338\n",
      "Iteration 45, loss = 0.22750044\n",
      "Iteration 46, loss = 0.22347313\n",
      "Iteration 47, loss = 0.22454413\n",
      "Iteration 48, loss = 0.21788745\n",
      "Iteration 49, loss = 0.22338403\n",
      "Iteration 50, loss = 0.21520342\n",
      "Iteration 51, loss = 0.22329809\n",
      "Iteration 52, loss = 0.21337356\n",
      "Iteration 53, loss = 0.21230533\n",
      "Iteration 54, loss = 0.20768356\n",
      "Iteration 55, loss = 0.22005649\n",
      "Iteration 56, loss = 0.21030362\n",
      "Iteration 57, loss = 0.20913876\n",
      "Iteration 58, loss = 0.21866543\n",
      "Iteration 59, loss = 0.22234884\n",
      "Iteration 60, loss = 0.21702610\n",
      "Iteration 61, loss = 0.20624759\n",
      "Iteration 62, loss = 0.23274108\n",
      "Iteration 63, loss = 0.26696974\n",
      "Iteration 64, loss = 0.22487172\n",
      "Iteration 65, loss = 0.21921596\n",
      "Iteration 66, loss = 0.22309455\n",
      "Iteration 67, loss = 0.20890953\n",
      "Iteration 68, loss = 0.20379997\n",
      "Iteration 69, loss = 0.20709583\n",
      "Iteration 70, loss = 0.20626163\n",
      "Iteration 71, loss = 0.20408387\n",
      "Iteration 72, loss = 0.20388508\n",
      "Iteration 73, loss = 0.21701562\n",
      "Iteration 74, loss = 0.21496979\n",
      "Iteration 75, loss = 0.20721362\n",
      "Iteration 76, loss = 0.20756027\n",
      "Iteration 77, loss = 0.19975054\n",
      "Iteration 78, loss = 0.19948443\n",
      "Iteration 79, loss = 0.19883962\n",
      "Iteration 80, loss = 0.19767376\n",
      "Iteration 81, loss = 0.19671017\n",
      "Iteration 82, loss = 0.20369823\n",
      "Iteration 83, loss = 0.20126825\n",
      "Iteration 84, loss = 0.19700610\n",
      "Iteration 85, loss = 0.19756510\n",
      "Iteration 86, loss = 0.20496793\n",
      "Iteration 87, loss = 0.20635606\n",
      "Iteration 88, loss = 0.20441556\n",
      "Iteration 89, loss = 0.21893709\n",
      "Iteration 90, loss = 0.22633657\n",
      "Iteration 91, loss = 0.21867391\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.53112860\n",
      "Iteration 2, loss = 2.81353427\n",
      "Iteration 3, loss = 2.25340212\n",
      "Iteration 4, loss = 1.82597331\n",
      "Iteration 5, loss = 1.49252038\n",
      "Iteration 6, loss = 1.24451382\n",
      "Iteration 7, loss = 1.05385380\n",
      "Iteration 8, loss = 0.90455474\n",
      "Iteration 9, loss = 0.80233220\n",
      "Iteration 10, loss = 0.73337310\n",
      "Iteration 11, loss = 0.66205772\n",
      "Iteration 12, loss = 0.58751076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.55011322\n",
      "Iteration 14, loss = 0.49488039\n",
      "Iteration 15, loss = 0.45493853\n",
      "Iteration 16, loss = 0.41650478\n",
      "Iteration 17, loss = 0.41511654\n",
      "Iteration 18, loss = 0.38572783\n",
      "Iteration 19, loss = 0.38346536\n",
      "Iteration 20, loss = 0.38141424\n",
      "Iteration 21, loss = 0.37142653\n",
      "Iteration 22, loss = 0.38506876\n",
      "Iteration 23, loss = 0.34672703\n",
      "Iteration 24, loss = 0.31908357\n",
      "Iteration 25, loss = 0.30411946\n",
      "Iteration 26, loss = 0.28569666\n",
      "Iteration 27, loss = 0.28159145\n",
      "Iteration 28, loss = 0.27236611\n",
      "Iteration 29, loss = 0.30542782\n",
      "Iteration 30, loss = 0.27173150\n",
      "Iteration 31, loss = 0.25940773\n",
      "Iteration 32, loss = 0.25601350\n",
      "Iteration 33, loss = 0.25308050\n",
      "Iteration 34, loss = 0.24402840\n",
      "Iteration 35, loss = 0.25136762\n",
      "Iteration 36, loss = 0.25575988\n",
      "Iteration 37, loss = 0.26521648\n",
      "Iteration 38, loss = 0.24528385\n",
      "Iteration 39, loss = 0.24243086\n",
      "Iteration 40, loss = 0.23942978\n",
      "Iteration 41, loss = 0.25999068\n",
      "Iteration 42, loss = 0.25189683\n",
      "Iteration 43, loss = 0.23008529\n",
      "Iteration 44, loss = 0.23205988\n",
      "Iteration 45, loss = 0.23239293\n",
      "Iteration 46, loss = 0.22738352\n",
      "Iteration 47, loss = 0.23032693\n",
      "Iteration 48, loss = 0.22781902\n",
      "Iteration 49, loss = 0.23968657\n",
      "Iteration 50, loss = 0.22802350\n",
      "Iteration 51, loss = 0.23944749\n",
      "Iteration 52, loss = 0.22144711\n",
      "Iteration 53, loss = 0.22267037\n",
      "Iteration 54, loss = 0.21769562\n",
      "Iteration 55, loss = 0.22789329\n",
      "Iteration 56, loss = 0.22148891\n",
      "Iteration 57, loss = 0.22572055\n",
      "Iteration 58, loss = 0.23422740\n",
      "Iteration 59, loss = 0.23977218\n",
      "Iteration 60, loss = 0.23175888\n",
      "Iteration 61, loss = 0.22305717\n",
      "Iteration 62, loss = 0.25674275\n",
      "Iteration 63, loss = 0.26592792\n",
      "Iteration 64, loss = 0.23772047\n",
      "Iteration 65, loss = 0.23137716\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.72521748\n",
      "Iteration 2, loss = 2.94330228\n",
      "Iteration 3, loss = 2.33367894\n",
      "Iteration 4, loss = 1.87370805\n",
      "Iteration 5, loss = 1.51937028\n",
      "Iteration 6, loss = 1.26021472\n",
      "Iteration 7, loss = 1.06128424\n",
      "Iteration 8, loss = 0.91119535\n",
      "Iteration 9, loss = 0.80441474\n",
      "Iteration 10, loss = 0.72545340\n",
      "Iteration 11, loss = 0.65454108\n",
      "Iteration 12, loss = 0.59639110\n",
      "Iteration 13, loss = 0.54783009\n",
      "Iteration 14, loss = 0.49677196\n",
      "Iteration 15, loss = 0.46099028\n",
      "Iteration 16, loss = 0.42500728\n",
      "Iteration 17, loss = 0.42112303\n",
      "Iteration 18, loss = 0.39524293\n",
      "Iteration 19, loss = 0.38641971\n",
      "Iteration 20, loss = 0.40322672\n",
      "Iteration 21, loss = 0.40356357\n",
      "Iteration 22, loss = 0.35237286\n",
      "Iteration 23, loss = 0.32283680\n",
      "Iteration 24, loss = 0.32448542\n",
      "Iteration 25, loss = 0.30084366\n",
      "Iteration 26, loss = 0.30178421\n",
      "Iteration 27, loss = 0.29634683\n",
      "Iteration 28, loss = 0.29379040\n",
      "Iteration 29, loss = 0.34516455\n",
      "Iteration 30, loss = 0.30278545\n",
      "Iteration 31, loss = 0.28190639\n",
      "Iteration 32, loss = 0.26868969\n",
      "Iteration 33, loss = 0.26978905\n",
      "Iteration 34, loss = 0.25705613\n",
      "Iteration 35, loss = 0.26402480\n",
      "Iteration 36, loss = 0.25937694\n",
      "Iteration 37, loss = 0.26619196\n",
      "Iteration 38, loss = 0.25341246\n",
      "Iteration 39, loss = 0.24837195\n",
      "Iteration 40, loss = 0.25616484\n",
      "Iteration 41, loss = 0.25924059\n",
      "Iteration 42, loss = 0.25855353\n",
      "Iteration 43, loss = 0.23916364\n",
      "Iteration 44, loss = 0.24266214\n",
      "Iteration 45, loss = 0.24339721\n",
      "Iteration 46, loss = 0.24781054\n",
      "Iteration 47, loss = 0.25337304\n",
      "Iteration 48, loss = 0.24085733\n",
      "Iteration 49, loss = 0.24556879\n",
      "Iteration 50, loss = 0.23491131\n",
      "Iteration 51, loss = 0.24327882\n",
      "Iteration 52, loss = 0.23142229\n",
      "Iteration 53, loss = 0.23178528\n",
      "Iteration 54, loss = 0.22925876\n",
      "Iteration 55, loss = 0.24188989\n",
      "Iteration 56, loss = 0.23250637\n",
      "Iteration 57, loss = 0.23542726\n",
      "Iteration 58, loss = 0.24818556\n",
      "Iteration 59, loss = 0.25774521\n",
      "Iteration 60, loss = 0.24420090\n",
      "Iteration 61, loss = 0.23808248\n",
      "Iteration 62, loss = 0.25287824\n",
      "Iteration 63, loss = 0.25163960\n",
      "Iteration 64, loss = 0.23591257\n",
      "Iteration 65, loss = 0.22868921\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.91879347\n",
      "Iteration 2, loss = 3.07382200\n",
      "Iteration 3, loss = 2.41479412\n",
      "Iteration 4, loss = 1.92150603\n",
      "Iteration 5, loss = 1.55431777\n",
      "Iteration 6, loss = 1.28337198\n",
      "Iteration 7, loss = 1.07376944\n",
      "Iteration 8, loss = 0.91882798\n",
      "Iteration 9, loss = 0.80962711\n",
      "Iteration 10, loss = 0.72573519\n",
      "Iteration 11, loss = 0.65835259\n",
      "Iteration 12, loss = 0.58662414\n",
      "Iteration 13, loss = 0.54936832\n",
      "Iteration 14, loss = 0.49510687\n",
      "Iteration 15, loss = 0.46812565\n",
      "Iteration 16, loss = 0.42588515\n",
      "Iteration 17, loss = 0.42433248\n",
      "Iteration 18, loss = 0.39630771\n",
      "Iteration 19, loss = 0.38839549\n",
      "Iteration 20, loss = 0.41656418\n",
      "Iteration 21, loss = 0.40187472\n",
      "Iteration 22, loss = 0.36368285\n",
      "Iteration 23, loss = 0.35030781\n",
      "Iteration 24, loss = 0.34069228\n",
      "Iteration 25, loss = 0.32518984\n",
      "Iteration 26, loss = 0.30694521\n",
      "Iteration 27, loss = 0.30404874\n",
      "Iteration 28, loss = 0.29094111\n",
      "Iteration 29, loss = 0.33988818\n",
      "Iteration 30, loss = 0.30062383\n",
      "Iteration 31, loss = 0.28539150\n",
      "Iteration 32, loss = 0.27381633\n",
      "Iteration 33, loss = 0.27772003\n",
      "Iteration 34, loss = 0.26856403\n",
      "Iteration 35, loss = 0.26876385\n",
      "Iteration 36, loss = 0.27169168\n",
      "Iteration 37, loss = 0.28620581\n",
      "Iteration 38, loss = 0.26948815\n",
      "Iteration 39, loss = 0.25832674\n",
      "Iteration 40, loss = 0.26818857\n",
      "Iteration 41, loss = 0.27899393\n",
      "Iteration 42, loss = 0.27220089\n",
      "Iteration 43, loss = 0.25132804\n",
      "Iteration 44, loss = 0.26119921\n",
      "Iteration 45, loss = 0.25979893\n",
      "Iteration 46, loss = 0.25120805\n",
      "Iteration 47, loss = 0.26082702\n",
      "Iteration 48, loss = 0.25526852\n",
      "Iteration 49, loss = 0.25950903\n",
      "Iteration 50, loss = 0.24746266\n",
      "Iteration 51, loss = 0.25763161\n",
      "Iteration 52, loss = 0.24491997\n",
      "Iteration 53, loss = 0.24484713\n",
      "Iteration 54, loss = 0.24059514\n",
      "Iteration 55, loss = 0.24758088\n",
      "Iteration 56, loss = 0.24144564\n",
      "Iteration 57, loss = 0.24129924\n",
      "Iteration 58, loss = 0.24944123\n",
      "Iteration 59, loss = 0.25551881\n",
      "Iteration 60, loss = 0.24718774\n",
      "Iteration 61, loss = 0.24066228\n",
      "Iteration 62, loss = 0.27488196\n",
      "Iteration 63, loss = 0.29028790\n",
      "Iteration 64, loss = 0.25133973\n",
      "Iteration 65, loss = 0.24123121\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.11179808\n",
      "Iteration 2, loss = 3.20223271\n",
      "Iteration 3, loss = 2.49034701\n",
      "Iteration 4, loss = 1.96531507\n",
      "Iteration 5, loss = 1.57743677\n",
      "Iteration 6, loss = 1.29367953\n",
      "Iteration 7, loss = 1.08013994\n",
      "Iteration 8, loss = 0.91995854\n",
      "Iteration 9, loss = 0.81510296\n",
      "Iteration 10, loss = 0.73464151\n",
      "Iteration 11, loss = 0.66006151\n",
      "Iteration 12, loss = 0.58890987\n",
      "Iteration 13, loss = 0.54900335\n",
      "Iteration 14, loss = 0.49878670\n",
      "Iteration 15, loss = 0.46673160\n",
      "Iteration 16, loss = 0.42997958\n",
      "Iteration 17, loss = 0.42674740\n",
      "Iteration 18, loss = 0.39584225\n",
      "Iteration 19, loss = 0.39313039\n",
      "Iteration 20, loss = 0.41394535\n",
      "Iteration 21, loss = 0.40209726\n",
      "Iteration 22, loss = 0.37170984\n",
      "Iteration 23, loss = 0.35640402\n",
      "Iteration 24, loss = 0.34052928\n",
      "Iteration 25, loss = 0.33273623\n",
      "Iteration 26, loss = 0.31252206\n",
      "Iteration 27, loss = 0.31066316\n",
      "Iteration 28, loss = 0.29596559\n",
      "Iteration 29, loss = 0.33905790\n",
      "Iteration 30, loss = 0.30433842\n",
      "Iteration 31, loss = 0.29088254\n",
      "Iteration 32, loss = 0.28067883\n",
      "Iteration 33, loss = 0.28206174\n",
      "Iteration 34, loss = 0.27331521\n",
      "Iteration 35, loss = 0.27807339\n",
      "Iteration 36, loss = 0.27999244\n",
      "Iteration 37, loss = 0.29023704\n",
      "Iteration 38, loss = 0.27661740\n",
      "Iteration 39, loss = 0.27280852\n",
      "Iteration 40, loss = 0.28058768\n",
      "Iteration 41, loss = 0.29082340\n",
      "Iteration 42, loss = 0.28670012\n",
      "Iteration 43, loss = 0.26614292\n",
      "Iteration 44, loss = 0.27345547\n",
      "Iteration 45, loss = 0.26903611\n",
      "Iteration 46, loss = 0.25944975\n",
      "Iteration 47, loss = 0.26646914\n",
      "Iteration 48, loss = 0.26257728\n",
      "Iteration 49, loss = 0.26347186\n",
      "Iteration 50, loss = 0.25496744\n",
      "Iteration 51, loss = 0.26595924\n",
      "Iteration 52, loss = 0.25410539\n",
      "Iteration 53, loss = 0.25590124\n",
      "Iteration 54, loss = 0.25171562\n",
      "Iteration 55, loss = 0.25868956\n",
      "Iteration 56, loss = 0.25135541\n",
      "Iteration 57, loss = 0.25629298\n",
      "Iteration 58, loss = 0.26818081\n",
      "Iteration 59, loss = 0.27377002\n",
      "Iteration 60, loss = 0.26008427\n",
      "Iteration 61, loss = 0.25446847\n",
      "Iteration 62, loss = 0.27594364\n",
      "Iteration 63, loss = 0.29074300\n",
      "Iteration 64, loss = 0.26470728\n",
      "Iteration 65, loss = 0.25241339\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.30241425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 3.32349823\n",
      "Iteration 3, loss = 2.56214161\n",
      "Iteration 4, loss = 2.00356464\n",
      "Iteration 5, loss = 1.59630659\n",
      "Iteration 6, loss = 1.30592968\n",
      "Iteration 7, loss = 1.08288748\n",
      "Iteration 8, loss = 0.92260681\n",
      "Iteration 9, loss = 0.81314858\n",
      "Iteration 10, loss = 0.73342814\n",
      "Iteration 11, loss = 0.65868881\n",
      "Iteration 12, loss = 0.58644967\n",
      "Iteration 13, loss = 0.54690442\n",
      "Iteration 14, loss = 0.50224263\n",
      "Iteration 15, loss = 0.46591474\n",
      "Iteration 16, loss = 0.43239187\n",
      "Iteration 17, loss = 0.43616099\n",
      "Iteration 18, loss = 0.41362552\n",
      "Iteration 19, loss = 0.39721284\n",
      "Iteration 20, loss = 0.41467789\n",
      "Iteration 21, loss = 0.39734845\n",
      "Iteration 22, loss = 0.38476973\n",
      "Iteration 23, loss = 0.37296161\n",
      "Iteration 24, loss = 0.33867781\n",
      "Iteration 25, loss = 0.33057559\n",
      "Iteration 26, loss = 0.31870851\n",
      "Iteration 27, loss = 0.31764837\n",
      "Iteration 28, loss = 0.30796541\n",
      "Iteration 29, loss = 0.35948984\n",
      "Iteration 30, loss = 0.32560135\n",
      "Iteration 31, loss = 0.30445877\n",
      "Iteration 32, loss = 0.29659492\n",
      "Iteration 33, loss = 0.29768244\n",
      "Iteration 34, loss = 0.28921474\n",
      "Iteration 35, loss = 0.29267981\n",
      "Iteration 36, loss = 0.28839902\n",
      "Iteration 37, loss = 0.30063275\n",
      "Iteration 38, loss = 0.28555696\n",
      "Iteration 39, loss = 0.27497959\n",
      "Iteration 40, loss = 0.28717466\n",
      "Iteration 41, loss = 0.29692010\n",
      "Iteration 42, loss = 0.29605880\n",
      "Iteration 43, loss = 0.27310483\n",
      "Iteration 44, loss = 0.27742236\n",
      "Iteration 45, loss = 0.27595001\n",
      "Iteration 46, loss = 0.27231736\n",
      "Iteration 47, loss = 0.27721090\n",
      "Iteration 48, loss = 0.27625531\n",
      "Iteration 49, loss = 0.28519497\n",
      "Iteration 50, loss = 0.27196635\n",
      "Iteration 51, loss = 0.28391837\n",
      "Iteration 52, loss = 0.26961314\n",
      "Iteration 53, loss = 0.27031478\n",
      "Iteration 54, loss = 0.26375990\n",
      "Iteration 55, loss = 0.27467369\n",
      "Iteration 56, loss = 0.26221285\n",
      "Iteration 57, loss = 0.26831813\n",
      "Iteration 58, loss = 0.27551556\n",
      "Iteration 59, loss = 0.28202277\n",
      "Iteration 60, loss = 0.26688677\n",
      "Iteration 61, loss = 0.26213009\n",
      "Iteration 62, loss = 0.29121263\n",
      "Iteration 63, loss = 0.31061957\n",
      "Iteration 64, loss = 0.27423597\n",
      "Iteration 65, loss = 0.26326951\n",
      "Iteration 66, loss = 0.27447541\n",
      "Iteration 67, loss = 0.26649749\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.49235823\n",
      "Iteration 2, loss = 3.44327518\n",
      "Iteration 3, loss = 2.63063265\n",
      "Iteration 4, loss = 2.04277321\n",
      "Iteration 5, loss = 1.62202172\n",
      "Iteration 6, loss = 1.31922758\n",
      "Iteration 7, loss = 1.09416807\n",
      "Iteration 8, loss = 0.92754892\n",
      "Iteration 9, loss = 0.82038383\n",
      "Iteration 10, loss = 0.73167678\n",
      "Iteration 11, loss = 0.66493314\n",
      "Iteration 12, loss = 0.59402455\n",
      "Iteration 13, loss = 0.55980193\n",
      "Iteration 14, loss = 0.50733259\n",
      "Iteration 15, loss = 0.47282968\n",
      "Iteration 16, loss = 0.44003059\n",
      "Iteration 17, loss = 0.43740440\n",
      "Iteration 18, loss = 0.42276996\n",
      "Iteration 19, loss = 0.40526284\n",
      "Iteration 20, loss = 0.42498274\n",
      "Iteration 21, loss = 0.40297738\n",
      "Iteration 22, loss = 0.38095765\n",
      "Iteration 23, loss = 0.37254880\n",
      "Iteration 24, loss = 0.36127913\n",
      "Iteration 25, loss = 0.35430912\n",
      "Iteration 26, loss = 0.33334375\n",
      "Iteration 27, loss = 0.33116834\n",
      "Iteration 28, loss = 0.31526831\n",
      "Iteration 29, loss = 0.36156045\n",
      "Iteration 30, loss = 0.32720779\n",
      "Iteration 31, loss = 0.30867635\n",
      "Iteration 32, loss = 0.30089547\n",
      "Iteration 33, loss = 0.30723776\n",
      "Iteration 34, loss = 0.29869908\n",
      "Iteration 35, loss = 0.30061601\n",
      "Iteration 36, loss = 0.29804347\n",
      "Iteration 37, loss = 0.30595553\n",
      "Iteration 38, loss = 0.29240731\n",
      "Iteration 39, loss = 0.28550244\n",
      "Iteration 40, loss = 0.29549979\n",
      "Iteration 41, loss = 0.30550448\n",
      "Iteration 42, loss = 0.31720677\n",
      "Iteration 43, loss = 0.29151438\n",
      "Iteration 44, loss = 0.30015011\n",
      "Iteration 45, loss = 0.29109852\n",
      "Iteration 46, loss = 0.28411646\n",
      "Iteration 47, loss = 0.28860099\n",
      "Iteration 48, loss = 0.28138705\n",
      "Iteration 49, loss = 0.28693509\n",
      "Iteration 50, loss = 0.27746491\n",
      "Iteration 51, loss = 0.28985733\n",
      "Iteration 52, loss = 0.27564209\n",
      "Iteration 53, loss = 0.27715143\n",
      "Iteration 54, loss = 0.27345847\n",
      "Iteration 55, loss = 0.28266355\n",
      "Iteration 56, loss = 0.27013777\n",
      "Iteration 57, loss = 0.27527276\n",
      "Iteration 58, loss = 0.28823172\n",
      "Iteration 59, loss = 0.29769724\n",
      "Iteration 60, loss = 0.28050287\n",
      "Iteration 61, loss = 0.27051147\n",
      "Iteration 62, loss = 0.29062169\n",
      "Iteration 63, loss = 0.31396057\n",
      "Iteration 64, loss = 0.28079543\n",
      "Iteration 65, loss = 0.27082958\n",
      "Iteration 66, loss = 0.28539047\n",
      "Iteration 67, loss = 0.27732502\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.68092275\n",
      "Iteration 2, loss = 3.56170489\n",
      "Iteration 3, loss = 2.69500162\n",
      "Iteration 4, loss = 2.07378271\n",
      "Iteration 5, loss = 1.62575452\n",
      "Iteration 6, loss = 1.31770919\n",
      "Iteration 7, loss = 1.09502094\n",
      "Iteration 8, loss = 0.92394493\n",
      "Iteration 9, loss = 0.81464399\n",
      "Iteration 10, loss = 0.72580371\n",
      "Iteration 11, loss = 0.66484669\n",
      "Iteration 12, loss = 0.60012509\n",
      "Iteration 13, loss = 0.55552230\n",
      "Iteration 14, loss = 0.50887635\n",
      "Iteration 15, loss = 0.49146311\n",
      "Iteration 16, loss = 0.46385785\n",
      "Iteration 17, loss = 0.48572764\n",
      "Iteration 18, loss = 0.44043205\n",
      "Iteration 19, loss = 0.41250697\n",
      "Iteration 20, loss = 0.42453419\n",
      "Iteration 21, loss = 0.42518222\n",
      "Iteration 22, loss = 0.38488181\n",
      "Iteration 23, loss = 0.38367848\n",
      "Iteration 24, loss = 0.35225424\n",
      "Iteration 25, loss = 0.34800427\n",
      "Iteration 26, loss = 0.33616501\n",
      "Iteration 27, loss = 0.33519035\n",
      "Iteration 28, loss = 0.32679024\n",
      "Iteration 29, loss = 0.36785964\n",
      "Iteration 30, loss = 0.33609903\n",
      "Iteration 31, loss = 0.32036003\n",
      "Iteration 32, loss = 0.31457240\n",
      "Iteration 33, loss = 0.31724791\n",
      "Iteration 34, loss = 0.30960527\n",
      "Iteration 35, loss = 0.31186780\n",
      "Iteration 36, loss = 0.30793211\n",
      "Iteration 37, loss = 0.31898736\n",
      "Iteration 38, loss = 0.30081054\n",
      "Iteration 39, loss = 0.29435795\n",
      "Iteration 40, loss = 0.31271536\n",
      "Iteration 41, loss = 0.33165340\n",
      "Iteration 42, loss = 0.32296281\n",
      "Iteration 43, loss = 0.30091636\n",
      "Iteration 44, loss = 0.31163435\n",
      "Iteration 45, loss = 0.29583707\n",
      "Iteration 46, loss = 0.29039279\n",
      "Iteration 47, loss = 0.29336302\n",
      "Iteration 48, loss = 0.28916364\n",
      "Iteration 49, loss = 0.29913207\n",
      "Iteration 50, loss = 0.28971314\n",
      "Iteration 51, loss = 0.30160873\n",
      "Iteration 52, loss = 0.28795163\n",
      "Iteration 53, loss = 0.28880042\n",
      "Iteration 54, loss = 0.28867407\n",
      "Iteration 55, loss = 0.29791199\n",
      "Iteration 56, loss = 0.28315647\n",
      "Iteration 57, loss = 0.28948441\n",
      "Iteration 58, loss = 0.29964455\n",
      "Iteration 59, loss = 0.29678059\n",
      "Iteration 60, loss = 0.28429305\n",
      "Iteration 61, loss = 0.27965195\n",
      "Iteration 62, loss = 0.30207412\n",
      "Iteration 63, loss = 0.31433970\n",
      "Iteration 64, loss = 0.28936265\n",
      "Iteration 65, loss = 0.28654298\n",
      "Iteration 66, loss = 0.29905258\n",
      "Iteration 67, loss = 0.28878823\n",
      "Iteration 68, loss = 0.27939769\n",
      "Iteration 69, loss = 0.28533389\n",
      "Iteration 70, loss = 0.28590279\n",
      "Iteration 71, loss = 0.28248822\n",
      "Iteration 72, loss = 0.27807624\n",
      "Iteration 73, loss = 0.28864776\n",
      "Iteration 74, loss = 0.29149304\n",
      "Iteration 75, loss = 0.28368685\n",
      "Iteration 76, loss = 0.28190433\n",
      "Iteration 77, loss = 0.27362093\n",
      "Iteration 78, loss = 0.27735733\n",
      "Iteration 79, loss = 0.27904957\n",
      "Iteration 80, loss = 0.27422193\n",
      "Iteration 81, loss = 0.27345826\n",
      "Iteration 82, loss = 0.28144074\n",
      "Iteration 83, loss = 0.28351084\n",
      "Iteration 84, loss = 0.27748365\n",
      "Iteration 85, loss = 0.27381721\n",
      "Iteration 86, loss = 0.27747446\n",
      "Iteration 87, loss = 0.28360184\n",
      "Iteration 88, loss = 0.27823615\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.86888569\n",
      "Iteration 2, loss = 3.67702594\n",
      "Iteration 3, loss = 2.75733097\n",
      "Iteration 4, loss = 2.10461222\n",
      "Iteration 5, loss = 1.64085357\n",
      "Iteration 6, loss = 1.32589069\n",
      "Iteration 7, loss = 1.10019639\n",
      "Iteration 8, loss = 0.92720783\n",
      "Iteration 9, loss = 0.81541187\n",
      "Iteration 10, loss = 0.72985684\n",
      "Iteration 11, loss = 0.67032113\n",
      "Iteration 12, loss = 0.60756518\n",
      "Iteration 13, loss = 0.55527552\n",
      "Iteration 14, loss = 0.51501563\n",
      "Iteration 15, loss = 0.50054261\n",
      "Iteration 16, loss = 0.47247742\n",
      "Iteration 17, loss = 0.48860435\n",
      "Iteration 18, loss = 0.44831381\n",
      "Iteration 19, loss = 0.42633563\n",
      "Iteration 20, loss = 0.41825532\n",
      "Iteration 21, loss = 0.41829764\n",
      "Iteration 22, loss = 0.39062738\n",
      "Iteration 23, loss = 0.39319984\n",
      "Iteration 24, loss = 0.35959753\n",
      "Iteration 25, loss = 0.35882646\n",
      "Iteration 26, loss = 0.34510973\n",
      "Iteration 27, loss = 0.34848906\n",
      "Iteration 28, loss = 0.33705826\n",
      "Iteration 29, loss = 0.38827324\n",
      "Iteration 30, loss = 0.34528191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31, loss = 0.32934685\n",
      "Iteration 32, loss = 0.32644484\n",
      "Iteration 33, loss = 0.32752123\n",
      "Iteration 34, loss = 0.32099076\n",
      "Iteration 35, loss = 0.32437790\n",
      "Iteration 36, loss = 0.31686396\n",
      "Iteration 37, loss = 0.32724855\n",
      "Iteration 38, loss = 0.31349268\n",
      "Iteration 39, loss = 0.30687468\n",
      "Iteration 40, loss = 0.32563487\n",
      "Iteration 41, loss = 0.34059182\n",
      "Iteration 42, loss = 0.32761633\n",
      "Iteration 43, loss = 0.30765252\n",
      "Iteration 44, loss = 0.31786900\n",
      "Iteration 45, loss = 0.31027898\n",
      "Iteration 46, loss = 0.30229539\n",
      "Iteration 47, loss = 0.30346407\n",
      "Iteration 48, loss = 0.30016556\n",
      "Iteration 49, loss = 0.30967616\n",
      "Iteration 50, loss = 0.30328745\n",
      "Iteration 51, loss = 0.31289307\n",
      "Iteration 52, loss = 0.30007827\n",
      "Iteration 53, loss = 0.30488816\n",
      "Iteration 54, loss = 0.29960950\n",
      "Iteration 55, loss = 0.30073838\n",
      "Iteration 56, loss = 0.29250331\n",
      "Iteration 57, loss = 0.29514293\n",
      "Iteration 58, loss = 0.31059528\n",
      "Iteration 59, loss = 0.31539663\n",
      "Iteration 60, loss = 0.29843520\n",
      "Iteration 61, loss = 0.29243022\n",
      "Iteration 62, loss = 0.31878889\n",
      "Iteration 63, loss = 0.33371731\n",
      "Iteration 64, loss = 0.30196671\n",
      "Iteration 65, loss = 0.29464684\n",
      "Iteration 66, loss = 0.30962339\n",
      "Iteration 67, loss = 0.30181025\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.05591401\n",
      "Iteration 2, loss = 3.79210770\n",
      "Iteration 3, loss = 2.82442007\n",
      "Iteration 4, loss = 2.13931615\n",
      "Iteration 5, loss = 1.66409138\n",
      "Iteration 6, loss = 1.33368542\n",
      "Iteration 7, loss = 1.09870665\n",
      "Iteration 8, loss = 0.92794434\n",
      "Iteration 9, loss = 0.81630494\n",
      "Iteration 10, loss = 0.72191247\n",
      "Iteration 11, loss = 0.66397810\n",
      "Iteration 12, loss = 0.60086122\n",
      "Iteration 13, loss = 0.55433121\n",
      "Iteration 14, loss = 0.51269408\n",
      "Iteration 15, loss = 0.50254608\n",
      "Iteration 16, loss = 0.47386884\n",
      "Iteration 17, loss = 0.48340866\n",
      "Iteration 18, loss = 0.46907193\n",
      "Iteration 19, loss = 0.43023806\n",
      "Iteration 20, loss = 0.41372162\n",
      "Iteration 21, loss = 0.42414071\n",
      "Iteration 22, loss = 0.39910830\n",
      "Iteration 23, loss = 0.39848777\n",
      "Iteration 24, loss = 0.36441114\n",
      "Iteration 25, loss = 0.36622655\n",
      "Iteration 26, loss = 0.35269412\n",
      "Iteration 27, loss = 0.35396388\n",
      "Iteration 28, loss = 0.35182740\n",
      "Iteration 29, loss = 0.39153253\n",
      "Iteration 30, loss = 0.34962705\n",
      "Iteration 31, loss = 0.33555040\n",
      "Iteration 32, loss = 0.33593718\n",
      "Iteration 33, loss = 0.33600444\n",
      "Iteration 34, loss = 0.32743576\n",
      "Iteration 35, loss = 0.33760561\n",
      "Iteration 36, loss = 0.32428068\n",
      "Iteration 37, loss = 0.33707845\n",
      "Iteration 38, loss = 0.32414278\n",
      "Iteration 39, loss = 0.31872404\n",
      "Iteration 40, loss = 0.33909087\n",
      "Iteration 41, loss = 0.36120949\n",
      "Iteration 42, loss = 0.32914581\n",
      "Iteration 43, loss = 0.31531264\n",
      "Iteration 44, loss = 0.32606079\n",
      "Iteration 45, loss = 0.31715124\n",
      "Iteration 46, loss = 0.31156644\n",
      "Iteration 47, loss = 0.31044834\n",
      "Iteration 48, loss = 0.30636053\n",
      "Iteration 49, loss = 0.31901088\n",
      "Iteration 50, loss = 0.31276861\n",
      "Iteration 51, loss = 0.32405610\n",
      "Iteration 52, loss = 0.31281371\n",
      "Iteration 53, loss = 0.31967906\n",
      "Iteration 54, loss = 0.31018638\n",
      "Iteration 55, loss = 0.31700082\n",
      "Iteration 56, loss = 0.30373944\n",
      "Iteration 57, loss = 0.30448528\n",
      "Iteration 58, loss = 0.31244107\n",
      "Iteration 59, loss = 0.30683444\n",
      "Iteration 60, loss = 0.30447899\n",
      "Iteration 61, loss = 0.29881323\n",
      "Iteration 62, loss = 0.32405593\n",
      "Iteration 63, loss = 0.34588024\n",
      "Iteration 64, loss = 0.31392670\n",
      "Iteration 65, loss = 0.30390841\n",
      "Iteration 66, loss = 0.31736372\n",
      "Iteration 67, loss = 0.30934224\n",
      "Iteration 68, loss = 0.29913361\n",
      "Iteration 69, loss = 0.30220311\n",
      "Iteration 70, loss = 0.31664643\n",
      "Iteration 71, loss = 0.31681145\n",
      "Iteration 72, loss = 0.30328931\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.24180803\n",
      "Iteration 2, loss = 3.90265170\n",
      "Iteration 3, loss = 2.88268825\n",
      "Iteration 4, loss = 2.16986574\n",
      "Iteration 5, loss = 1.67722277\n",
      "Iteration 6, loss = 1.33959901\n",
      "Iteration 7, loss = 1.09920119\n",
      "Iteration 8, loss = 0.92605308\n",
      "Iteration 9, loss = 0.81549544\n",
      "Iteration 10, loss = 0.72135373\n",
      "Iteration 11, loss = 0.66485353\n",
      "Iteration 12, loss = 0.60593525\n",
      "Iteration 13, loss = 0.55393195\n",
      "Iteration 14, loss = 0.51632298\n",
      "Iteration 15, loss = 0.49943421\n",
      "Iteration 16, loss = 0.47137629\n",
      "Iteration 17, loss = 0.49482165\n",
      "Iteration 18, loss = 0.48339340\n",
      "Iteration 19, loss = 0.43453018\n",
      "Iteration 20, loss = 0.42468060\n",
      "Iteration 21, loss = 0.41839138\n",
      "Iteration 22, loss = 0.39487822\n",
      "Iteration 23, loss = 0.40174131\n",
      "Iteration 24, loss = 0.37330323\n",
      "Iteration 25, loss = 0.37464314\n",
      "Iteration 26, loss = 0.36267725\n",
      "Iteration 27, loss = 0.36470218\n",
      "Iteration 28, loss = 0.36219774\n",
      "Iteration 29, loss = 0.39486459\n",
      "Iteration 30, loss = 0.36037729\n",
      "Iteration 31, loss = 0.34610215\n",
      "Iteration 32, loss = 0.34637344\n",
      "Iteration 33, loss = 0.34693132\n",
      "Iteration 34, loss = 0.33749729\n",
      "Iteration 35, loss = 0.34902156\n",
      "Iteration 36, loss = 0.34033274\n",
      "Iteration 37, loss = 0.35138870\n",
      "Iteration 38, loss = 0.33782800\n",
      "Iteration 39, loss = 0.33167796\n",
      "Iteration 40, loss = 0.33977056\n",
      "Iteration 41, loss = 0.33924542\n",
      "Iteration 42, loss = 0.32712207\n",
      "Iteration 43, loss = 0.32136880\n",
      "Iteration 44, loss = 0.33564897\n",
      "Iteration 45, loss = 0.33087101\n",
      "Iteration 46, loss = 0.32168988\n",
      "Iteration 47, loss = 0.32189740\n",
      "Iteration 48, loss = 0.31578695\n",
      "Iteration 49, loss = 0.32665123\n",
      "Iteration 50, loss = 0.32159429\n",
      "Iteration 51, loss = 0.33038673\n",
      "Iteration 52, loss = 0.32341752\n",
      "Iteration 53, loss = 0.33154267\n",
      "Iteration 54, loss = 0.31872173\n",
      "Iteration 55, loss = 0.32534728\n",
      "Iteration 56, loss = 0.31246291\n",
      "Iteration 57, loss = 0.31470132\n",
      "Iteration 58, loss = 0.32222218\n",
      "Iteration 59, loss = 0.31636822\n",
      "Iteration 60, loss = 0.31497062\n",
      "Iteration 61, loss = 0.31012278\n",
      "Iteration 62, loss = 0.33378021\n",
      "Iteration 63, loss = 0.34574330\n",
      "Iteration 64, loss = 0.31654332\n",
      "Iteration 65, loss = 0.31298579\n",
      "Iteration 66, loss = 0.32936238\n",
      "Iteration 67, loss = 0.32019978\n",
      "Iteration 68, loss = 0.31073979\n",
      "Iteration 69, loss = 0.31259026\n",
      "Iteration 70, loss = 0.32787856\n",
      "Iteration 71, loss = 0.32866989\n",
      "Iteration 72, loss = 0.31086983\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.42676031\n",
      "Iteration 2, loss = 4.00955241\n",
      "Iteration 3, loss = 2.94085684\n",
      "Iteration 4, loss = 2.19911663\n",
      "Iteration 5, loss = 1.69290249\n",
      "Iteration 6, loss = 1.34832886\n",
      "Iteration 7, loss = 1.10349064\n",
      "Iteration 8, loss = 0.92833309\n",
      "Iteration 9, loss = 0.81475098\n",
      "Iteration 10, loss = 0.72642844\n",
      "Iteration 11, loss = 0.67077820\n",
      "Iteration 12, loss = 0.61535122\n",
      "Iteration 13, loss = 0.55382206\n",
      "Iteration 14, loss = 0.51578168\n",
      "Iteration 15, loss = 0.50134217\n",
      "Iteration 16, loss = 0.47904693\n",
      "Iteration 17, loss = 0.49586597\n",
      "Iteration 18, loss = 0.49668104\n",
      "Iteration 19, loss = 0.44613568\n",
      "Iteration 20, loss = 0.42079390\n",
      "Iteration 21, loss = 0.41367613\n",
      "Iteration 22, loss = 0.39813052\n",
      "Iteration 23, loss = 0.40369877\n",
      "Iteration 24, loss = 0.37797580\n",
      "Iteration 25, loss = 0.37997046\n",
      "Iteration 26, loss = 0.36930875\n",
      "Iteration 27, loss = 0.37318814\n",
      "Iteration 28, loss = 0.36501817\n",
      "Iteration 29, loss = 0.40044163\n",
      "Iteration 30, loss = 0.36250842\n",
      "Iteration 31, loss = 0.34959632\n",
      "Iteration 32, loss = 0.34994748\n",
      "Iteration 33, loss = 0.35400403\n",
      "Iteration 34, loss = 0.34450342\n",
      "Iteration 35, loss = 0.35255246\n",
      "Iteration 36, loss = 0.34234018\n",
      "Iteration 37, loss = 0.35609703\n",
      "Iteration 38, loss = 0.34138338\n",
      "Iteration 39, loss = 0.33631366\n",
      "Iteration 40, loss = 0.34959228\n",
      "Iteration 41, loss = 0.36835241\n",
      "Iteration 42, loss = 0.34031222\n",
      "Iteration 43, loss = 0.33150833\n",
      "Iteration 44, loss = 0.34211988\n",
      "Iteration 45, loss = 0.33933306\n",
      "Iteration 46, loss = 0.33110285\n",
      "Iteration 47, loss = 0.33178317\n",
      "Iteration 48, loss = 0.32433715\n",
      "Iteration 49, loss = 0.33863871\n",
      "Iteration 50, loss = 0.33077178\n",
      "Iteration 51, loss = 0.33585362\n",
      "Iteration 52, loss = 0.32870434\n",
      "Iteration 53, loss = 0.33802079\n",
      "Iteration 54, loss = 0.32415499\n",
      "Iteration 55, loss = 0.33680495\n",
      "Iteration 56, loss = 0.32372383\n",
      "Iteration 57, loss = 0.32194131\n",
      "Iteration 58, loss = 0.33004059\n",
      "Iteration 59, loss = 0.32434508\n",
      "Iteration 60, loss = 0.32233476\n",
      "Iteration 61, loss = 0.31759328\n",
      "Iteration 62, loss = 0.34066324\n",
      "Iteration 63, loss = 0.34920568\n",
      "Iteration 64, loss = 0.32519988\n",
      "Iteration 65, loss = 0.32181504\n",
      "Iteration 66, loss = 0.34109494\n",
      "Iteration 67, loss = 0.32927166\n",
      "Iteration 68, loss = 0.31822225\n",
      "Iteration 69, loss = 0.32079433\n",
      "Iteration 70, loss = 0.33349405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 71, loss = 0.33550242\n",
      "Iteration 72, loss = 0.31789582\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64020699\n",
      "Iteration 2, loss = 0.45375192\n",
      "Iteration 3, loss = 0.36535214\n",
      "Iteration 4, loss = 0.29989023\n",
      "Iteration 5, loss = 0.26294606\n",
      "Iteration 6, loss = 0.21521322\n",
      "Iteration 7, loss = 0.17290160\n",
      "Iteration 8, loss = 0.14688041\n",
      "Iteration 9, loss = 0.11438281\n",
      "Iteration 10, loss = 0.08955941\n",
      "Iteration 11, loss = 0.06569462\n",
      "Iteration 12, loss = 0.04950183\n",
      "Iteration 13, loss = 0.04094619\n",
      "Iteration 14, loss = 0.03343195\n",
      "Iteration 15, loss = 0.02026761\n",
      "Iteration 16, loss = 0.01639229\n",
      "Iteration 17, loss = 0.01197617\n",
      "Iteration 18, loss = 0.00870137\n",
      "Iteration 19, loss = 0.01019992\n",
      "Iteration 20, loss = 0.00737855\n",
      "Iteration 21, loss = 0.00557300\n",
      "Iteration 22, loss = 0.00380508\n",
      "Iteration 23, loss = 0.00276115\n",
      "Iteration 24, loss = 0.00231511\n",
      "Iteration 25, loss = 0.00207245\n",
      "Iteration 26, loss = 0.00188272\n",
      "Iteration 27, loss = 0.00175958\n",
      "Iteration 28, loss = 0.00147860\n",
      "Iteration 29, loss = 0.00135432\n",
      "Iteration 30, loss = 0.00121640\n",
      "Iteration 31, loss = 0.00112947\n",
      "Iteration 32, loss = 0.00104249\n",
      "Iteration 33, loss = 0.00097431\n",
      "Iteration 34, loss = 0.00088308\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85777322\n",
      "Iteration 2, loss = 0.66932295\n",
      "Iteration 3, loss = 0.57881945\n",
      "Iteration 4, loss = 0.50912012\n",
      "Iteration 5, loss = 0.46923421\n",
      "Iteration 6, loss = 0.42223451\n",
      "Iteration 7, loss = 0.38343518\n",
      "Iteration 8, loss = 0.35379265\n",
      "Iteration 9, loss = 0.31383673\n",
      "Iteration 10, loss = 0.29024107\n",
      "Iteration 11, loss = 0.26946075\n",
      "Iteration 12, loss = 0.24857469\n",
      "Iteration 13, loss = 0.22914917\n",
      "Iteration 14, loss = 0.21991774\n",
      "Iteration 15, loss = 0.20233440\n",
      "Iteration 16, loss = 0.19504245\n",
      "Iteration 17, loss = 0.18454848\n",
      "Iteration 18, loss = 0.17855658\n",
      "Iteration 19, loss = 0.17336670\n",
      "Iteration 20, loss = 0.16908244\n",
      "Iteration 21, loss = 0.16009257\n",
      "Iteration 22, loss = 0.15469303\n",
      "Iteration 23, loss = 0.14939521\n",
      "Iteration 24, loss = 0.14448529\n",
      "Iteration 25, loss = 0.14054518\n",
      "Iteration 26, loss = 0.13752281\n",
      "Iteration 27, loss = 0.13435222\n",
      "Iteration 28, loss = 0.12910588\n",
      "Iteration 29, loss = 0.12518640\n",
      "Iteration 30, loss = 0.12164092\n",
      "Iteration 31, loss = 0.11829478\n",
      "Iteration 32, loss = 0.11497399\n",
      "Iteration 33, loss = 0.11170952\n",
      "Iteration 34, loss = 0.11027534\n",
      "Iteration 35, loss = 0.10617696\n",
      "Iteration 36, loss = 0.10322129\n",
      "Iteration 37, loss = 0.10075311\n",
      "Iteration 38, loss = 0.09779761\n",
      "Iteration 39, loss = 0.09522913\n",
      "Iteration 40, loss = 0.09319981\n",
      "Iteration 41, loss = 0.09059386\n",
      "Iteration 42, loss = 0.08768543\n",
      "Iteration 43, loss = 0.08541514\n",
      "Iteration 44, loss = 0.08309498\n",
      "Iteration 45, loss = 0.08134714\n",
      "Iteration 46, loss = 0.07883790\n",
      "Iteration 47, loss = 0.07667223\n",
      "Iteration 48, loss = 0.07532252\n",
      "Iteration 49, loss = 0.07314194\n",
      "Iteration 50, loss = 0.07135762\n",
      "Iteration 51, loss = 0.06953797\n",
      "Iteration 52, loss = 0.06802708\n",
      "Iteration 53, loss = 0.06667699\n",
      "Iteration 54, loss = 0.06522441\n",
      "Iteration 55, loss = 0.06363544\n",
      "Iteration 56, loss = 0.06299129\n",
      "Iteration 57, loss = 0.39474711\n",
      "Iteration 58, loss = 0.41674680\n",
      "Iteration 59, loss = 0.26985759\n",
      "Iteration 60, loss = 0.17396021\n",
      "Iteration 61, loss = 0.12154737\n",
      "Iteration 62, loss = 0.12816786\n",
      "Iteration 63, loss = 0.09506269\n",
      "Iteration 64, loss = 0.08739343\n",
      "Iteration 65, loss = 0.08363507\n",
      "Iteration 66, loss = 0.08007999\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.07294274\n",
      "Iteration 2, loss = 0.87487616\n",
      "Iteration 3, loss = 0.77053453\n",
      "Iteration 4, loss = 0.69011888\n",
      "Iteration 5, loss = 0.64428803\n",
      "Iteration 6, loss = 0.58600966\n",
      "Iteration 7, loss = 0.53168731\n",
      "Iteration 8, loss = 0.49302382\n",
      "Iteration 9, loss = 0.46744382\n",
      "Iteration 10, loss = 0.41884978\n",
      "Iteration 11, loss = 0.38350480\n",
      "Iteration 12, loss = 0.35277381\n",
      "Iteration 13, loss = 0.33737696\n",
      "Iteration 14, loss = 0.31731234\n",
      "Iteration 15, loss = 0.29304871\n",
      "Iteration 16, loss = 0.28345444\n",
      "Iteration 17, loss = 0.26360924\n",
      "Iteration 18, loss = 0.25126560\n",
      "Iteration 19, loss = 0.24623238\n",
      "Iteration 20, loss = 0.29364162\n",
      "Iteration 21, loss = 0.27725595\n",
      "Iteration 22, loss = 0.24476653\n",
      "Iteration 23, loss = 0.22533091\n",
      "Iteration 24, loss = 0.20723963\n",
      "Iteration 25, loss = 0.19821277\n",
      "Iteration 26, loss = 0.19118757\n",
      "Iteration 27, loss = 0.18319121\n",
      "Iteration 28, loss = 0.17621837\n",
      "Iteration 29, loss = 0.16907441\n",
      "Iteration 30, loss = 0.16275956\n",
      "Iteration 31, loss = 0.15702269\n",
      "Iteration 32, loss = 0.15110466\n",
      "Iteration 33, loss = 0.14606061\n",
      "Iteration 34, loss = 0.14288557\n",
      "Iteration 35, loss = 0.13682554\n",
      "Iteration 36, loss = 0.13251508\n",
      "Iteration 37, loss = 0.12952021\n",
      "Iteration 38, loss = 0.12468040\n",
      "Iteration 39, loss = 0.12062572\n",
      "Iteration 40, loss = 0.11772163\n",
      "Iteration 41, loss = 0.11395054\n",
      "Iteration 42, loss = 0.11166510\n",
      "Iteration 43, loss = 0.11398404\n",
      "Iteration 44, loss = 0.17889630\n",
      "Iteration 45, loss = 0.37784039\n",
      "Iteration 46, loss = 0.25579284\n",
      "Iteration 47, loss = 0.24400168\n",
      "Iteration 48, loss = 0.17311276\n",
      "Iteration 49, loss = 0.14348314\n",
      "Iteration 50, loss = 0.13655548\n",
      "Iteration 51, loss = 0.12504007\n",
      "Iteration 52, loss = 0.11600405\n",
      "Iteration 53, loss = 0.11134758\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.28663874\n",
      "Iteration 2, loss = 1.07147664\n",
      "Iteration 3, loss = 0.95150920\n",
      "Iteration 4, loss = 0.85438644\n",
      "Iteration 5, loss = 0.78997695\n",
      "Iteration 6, loss = 0.71886638\n",
      "Iteration 7, loss = 0.65527924\n",
      "Iteration 8, loss = 0.60624415\n",
      "Iteration 9, loss = 0.56195541\n",
      "Iteration 10, loss = 0.51348175\n",
      "Iteration 11, loss = 0.46825704\n",
      "Iteration 12, loss = 0.43075025\n",
      "Iteration 13, loss = 0.40939766\n",
      "Iteration 14, loss = 0.38269871\n",
      "Iteration 15, loss = 0.35657578\n",
      "Iteration 16, loss = 0.34645415\n",
      "Iteration 17, loss = 0.31707245\n",
      "Iteration 18, loss = 0.30000777\n",
      "Iteration 19, loss = 0.28664286\n",
      "Iteration 20, loss = 0.32149690\n",
      "Iteration 21, loss = 0.29672925\n",
      "Iteration 22, loss = 0.26666896\n",
      "Iteration 23, loss = 0.24600658\n",
      "Iteration 24, loss = 0.23277490\n",
      "Iteration 25, loss = 0.22043636\n",
      "Iteration 26, loss = 0.21140751\n",
      "Iteration 27, loss = 0.20324890\n",
      "Iteration 28, loss = 0.19230123\n",
      "Iteration 29, loss = 0.18398492\n",
      "Iteration 30, loss = 0.17730326\n",
      "Iteration 31, loss = 0.17034458\n",
      "Iteration 32, loss = 0.16473032\n",
      "Iteration 33, loss = 0.15922551\n",
      "Iteration 34, loss = 0.24754507\n",
      "Iteration 35, loss = 0.23182273\n",
      "Iteration 36, loss = 0.21533261\n",
      "Iteration 37, loss = 0.25391231\n",
      "Iteration 38, loss = 0.19569984\n",
      "Iteration 39, loss = 0.16806612\n",
      "Iteration 40, loss = 0.15588085\n",
      "Iteration 41, loss = 0.14724438\n",
      "Iteration 42, loss = 0.14063010\n",
      "Iteration 43, loss = 0.13556351\n",
      "Iteration 44, loss = 0.13100095\n",
      "Iteration 45, loss = 0.12673557\n",
      "Iteration 46, loss = 0.12253442\n",
      "Iteration 47, loss = 0.11920464\n",
      "Iteration 48, loss = 0.11601164\n",
      "Iteration 49, loss = 0.11323307\n",
      "Iteration 50, loss = 0.11024586\n",
      "Iteration 51, loss = 0.10850134\n",
      "Iteration 52, loss = 0.10583191\n",
      "Iteration 53, loss = 0.10390704\n",
      "Iteration 54, loss = 0.10095593\n",
      "Iteration 55, loss = 0.09856366\n",
      "Iteration 56, loss = 0.09645408\n",
      "Iteration 57, loss = 0.09455717\n",
      "Iteration 58, loss = 0.09313054\n",
      "Iteration 59, loss = 0.09132407\n",
      "Iteration 60, loss = 0.08919468\n",
      "Iteration 61, loss = 0.08765922\n",
      "Iteration 62, loss = 0.08726758\n",
      "Iteration 63, loss = 0.08581100\n",
      "Iteration 64, loss = 0.08389205\n",
      "Iteration 65, loss = 0.08323590\n",
      "Iteration 66, loss = 0.08176419\n",
      "Iteration 67, loss = 0.08130935\n",
      "Iteration 68, loss = 0.07929729\n",
      "Iteration 69, loss = 0.07834285\n",
      "Iteration 70, loss = 0.07691178\n",
      "Iteration 71, loss = 0.07629027\n",
      "Iteration 72, loss = 0.07621153\n",
      "Iteration 73, loss = 0.14583512\n",
      "Iteration 74, loss = 0.21130617\n",
      "Iteration 75, loss = 0.23321373\n",
      "Iteration 76, loss = 0.18112080\n",
      "Iteration 77, loss = 0.15788234\n",
      "Iteration 78, loss = 0.12741378\n",
      "Iteration 79, loss = 0.11439152\n",
      "Iteration 80, loss = 0.10449136\n",
      "Iteration 81, loss = 0.10002997\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.49837120\n",
      "Iteration 2, loss = 1.26094134\n",
      "Iteration 3, loss = 1.11814944\n",
      "Iteration 4, loss = 0.99943176\n",
      "Iteration 5, loss = 0.91697799\n",
      "Iteration 6, loss = 0.82704136\n",
      "Iteration 7, loss = 0.74868230\n",
      "Iteration 8, loss = 0.69010966\n",
      "Iteration 9, loss = 0.64028356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.58268921\n",
      "Iteration 11, loss = 0.52886868\n",
      "Iteration 12, loss = 0.48299182\n",
      "Iteration 13, loss = 0.45757868\n",
      "Iteration 14, loss = 0.42356653\n",
      "Iteration 15, loss = 0.38987792\n",
      "Iteration 16, loss = 0.37678121\n",
      "Iteration 17, loss = 0.35168042\n",
      "Iteration 18, loss = 0.33348860\n",
      "Iteration 19, loss = 0.31227202\n",
      "Iteration 20, loss = 0.31012159\n",
      "Iteration 21, loss = 0.28682191\n",
      "Iteration 22, loss = 0.27213641\n",
      "Iteration 23, loss = 0.25162528\n",
      "Iteration 24, loss = 0.23830836\n",
      "Iteration 25, loss = 0.23579271\n",
      "Iteration 26, loss = 0.22674255\n",
      "Iteration 27, loss = 0.22177723\n",
      "Iteration 28, loss = 0.28593584\n",
      "Iteration 29, loss = 0.29308132\n",
      "Iteration 30, loss = 0.26153091\n",
      "Iteration 31, loss = 0.23452466\n",
      "Iteration 32, loss = 0.20933187\n",
      "Iteration 33, loss = 0.19052211\n",
      "Iteration 34, loss = 0.18007806\n",
      "Iteration 35, loss = 0.17109529\n",
      "Iteration 36, loss = 0.16462371\n",
      "Iteration 37, loss = 0.15928141\n",
      "Iteration 38, loss = 0.15315121\n",
      "Iteration 39, loss = 0.14890166\n",
      "Iteration 40, loss = 0.14455811\n",
      "Iteration 41, loss = 0.14011226\n",
      "Iteration 42, loss = 0.13556797\n",
      "Iteration 43, loss = 0.13303784\n",
      "Iteration 44, loss = 0.13020150\n",
      "Iteration 45, loss = 0.12860122\n",
      "Iteration 46, loss = 0.12387654\n",
      "Iteration 47, loss = 0.12010068\n",
      "Iteration 48, loss = 0.11953020\n",
      "Iteration 49, loss = 0.11709118\n",
      "Iteration 50, loss = 0.11556202\n",
      "Iteration 51, loss = 0.11276080\n",
      "Iteration 52, loss = 0.11649109\n",
      "Iteration 53, loss = 0.11896394\n",
      "Iteration 54, loss = 0.11575386\n",
      "Iteration 55, loss = 0.11824007\n",
      "Iteration 56, loss = 0.12747622\n",
      "Iteration 57, loss = 0.14658896\n",
      "Iteration 58, loss = 0.20889276\n",
      "Iteration 59, loss = 0.18548623\n",
      "Iteration 60, loss = 0.14581756\n",
      "Iteration 61, loss = 0.12931162\n",
      "Iteration 62, loss = 0.12147424\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.70767760\n",
      "Iteration 2, loss = 1.44222921\n",
      "Iteration 3, loss = 1.26926411\n",
      "Iteration 4, loss = 1.12686553\n",
      "Iteration 5, loss = 1.02085339\n",
      "Iteration 6, loss = 0.91571502\n",
      "Iteration 7, loss = 0.82087913\n",
      "Iteration 8, loss = 0.75025545\n",
      "Iteration 9, loss = 0.69191115\n",
      "Iteration 10, loss = 0.61879594\n",
      "Iteration 11, loss = 0.55870149\n",
      "Iteration 12, loss = 0.51410713\n",
      "Iteration 13, loss = 0.48380543\n",
      "Iteration 14, loss = 0.44693917\n",
      "Iteration 15, loss = 0.41399394\n",
      "Iteration 16, loss = 0.38570828\n",
      "Iteration 17, loss = 0.37072289\n",
      "Iteration 18, loss = 0.34702575\n",
      "Iteration 19, loss = 0.33647015\n",
      "Iteration 20, loss = 0.35931862\n",
      "Iteration 21, loss = 0.35035906\n",
      "Iteration 22, loss = 0.32959210\n",
      "Iteration 23, loss = 0.28359842\n",
      "Iteration 24, loss = 0.26441043\n",
      "Iteration 25, loss = 0.26532522\n",
      "Iteration 26, loss = 0.23885597\n",
      "Iteration 27, loss = 0.23996167\n",
      "Iteration 28, loss = 0.21907706\n",
      "Iteration 29, loss = 0.20826718\n",
      "Iteration 30, loss = 0.19956183\n",
      "Iteration 31, loss = 0.19068137\n",
      "Iteration 32, loss = 0.18430789\n",
      "Iteration 33, loss = 0.17829585\n",
      "Iteration 34, loss = 0.17814895\n",
      "Iteration 35, loss = 0.17034109\n",
      "Iteration 36, loss = 0.17685488\n",
      "Iteration 37, loss = 0.19257139\n",
      "Iteration 38, loss = 0.21626931\n",
      "Iteration 39, loss = 0.18724032\n",
      "Iteration 40, loss = 0.18349037\n",
      "Iteration 41, loss = 0.16462388\n",
      "Iteration 42, loss = 0.15337254\n",
      "Iteration 43, loss = 0.14620861\n",
      "Iteration 44, loss = 0.14118619\n",
      "Iteration 45, loss = 0.14154047\n",
      "Iteration 46, loss = 0.13376921\n",
      "Iteration 47, loss = 0.12939502\n",
      "Iteration 48, loss = 0.12863961\n",
      "Iteration 49, loss = 0.12742229\n",
      "Iteration 50, loss = 0.12509088\n",
      "Iteration 51, loss = 0.12105322\n",
      "Iteration 52, loss = 0.12053361\n",
      "Iteration 53, loss = 0.12336820\n",
      "Iteration 54, loss = 0.16196199\n",
      "Iteration 55, loss = 0.17638684\n",
      "Iteration 56, loss = 0.18091457\n",
      "Iteration 57, loss = 0.18561120\n",
      "Iteration 58, loss = 0.15374952\n",
      "Iteration 59, loss = 0.15067228\n",
      "Iteration 60, loss = 0.13824611\n",
      "Iteration 61, loss = 0.12851564\n",
      "Iteration 62, loss = 0.12243164\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.91545692\n",
      "Iteration 2, loss = 1.61837645\n",
      "Iteration 3, loss = 1.41321295\n",
      "Iteration 4, loss = 1.24230749\n",
      "Iteration 5, loss = 1.11402469\n",
      "Iteration 6, loss = 0.99033216\n",
      "Iteration 7, loss = 0.88148560\n",
      "Iteration 8, loss = 0.79760907\n",
      "Iteration 9, loss = 0.74106871\n",
      "Iteration 10, loss = 0.65788222\n",
      "Iteration 11, loss = 0.59417253\n",
      "Iteration 12, loss = 0.54291320\n",
      "Iteration 13, loss = 0.51087070\n",
      "Iteration 14, loss = 0.47488994\n",
      "Iteration 15, loss = 0.44304813\n",
      "Iteration 16, loss = 0.42128656\n",
      "Iteration 17, loss = 0.38188036\n",
      "Iteration 18, loss = 0.36753810\n",
      "Iteration 19, loss = 0.35165197\n",
      "Iteration 20, loss = 0.34840343\n",
      "Iteration 21, loss = 0.33544244\n",
      "Iteration 22, loss = 0.31693068\n",
      "Iteration 23, loss = 0.28796498\n",
      "Iteration 24, loss = 0.26897928\n",
      "Iteration 25, loss = 0.28120173\n",
      "Iteration 26, loss = 0.24865204\n",
      "Iteration 27, loss = 0.26219993\n",
      "Iteration 28, loss = 0.23830471\n",
      "Iteration 29, loss = 0.22494431\n",
      "Iteration 30, loss = 0.21121107\n",
      "Iteration 31, loss = 0.19898924\n",
      "Iteration 32, loss = 0.19308341\n",
      "Iteration 33, loss = 0.18599987\n",
      "Iteration 34, loss = 0.18766214\n",
      "Iteration 35, loss = 0.17864813\n",
      "Iteration 36, loss = 0.17832273\n",
      "Iteration 37, loss = 0.17619017\n",
      "Iteration 38, loss = 0.17948364\n",
      "Iteration 39, loss = 0.16762987\n",
      "Iteration 40, loss = 0.15843481\n",
      "Iteration 41, loss = 0.15452538\n",
      "Iteration 42, loss = 0.16574428\n",
      "Iteration 43, loss = 0.16638651\n",
      "Iteration 44, loss = 0.16964656\n",
      "Iteration 45, loss = 0.16187956\n",
      "Iteration 46, loss = 0.15374435\n",
      "Iteration 47, loss = 0.17839820\n",
      "Iteration 48, loss = 0.17330781\n",
      "Iteration 49, loss = 0.19622983\n",
      "Iteration 50, loss = 0.18068333\n",
      "Iteration 51, loss = 0.15805921\n",
      "Iteration 52, loss = 0.14723181\n",
      "Iteration 53, loss = 0.14105569\n",
      "Iteration 54, loss = 0.13442859\n",
      "Iteration 55, loss = 0.13017751\n",
      "Iteration 56, loss = 0.12905642\n",
      "Iteration 57, loss = 0.12692906\n",
      "Iteration 58, loss = 0.12516580\n",
      "Iteration 59, loss = 0.13006491\n",
      "Iteration 60, loss = 0.12675947\n",
      "Iteration 61, loss = 0.12282363\n",
      "Iteration 62, loss = 0.12236056\n",
      "Iteration 63, loss = 0.12352986\n",
      "Iteration 64, loss = 0.12428873\n",
      "Iteration 65, loss = 0.12338432\n",
      "Iteration 66, loss = 0.12016853\n",
      "Iteration 67, loss = 0.12389821\n",
      "Iteration 68, loss = 0.11766772\n",
      "Iteration 69, loss = 0.11659939\n",
      "Iteration 70, loss = 0.11478019\n",
      "Iteration 71, loss = 0.11213032\n",
      "Iteration 72, loss = 0.11209015\n",
      "Iteration 73, loss = 0.13340450\n",
      "Iteration 74, loss = 0.12927572\n",
      "Iteration 75, loss = 0.15205224\n",
      "Iteration 76, loss = 0.15242637\n",
      "Iteration 77, loss = 0.15028591\n",
      "Iteration 78, loss = 0.15779769\n",
      "Iteration 79, loss = 0.16079443\n",
      "Iteration 80, loss = 0.15561088\n",
      "Iteration 81, loss = 0.13921820\n",
      "Iteration 82, loss = 0.12665310\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12145916\n",
      "Iteration 2, loss = 1.78543438\n",
      "Iteration 3, loss = 1.54764061\n",
      "Iteration 4, loss = 1.34601158\n",
      "Iteration 5, loss = 1.19606868\n",
      "Iteration 6, loss = 1.05248070\n",
      "Iteration 7, loss = 0.93292680\n",
      "Iteration 8, loss = 0.83803402\n",
      "Iteration 9, loss = 0.76995798\n",
      "Iteration 10, loss = 0.69141380\n",
      "Iteration 11, loss = 0.61654043\n",
      "Iteration 12, loss = 0.56139536\n",
      "Iteration 13, loss = 0.52658981\n",
      "Iteration 14, loss = 0.48678556\n",
      "Iteration 15, loss = 0.47384374\n",
      "Iteration 16, loss = 0.45180304\n",
      "Iteration 17, loss = 0.42636126\n",
      "Iteration 18, loss = 0.40315547\n",
      "Iteration 19, loss = 0.37843097\n",
      "Iteration 20, loss = 0.36823325\n",
      "Iteration 21, loss = 0.32553172\n",
      "Iteration 22, loss = 0.30053645\n",
      "Iteration 23, loss = 0.28382724\n",
      "Iteration 24, loss = 0.27454024\n",
      "Iteration 25, loss = 0.26887226\n",
      "Iteration 26, loss = 0.25325855\n",
      "Iteration 27, loss = 0.27346373\n",
      "Iteration 28, loss = 0.28511570\n",
      "Iteration 29, loss = 0.26504580\n",
      "Iteration 30, loss = 0.24291104\n",
      "Iteration 31, loss = 0.22459267\n",
      "Iteration 32, loss = 0.21715127\n",
      "Iteration 33, loss = 0.20669363\n",
      "Iteration 34, loss = 0.21111789\n",
      "Iteration 35, loss = 0.19540648\n",
      "Iteration 36, loss = 0.18832615\n",
      "Iteration 37, loss = 0.18200210\n",
      "Iteration 38, loss = 0.18413264\n",
      "Iteration 39, loss = 0.17732696\n",
      "Iteration 40, loss = 0.17237037\n",
      "Iteration 41, loss = 0.16642476\n",
      "Iteration 42, loss = 0.17148502\n",
      "Iteration 43, loss = 0.17401416\n",
      "Iteration 44, loss = 0.18695838\n",
      "Iteration 45, loss = 0.17421783\n",
      "Iteration 46, loss = 0.16518034\n",
      "Iteration 47, loss = 0.20624174\n",
      "Iteration 48, loss = 0.20006305\n",
      "Iteration 49, loss = 0.19698242\n",
      "Iteration 50, loss = 0.17246022\n",
      "Iteration 51, loss = 0.15913727\n",
      "Iteration 52, loss = 0.15509076\n",
      "Iteration 53, loss = 0.15328538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 54, loss = 0.14725835\n",
      "Iteration 55, loss = 0.14243202\n",
      "Iteration 56, loss = 0.14130392\n",
      "Iteration 57, loss = 0.13986908\n",
      "Iteration 58, loss = 0.13756192\n",
      "Iteration 59, loss = 0.13870526\n",
      "Iteration 60, loss = 0.13417744\n",
      "Iteration 61, loss = 0.13224535\n",
      "Iteration 62, loss = 0.13245786\n",
      "Iteration 63, loss = 0.13217627\n",
      "Iteration 64, loss = 0.13057330\n",
      "Iteration 65, loss = 0.13142650\n",
      "Iteration 66, loss = 0.12959381\n",
      "Iteration 67, loss = 0.13049028\n",
      "Iteration 68, loss = 0.12738293\n",
      "Iteration 69, loss = 0.12961827\n",
      "Iteration 70, loss = 0.12855176\n",
      "Iteration 71, loss = 0.12980560\n",
      "Iteration 72, loss = 0.12737483\n",
      "Iteration 73, loss = 0.14761501\n",
      "Iteration 74, loss = 0.14438328\n",
      "Iteration 75, loss = 0.14305801\n",
      "Iteration 76, loss = 0.13737123\n",
      "Iteration 77, loss = 0.14021359\n",
      "Iteration 78, loss = 0.13933223\n",
      "Iteration 79, loss = 0.14481121\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32655036\n",
      "Iteration 2, loss = 1.95013740\n",
      "Iteration 3, loss = 1.66729882\n",
      "Iteration 4, loss = 1.43471672\n",
      "Iteration 5, loss = 1.26039813\n",
      "Iteration 6, loss = 1.10117400\n",
      "Iteration 7, loss = 0.96624388\n",
      "Iteration 8, loss = 0.86643051\n",
      "Iteration 9, loss = 0.79573799\n",
      "Iteration 10, loss = 0.70241904\n",
      "Iteration 11, loss = 0.63019322\n",
      "Iteration 12, loss = 0.57466959\n",
      "Iteration 13, loss = 0.53340937\n",
      "Iteration 14, loss = 0.49926528\n",
      "Iteration 15, loss = 0.46481555\n",
      "Iteration 16, loss = 0.44641423\n",
      "Iteration 17, loss = 0.40356180\n",
      "Iteration 18, loss = 0.38462816\n",
      "Iteration 19, loss = 0.37079308\n",
      "Iteration 20, loss = 0.37004862\n",
      "Iteration 21, loss = 0.36071485\n",
      "Iteration 22, loss = 0.33684496\n",
      "Iteration 23, loss = 0.29505460\n",
      "Iteration 24, loss = 0.27973130\n",
      "Iteration 25, loss = 0.27736822\n",
      "Iteration 26, loss = 0.26078766\n",
      "Iteration 27, loss = 0.26386945\n",
      "Iteration 28, loss = 0.24785644\n",
      "Iteration 29, loss = 0.23812548\n",
      "Iteration 30, loss = 0.23691081\n",
      "Iteration 31, loss = 0.22436879\n",
      "Iteration 32, loss = 0.21758589\n",
      "Iteration 33, loss = 0.20703381\n",
      "Iteration 34, loss = 0.21373197\n",
      "Iteration 35, loss = 0.19892029\n",
      "Iteration 36, loss = 0.20643145\n",
      "Iteration 37, loss = 0.20342261\n",
      "Iteration 38, loss = 0.19090564\n",
      "Iteration 39, loss = 0.18070574\n",
      "Iteration 40, loss = 0.17578055\n",
      "Iteration 41, loss = 0.17117531\n",
      "Iteration 42, loss = 0.17563636\n",
      "Iteration 43, loss = 0.17991965\n",
      "Iteration 44, loss = 0.18176124\n",
      "Iteration 45, loss = 0.17467807\n",
      "Iteration 46, loss = 0.16872474\n",
      "Iteration 47, loss = 0.18544936\n",
      "Iteration 48, loss = 0.18083081\n",
      "Iteration 49, loss = 0.18139719\n",
      "Iteration 50, loss = 0.18078316\n",
      "Iteration 51, loss = 0.17337228\n",
      "Iteration 52, loss = 0.17503848\n",
      "Iteration 53, loss = 0.18602886\n",
      "Iteration 54, loss = 0.17827872\n",
      "Iteration 55, loss = 0.16430683\n",
      "Iteration 56, loss = 0.15311689\n",
      "Iteration 57, loss = 0.15000808\n",
      "Iteration 58, loss = 0.15172752\n",
      "Iteration 59, loss = 0.15364750\n",
      "Iteration 60, loss = 0.14714220\n",
      "Iteration 61, loss = 0.14424136\n",
      "Iteration 62, loss = 0.14619249\n",
      "Iteration 63, loss = 0.14613842\n",
      "Iteration 64, loss = 0.14733305\n",
      "Iteration 65, loss = 0.14637433\n",
      "Iteration 66, loss = 0.14283524\n",
      "Iteration 67, loss = 0.14795701\n",
      "Iteration 68, loss = 0.14551622\n",
      "Iteration 69, loss = 0.14390783\n",
      "Iteration 70, loss = 0.14186166\n",
      "Iteration 71, loss = 0.13959125\n",
      "Iteration 72, loss = 0.13839493\n",
      "Iteration 73, loss = 0.15064076\n",
      "Iteration 74, loss = 0.14924680\n",
      "Iteration 75, loss = 0.14775755\n",
      "Iteration 76, loss = 0.15077348\n",
      "Iteration 77, loss = 0.14616734\n",
      "Iteration 78, loss = 0.15719455\n",
      "Iteration 79, loss = 0.15525983\n",
      "Iteration 80, loss = 0.15674197\n",
      "Iteration 81, loss = 0.15682179\n",
      "Iteration 82, loss = 0.15322092\n",
      "Iteration 83, loss = 0.16471745\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.52954111\n",
      "Iteration 2, loss = 2.10898487\n",
      "Iteration 3, loss = 1.78515585\n",
      "Iteration 4, loss = 1.52108324\n",
      "Iteration 5, loss = 1.32078296\n",
      "Iteration 6, loss = 1.14597639\n",
      "Iteration 7, loss = 1.00170435\n",
      "Iteration 8, loss = 0.89288176\n",
      "Iteration 9, loss = 0.81173873\n",
      "Iteration 10, loss = 0.71762443\n",
      "Iteration 11, loss = 0.64007368\n",
      "Iteration 12, loss = 0.58508049\n",
      "Iteration 13, loss = 0.54850164\n",
      "Iteration 14, loss = 0.51389343\n",
      "Iteration 15, loss = 0.47644942\n",
      "Iteration 16, loss = 0.45677684\n",
      "Iteration 17, loss = 0.41790904\n",
      "Iteration 18, loss = 0.39795503\n",
      "Iteration 19, loss = 0.36444336\n",
      "Iteration 20, loss = 0.35479083\n",
      "Iteration 21, loss = 0.32386113\n",
      "Iteration 22, loss = 0.30605195\n",
      "Iteration 23, loss = 0.28917568\n",
      "Iteration 24, loss = 0.27806418\n",
      "Iteration 25, loss = 0.26968529\n",
      "Iteration 26, loss = 0.26733194\n",
      "Iteration 27, loss = 0.29795174\n",
      "Iteration 28, loss = 0.31555073\n",
      "Iteration 29, loss = 0.29132581\n",
      "Iteration 30, loss = 0.27042161\n",
      "Iteration 31, loss = 0.24375938\n",
      "Iteration 32, loss = 0.23285483\n",
      "Iteration 33, loss = 0.22367550\n",
      "Iteration 34, loss = 0.21974951\n",
      "Iteration 35, loss = 0.20887915\n",
      "Iteration 36, loss = 0.20201173\n",
      "Iteration 37, loss = 0.20140382\n",
      "Iteration 38, loss = 0.21438848\n",
      "Iteration 39, loss = 0.19987713\n",
      "Iteration 40, loss = 0.19295952\n",
      "Iteration 41, loss = 0.18407809\n",
      "Iteration 42, loss = 0.18820569\n",
      "Iteration 43, loss = 0.18135495\n",
      "Iteration 44, loss = 0.19096940\n",
      "Iteration 45, loss = 0.18795000\n",
      "Iteration 46, loss = 0.18548404\n",
      "Iteration 47, loss = 0.20087667\n",
      "Iteration 48, loss = 0.18867179\n",
      "Iteration 49, loss = 0.17514155\n",
      "Iteration 50, loss = 0.17839596\n",
      "Iteration 51, loss = 0.17005473\n",
      "Iteration 52, loss = 0.16824514\n",
      "Iteration 53, loss = 0.16426180\n",
      "Iteration 54, loss = 0.16608457\n",
      "Iteration 55, loss = 0.16136831\n",
      "Iteration 56, loss = 0.16121395\n",
      "Iteration 57, loss = 0.16024367\n",
      "Iteration 58, loss = 0.16374762\n",
      "Iteration 59, loss = 0.16410791\n",
      "Iteration 60, loss = 0.15942877\n",
      "Iteration 61, loss = 0.15801316\n",
      "Iteration 62, loss = 0.16553684\n",
      "Iteration 63, loss = 0.16036354\n",
      "Iteration 64, loss = 0.15923608\n",
      "Iteration 65, loss = 0.15700759\n",
      "Iteration 66, loss = 0.15464471\n",
      "Iteration 67, loss = 0.15590752\n",
      "Iteration 68, loss = 0.15218423\n",
      "Iteration 69, loss = 0.15360109\n",
      "Iteration 70, loss = 0.15558866\n",
      "Iteration 71, loss = 0.15376899\n",
      "Iteration 72, loss = 0.15319037\n",
      "Iteration 73, loss = 0.17857786\n",
      "Iteration 74, loss = 0.17085111\n",
      "Iteration 75, loss = 0.16930794\n",
      "Iteration 76, loss = 0.16510641\n",
      "Iteration 77, loss = 0.16160721\n",
      "Iteration 78, loss = 0.16080161\n",
      "Iteration 79, loss = 0.15862338\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.73088783\n",
      "Iteration 2, loss = 2.26079651\n",
      "Iteration 3, loss = 1.89623056\n",
      "Iteration 4, loss = 1.59827949\n",
      "Iteration 5, loss = 1.37147312\n",
      "Iteration 6, loss = 1.18034281\n",
      "Iteration 7, loss = 1.02206655\n",
      "Iteration 8, loss = 0.90144193\n",
      "Iteration 9, loss = 0.83001300\n",
      "Iteration 10, loss = 0.73762754\n",
      "Iteration 11, loss = 0.65546462\n",
      "Iteration 12, loss = 0.59286344\n",
      "Iteration 13, loss = 0.54545032\n",
      "Iteration 14, loss = 0.50980498\n",
      "Iteration 15, loss = 0.48337017\n",
      "Iteration 16, loss = 0.47461402\n",
      "Iteration 17, loss = 0.42687697\n",
      "Iteration 18, loss = 0.41195137\n",
      "Iteration 19, loss = 0.38526821\n",
      "Iteration 20, loss = 0.39083343\n",
      "Iteration 21, loss = 0.34768898\n",
      "Iteration 22, loss = 0.31521866\n",
      "Iteration 23, loss = 0.29832080\n",
      "Iteration 24, loss = 0.28673465\n",
      "Iteration 25, loss = 0.28125937\n",
      "Iteration 26, loss = 0.27423612\n",
      "Iteration 27, loss = 0.28197311\n",
      "Iteration 28, loss = 0.27934415\n",
      "Iteration 29, loss = 0.26696785\n",
      "Iteration 30, loss = 0.28208696\n",
      "Iteration 31, loss = 0.26267520\n",
      "Iteration 32, loss = 0.25438967\n",
      "Iteration 33, loss = 0.23479972\n",
      "Iteration 34, loss = 0.23437027\n",
      "Iteration 35, loss = 0.21596944\n",
      "Iteration 36, loss = 0.21310245\n",
      "Iteration 37, loss = 0.20514410\n",
      "Iteration 38, loss = 0.20264857\n",
      "Iteration 39, loss = 0.19892933\n",
      "Iteration 40, loss = 0.19513914\n",
      "Iteration 41, loss = 0.19042474\n",
      "Iteration 42, loss = 0.20394631\n",
      "Iteration 43, loss = 0.21123804\n",
      "Iteration 44, loss = 0.21191090\n",
      "Iteration 45, loss = 0.20157496\n",
      "Iteration 46, loss = 0.19453049\n",
      "Iteration 47, loss = 0.20270979\n",
      "Iteration 48, loss = 0.19414504\n",
      "Iteration 49, loss = 0.20672545\n",
      "Iteration 50, loss = 0.19870212\n",
      "Iteration 51, loss = 0.18607011\n",
      "Iteration 52, loss = 0.18575836\n",
      "Iteration 53, loss = 0.18313022\n",
      "Iteration 54, loss = 0.18634282\n",
      "Iteration 55, loss = 0.17963732\n",
      "Iteration 56, loss = 0.17704251\n",
      "Iteration 57, loss = 0.17364003\n",
      "Iteration 58, loss = 0.17867291\n",
      "Iteration 59, loss = 0.18115375\n",
      "Iteration 60, loss = 0.17575367\n",
      "Iteration 61, loss = 0.17015751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 62, loss = 0.17058066\n",
      "Iteration 63, loss = 0.17446581\n",
      "Iteration 64, loss = 0.17325033\n",
      "Iteration 65, loss = 0.16930986\n",
      "Iteration 66, loss = 0.16776257\n",
      "Iteration 67, loss = 0.17246313\n",
      "Iteration 68, loss = 0.16637682\n",
      "Iteration 69, loss = 0.16526183\n",
      "Iteration 70, loss = 0.16521805\n",
      "Iteration 71, loss = 0.16523098\n",
      "Iteration 72, loss = 0.16294743\n",
      "Iteration 73, loss = 0.18404240\n",
      "Iteration 74, loss = 0.18149372\n",
      "Iteration 75, loss = 0.17616271\n",
      "Iteration 76, loss = 0.17130581\n",
      "Iteration 77, loss = 0.17385722\n",
      "Iteration 78, loss = 0.17963683\n",
      "Iteration 79, loss = 0.18887443\n",
      "Iteration 80, loss = 0.18130246\n",
      "Iteration 81, loss = 0.19045232\n",
      "Iteration 82, loss = 0.19668380\n",
      "Iteration 83, loss = 0.19079008\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.93040289\n",
      "Iteration 2, loss = 2.40700331\n",
      "Iteration 3, loss = 1.99712153\n",
      "Iteration 4, loss = 1.66660684\n",
      "Iteration 5, loss = 1.41914012\n",
      "Iteration 6, loss = 1.21266822\n",
      "Iteration 7, loss = 1.04602696\n",
      "Iteration 8, loss = 0.92095636\n",
      "Iteration 9, loss = 0.83716252\n",
      "Iteration 10, loss = 0.73557899\n",
      "Iteration 11, loss = 0.66408960\n",
      "Iteration 12, loss = 0.59293556\n",
      "Iteration 13, loss = 0.55066394\n",
      "Iteration 14, loss = 0.51831755\n",
      "Iteration 15, loss = 0.49953618\n",
      "Iteration 16, loss = 0.48928589\n",
      "Iteration 17, loss = 0.43770098\n",
      "Iteration 18, loss = 0.40189714\n",
      "Iteration 19, loss = 0.37939958\n",
      "Iteration 20, loss = 0.39533939\n",
      "Iteration 21, loss = 0.35655513\n",
      "Iteration 22, loss = 0.32399712\n",
      "Iteration 23, loss = 0.30445807\n",
      "Iteration 24, loss = 0.29460537\n",
      "Iteration 25, loss = 0.28376542\n",
      "Iteration 26, loss = 0.28623573\n",
      "Iteration 27, loss = 0.29099093\n",
      "Iteration 28, loss = 0.29255301\n",
      "Iteration 29, loss = 0.28053007\n",
      "Iteration 30, loss = 0.28933716\n",
      "Iteration 31, loss = 0.26083600\n",
      "Iteration 32, loss = 0.25171664\n",
      "Iteration 33, loss = 0.24331731\n",
      "Iteration 34, loss = 0.23943637\n",
      "Iteration 35, loss = 0.22291542\n",
      "Iteration 36, loss = 0.22279666\n",
      "Iteration 37, loss = 0.21637338\n",
      "Iteration 38, loss = 0.21179553\n",
      "Iteration 39, loss = 0.20804051\n",
      "Iteration 40, loss = 0.20340978\n",
      "Iteration 41, loss = 0.20137014\n",
      "Iteration 42, loss = 0.21347183\n",
      "Iteration 43, loss = 0.21609872\n",
      "Iteration 44, loss = 0.21479056\n",
      "Iteration 45, loss = 0.20756249\n",
      "Iteration 46, loss = 0.20259541\n",
      "Iteration 47, loss = 0.21693653\n",
      "Iteration 48, loss = 0.21337409\n",
      "Iteration 49, loss = 0.23509932\n",
      "Iteration 50, loss = 0.21593077\n",
      "Iteration 51, loss = 0.20225171\n",
      "Iteration 52, loss = 0.19830507\n",
      "Iteration 53, loss = 0.19191320\n",
      "Iteration 54, loss = 0.19112475\n",
      "Iteration 55, loss = 0.18594542\n",
      "Iteration 56, loss = 0.18489334\n",
      "Iteration 57, loss = 0.18407615\n",
      "Iteration 58, loss = 0.18848048\n",
      "Iteration 59, loss = 0.19079701\n",
      "Iteration 60, loss = 0.18575402\n",
      "Iteration 61, loss = 0.18132179\n",
      "Iteration 62, loss = 0.18490559\n",
      "Iteration 63, loss = 0.19270369\n",
      "Iteration 64, loss = 0.18965009\n",
      "Iteration 65, loss = 0.18248691\n",
      "Iteration 66, loss = 0.17899673\n",
      "Iteration 67, loss = 0.17918194\n",
      "Iteration 68, loss = 0.17489057\n",
      "Iteration 69, loss = 0.17732734\n",
      "Iteration 70, loss = 0.17747001\n",
      "Iteration 71, loss = 0.17760277\n",
      "Iteration 72, loss = 0.17644361\n",
      "Iteration 73, loss = 0.19677282\n",
      "Iteration 74, loss = 0.19406925\n",
      "Iteration 75, loss = 0.18851007\n",
      "Iteration 76, loss = 0.19265739\n",
      "Iteration 77, loss = 0.19014974\n",
      "Iteration 78, loss = 0.19837555\n",
      "Iteration 79, loss = 0.20060335\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.12947090\n",
      "Iteration 2, loss = 2.55235429\n",
      "Iteration 3, loss = 2.09287445\n",
      "Iteration 4, loss = 1.72881366\n",
      "Iteration 5, loss = 1.45867588\n",
      "Iteration 6, loss = 1.24004796\n",
      "Iteration 7, loss = 1.06097614\n",
      "Iteration 8, loss = 0.93419582\n",
      "Iteration 9, loss = 0.83741131\n",
      "Iteration 10, loss = 0.74185807\n",
      "Iteration 11, loss = 0.66390532\n",
      "Iteration 12, loss = 0.59286549\n",
      "Iteration 13, loss = 0.55978757\n",
      "Iteration 14, loss = 0.52677146\n",
      "Iteration 15, loss = 0.49295544\n",
      "Iteration 16, loss = 0.47930177\n",
      "Iteration 17, loss = 0.43943577\n",
      "Iteration 18, loss = 0.42503799\n",
      "Iteration 19, loss = 0.38749701\n",
      "Iteration 20, loss = 0.38846136\n",
      "Iteration 21, loss = 0.35540343\n",
      "Iteration 22, loss = 0.32432809\n",
      "Iteration 23, loss = 0.30793902\n",
      "Iteration 24, loss = 0.29842259\n",
      "Iteration 25, loss = 0.29324005\n",
      "Iteration 26, loss = 0.29348395\n",
      "Iteration 27, loss = 0.29751647\n",
      "Iteration 28, loss = 0.29137918\n",
      "Iteration 29, loss = 0.27571827\n",
      "Iteration 30, loss = 0.28969451\n",
      "Iteration 31, loss = 0.27793797\n",
      "Iteration 32, loss = 0.26540306\n",
      "Iteration 33, loss = 0.25779372\n",
      "Iteration 34, loss = 0.24860958\n",
      "Iteration 35, loss = 0.23466430\n",
      "Iteration 36, loss = 0.24180407\n",
      "Iteration 37, loss = 0.22486082\n",
      "Iteration 38, loss = 0.22952386\n",
      "Iteration 39, loss = 0.22295573\n",
      "Iteration 40, loss = 0.21733719\n",
      "Iteration 41, loss = 0.21244444\n",
      "Iteration 42, loss = 0.22191271\n",
      "Iteration 43, loss = 0.22747005\n",
      "Iteration 44, loss = 0.24310664\n",
      "Iteration 45, loss = 0.23766138\n",
      "Iteration 46, loss = 0.22662865\n",
      "Iteration 47, loss = 0.22048704\n",
      "Iteration 48, loss = 0.22068030\n",
      "Iteration 49, loss = 0.21898849\n",
      "Iteration 50, loss = 0.20894555\n",
      "Iteration 51, loss = 0.20246672\n",
      "Iteration 52, loss = 0.20040276\n",
      "Iteration 53, loss = 0.20811786\n",
      "Iteration 54, loss = 0.22282194\n",
      "Iteration 55, loss = 0.21167490\n",
      "Iteration 56, loss = 0.20203597\n",
      "Iteration 57, loss = 0.20136511\n",
      "Iteration 58, loss = 0.21020661\n",
      "Iteration 59, loss = 0.20844444\n",
      "Iteration 60, loss = 0.20116610\n",
      "Iteration 61, loss = 0.19891390\n",
      "Iteration 62, loss = 0.19543086\n",
      "Iteration 63, loss = 0.19391646\n",
      "Iteration 64, loss = 0.19228824\n",
      "Iteration 65, loss = 0.19159152\n",
      "Iteration 66, loss = 0.19006950\n",
      "Iteration 67, loss = 0.19195781\n",
      "Iteration 68, loss = 0.18734602\n",
      "Iteration 69, loss = 0.19035262\n",
      "Iteration 70, loss = 0.19096284\n",
      "Iteration 71, loss = 0.19133363\n",
      "Iteration 72, loss = 0.18789245\n",
      "Iteration 73, loss = 0.20927404\n",
      "Iteration 74, loss = 0.20248180\n",
      "Iteration 75, loss = 0.20372660\n",
      "Iteration 76, loss = 0.20141715\n",
      "Iteration 77, loss = 0.20031312\n",
      "Iteration 78, loss = 0.20762675\n",
      "Iteration 79, loss = 0.21924444\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.32578061\n",
      "Iteration 2, loss = 2.69225763\n",
      "Iteration 3, loss = 2.18508673\n",
      "Iteration 4, loss = 1.78903249\n",
      "Iteration 5, loss = 1.49438370\n",
      "Iteration 6, loss = 1.26002868\n",
      "Iteration 7, loss = 1.07400003\n",
      "Iteration 8, loss = 0.93949643\n",
      "Iteration 9, loss = 0.85245571\n",
      "Iteration 10, loss = 0.73960444\n",
      "Iteration 11, loss = 0.66671921\n",
      "Iteration 12, loss = 0.59884585\n",
      "Iteration 13, loss = 0.55872123\n",
      "Iteration 14, loss = 0.53244782\n",
      "Iteration 15, loss = 0.49430758\n",
      "Iteration 16, loss = 0.48071123\n",
      "Iteration 17, loss = 0.44107604\n",
      "Iteration 18, loss = 0.41937802\n",
      "Iteration 19, loss = 0.39103728\n",
      "Iteration 20, loss = 0.38859698\n",
      "Iteration 21, loss = 0.35571854\n",
      "Iteration 22, loss = 0.32989528\n",
      "Iteration 23, loss = 0.31360833\n",
      "Iteration 24, loss = 0.30457432\n",
      "Iteration 25, loss = 0.29826700\n",
      "Iteration 26, loss = 0.30031594\n",
      "Iteration 27, loss = 0.31169968\n",
      "Iteration 28, loss = 0.31332998\n",
      "Iteration 29, loss = 0.29620653\n",
      "Iteration 30, loss = 0.30974408\n",
      "Iteration 31, loss = 0.28245775\n",
      "Iteration 32, loss = 0.27191680\n",
      "Iteration 33, loss = 0.26464760\n",
      "Iteration 34, loss = 0.26136219\n",
      "Iteration 35, loss = 0.24209547\n",
      "Iteration 36, loss = 0.24622346\n",
      "Iteration 37, loss = 0.23669324\n",
      "Iteration 38, loss = 0.23291922\n",
      "Iteration 39, loss = 0.22824589\n",
      "Iteration 40, loss = 0.22531572\n",
      "Iteration 41, loss = 0.22246426\n",
      "Iteration 42, loss = 0.24233661\n",
      "Iteration 43, loss = 0.24706266\n",
      "Iteration 44, loss = 0.25545027\n",
      "Iteration 45, loss = 0.24573021\n",
      "Iteration 46, loss = 0.23520648\n",
      "Iteration 47, loss = 0.23128861\n",
      "Iteration 48, loss = 0.22996271\n",
      "Iteration 49, loss = 0.22190481\n",
      "Iteration 50, loss = 0.21631971\n",
      "Iteration 51, loss = 0.20971088\n",
      "Iteration 52, loss = 0.21050646\n",
      "Iteration 53, loss = 0.21625581\n",
      "Iteration 54, loss = 0.24249301\n",
      "Iteration 55, loss = 0.22262792\n",
      "Iteration 56, loss = 0.21387331\n",
      "Iteration 57, loss = 0.20945302\n",
      "Iteration 58, loss = 0.21791593\n",
      "Iteration 59, loss = 0.22027997\n",
      "Iteration 60, loss = 0.21155461\n",
      "Iteration 61, loss = 0.20689597\n",
      "Iteration 62, loss = 0.20688687\n",
      "Iteration 63, loss = 0.20763669\n",
      "Iteration 64, loss = 0.20615436\n",
      "Iteration 65, loss = 0.20354413\n",
      "Iteration 66, loss = 0.20213192\n",
      "Iteration 67, loss = 0.20361638\n",
      "Iteration 68, loss = 0.19973036\n",
      "Iteration 69, loss = 0.20119610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70, loss = 0.20146883\n",
      "Iteration 71, loss = 0.20052821\n",
      "Iteration 72, loss = 0.19844179\n",
      "Iteration 73, loss = 0.21592756\n",
      "Iteration 74, loss = 0.21390925\n",
      "Iteration 75, loss = 0.21269752\n",
      "Iteration 76, loss = 0.21608147\n",
      "Iteration 77, loss = 0.20774336\n",
      "Iteration 78, loss = 0.21537821\n",
      "Iteration 79, loss = 0.23089747\n",
      "Iteration 80, loss = 0.22351885\n",
      "Iteration 81, loss = 0.22161843\n",
      "Iteration 82, loss = 0.22548430\n",
      "Iteration 83, loss = 0.20553905\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.52182602\n",
      "Iteration 2, loss = 2.82910963\n",
      "Iteration 3, loss = 2.27512676\n",
      "Iteration 4, loss = 1.84654629\n",
      "Iteration 5, loss = 1.53276457\n",
      "Iteration 6, loss = 1.28721229\n",
      "Iteration 7, loss = 1.09257394\n",
      "Iteration 8, loss = 0.95815629\n",
      "Iteration 9, loss = 0.85039818\n",
      "Iteration 10, loss = 0.74832429\n",
      "Iteration 11, loss = 0.67195916\n",
      "Iteration 12, loss = 0.60271089\n",
      "Iteration 13, loss = 0.56224567\n",
      "Iteration 14, loss = 0.53150594\n",
      "Iteration 15, loss = 0.50738657\n",
      "Iteration 16, loss = 0.49860330\n",
      "Iteration 17, loss = 0.44563049\n",
      "Iteration 18, loss = 0.42414005\n",
      "Iteration 19, loss = 0.39652772\n",
      "Iteration 20, loss = 0.39366262\n",
      "Iteration 21, loss = 0.36353781\n",
      "Iteration 22, loss = 0.33280110\n",
      "Iteration 23, loss = 0.32147850\n",
      "Iteration 24, loss = 0.31280875\n",
      "Iteration 25, loss = 0.30152538\n",
      "Iteration 26, loss = 0.30142059\n",
      "Iteration 27, loss = 0.31579000\n",
      "Iteration 28, loss = 0.31758957\n",
      "Iteration 29, loss = 0.30221965\n",
      "Iteration 30, loss = 0.31964064\n",
      "Iteration 31, loss = 0.29389292\n",
      "Iteration 32, loss = 0.27725805\n",
      "Iteration 33, loss = 0.26930570\n",
      "Iteration 34, loss = 0.26786417\n",
      "Iteration 35, loss = 0.25147164\n",
      "Iteration 36, loss = 0.25936593\n",
      "Iteration 37, loss = 0.24841641\n",
      "Iteration 38, loss = 0.24750455\n",
      "Iteration 39, loss = 0.24204246\n",
      "Iteration 40, loss = 0.23765203\n",
      "Iteration 41, loss = 0.23271704\n",
      "Iteration 42, loss = 0.24565702\n",
      "Iteration 43, loss = 0.25615844\n",
      "Iteration 44, loss = 0.26227549\n",
      "Iteration 45, loss = 0.25798249\n",
      "Iteration 46, loss = 0.23805411\n",
      "Iteration 47, loss = 0.23180987\n",
      "Iteration 48, loss = 0.23400475\n",
      "Iteration 49, loss = 0.23692856\n",
      "Iteration 50, loss = 0.22862750\n",
      "Iteration 51, loss = 0.22487786\n",
      "Iteration 52, loss = 0.22549894\n",
      "Iteration 53, loss = 0.24156727\n",
      "Iteration 54, loss = 0.26141295\n",
      "Iteration 55, loss = 0.23682165\n",
      "Iteration 56, loss = 0.22748524\n",
      "Iteration 57, loss = 0.22222212\n",
      "Iteration 58, loss = 0.22994397\n",
      "Iteration 59, loss = 0.23386546\n",
      "Iteration 60, loss = 0.22222997\n",
      "Iteration 61, loss = 0.21753032\n",
      "Iteration 62, loss = 0.21880635\n",
      "Iteration 63, loss = 0.21787077\n",
      "Iteration 64, loss = 0.21673203\n",
      "Iteration 65, loss = 0.21782395\n",
      "Iteration 66, loss = 0.21453061\n",
      "Iteration 67, loss = 0.21573091\n",
      "Iteration 68, loss = 0.20965011\n",
      "Iteration 69, loss = 0.21320821\n",
      "Iteration 70, loss = 0.21742329\n",
      "Iteration 71, loss = 0.21559418\n",
      "Iteration 72, loss = 0.21181484\n",
      "Iteration 73, loss = 0.23470541\n",
      "Iteration 74, loss = 0.22158158\n",
      "Iteration 75, loss = 0.22154660\n",
      "Iteration 76, loss = 0.22265704\n",
      "Iteration 77, loss = 0.21254878\n",
      "Iteration 78, loss = 0.22002112\n",
      "Iteration 79, loss = 0.22124537\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.71690298\n",
      "Iteration 2, loss = 2.96233113\n",
      "Iteration 3, loss = 2.35912705\n",
      "Iteration 4, loss = 1.89797699\n",
      "Iteration 5, loss = 1.56182644\n",
      "Iteration 6, loss = 1.30480877\n",
      "Iteration 7, loss = 1.10231738\n",
      "Iteration 8, loss = 0.95957932\n",
      "Iteration 9, loss = 0.85337086\n",
      "Iteration 10, loss = 0.75736454\n",
      "Iteration 11, loss = 0.67160308\n",
      "Iteration 12, loss = 0.60055618\n",
      "Iteration 13, loss = 0.56428948\n",
      "Iteration 14, loss = 0.53874429\n",
      "Iteration 15, loss = 0.51060183\n",
      "Iteration 16, loss = 0.50255450\n",
      "Iteration 17, loss = 0.44798881\n",
      "Iteration 18, loss = 0.42579482\n",
      "Iteration 19, loss = 0.39785215\n",
      "Iteration 20, loss = 0.40316678\n",
      "Iteration 21, loss = 0.37117006\n",
      "Iteration 22, loss = 0.33995242\n",
      "Iteration 23, loss = 0.32577910\n",
      "Iteration 24, loss = 0.31631292\n",
      "Iteration 25, loss = 0.31395132\n",
      "Iteration 26, loss = 0.31611411\n",
      "Iteration 27, loss = 0.32883022\n",
      "Iteration 28, loss = 0.33125809\n",
      "Iteration 29, loss = 0.30766465\n",
      "Iteration 30, loss = 0.31080135\n",
      "Iteration 31, loss = 0.29253513\n",
      "Iteration 32, loss = 0.28683597\n",
      "Iteration 33, loss = 0.28164289\n",
      "Iteration 34, loss = 0.27757827\n",
      "Iteration 35, loss = 0.26117610\n",
      "Iteration 36, loss = 0.27121934\n",
      "Iteration 37, loss = 0.26389851\n",
      "Iteration 38, loss = 0.26372912\n",
      "Iteration 39, loss = 0.25886344\n",
      "Iteration 40, loss = 0.25148770\n",
      "Iteration 41, loss = 0.24513825\n",
      "Iteration 42, loss = 0.25487270\n",
      "Iteration 43, loss = 0.25258597\n",
      "Iteration 44, loss = 0.27061992\n",
      "Iteration 45, loss = 0.26107978\n",
      "Iteration 46, loss = 0.24741079\n",
      "Iteration 47, loss = 0.24367367\n",
      "Iteration 48, loss = 0.24562868\n",
      "Iteration 49, loss = 0.24597685\n",
      "Iteration 50, loss = 0.23921517\n",
      "Iteration 51, loss = 0.23556361\n",
      "Iteration 52, loss = 0.23508052\n",
      "Iteration 53, loss = 0.24302943\n",
      "Iteration 54, loss = 0.25799311\n",
      "Iteration 55, loss = 0.24000434\n",
      "Iteration 56, loss = 0.23385124\n",
      "Iteration 57, loss = 0.23238266\n",
      "Iteration 58, loss = 0.23989472\n",
      "Iteration 59, loss = 0.24777243\n",
      "Iteration 60, loss = 0.23594540\n",
      "Iteration 61, loss = 0.22960259\n",
      "Iteration 62, loss = 0.22816480\n",
      "Iteration 63, loss = 0.23200006\n",
      "Iteration 64, loss = 0.22976848\n",
      "Iteration 65, loss = 0.22788150\n",
      "Iteration 66, loss = 0.22824263\n",
      "Iteration 67, loss = 0.22865094\n",
      "Iteration 68, loss = 0.22274132\n",
      "Iteration 69, loss = 0.22538133\n",
      "Iteration 70, loss = 0.22800931\n",
      "Iteration 71, loss = 0.23050267\n",
      "Iteration 72, loss = 0.22709425\n",
      "Iteration 73, loss = 0.24780439\n",
      "Iteration 74, loss = 0.23522685\n",
      "Iteration 75, loss = 0.23648164\n",
      "Iteration 76, loss = 0.23740214\n",
      "Iteration 77, loss = 0.22879060\n",
      "Iteration 78, loss = 0.23490086\n",
      "Iteration 79, loss = 0.23567000\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.91001471\n",
      "Iteration 2, loss = 3.08934155\n",
      "Iteration 3, loss = 2.43928180\n",
      "Iteration 4, loss = 1.94484638\n",
      "Iteration 5, loss = 1.59053410\n",
      "Iteration 6, loss = 1.32063534\n",
      "Iteration 7, loss = 1.11242127\n",
      "Iteration 8, loss = 0.97198929\n",
      "Iteration 9, loss = 0.85513420\n",
      "Iteration 10, loss = 0.74991871\n",
      "Iteration 11, loss = 0.67523971\n",
      "Iteration 12, loss = 0.60944235\n",
      "Iteration 13, loss = 0.56768045\n",
      "Iteration 14, loss = 0.54211279\n",
      "Iteration 15, loss = 0.50308240\n",
      "Iteration 16, loss = 0.49471155\n",
      "Iteration 17, loss = 0.45016595\n",
      "Iteration 18, loss = 0.42897292\n",
      "Iteration 19, loss = 0.40549396\n",
      "Iteration 20, loss = 0.40807364\n",
      "Iteration 21, loss = 0.37527516\n",
      "Iteration 22, loss = 0.34830050\n",
      "Iteration 23, loss = 0.33098584\n",
      "Iteration 24, loss = 0.32237794\n",
      "Iteration 25, loss = 0.32498172\n",
      "Iteration 26, loss = 0.31790516\n",
      "Iteration 27, loss = 0.33765071\n",
      "Iteration 28, loss = 0.34207891\n",
      "Iteration 29, loss = 0.32363081\n",
      "Iteration 30, loss = 0.33388829\n",
      "Iteration 31, loss = 0.31058516\n",
      "Iteration 32, loss = 0.29959177\n",
      "Iteration 33, loss = 0.29248461\n",
      "Iteration 34, loss = 0.28928844\n",
      "Iteration 35, loss = 0.27014754\n",
      "Iteration 36, loss = 0.27463082\n",
      "Iteration 37, loss = 0.26779620\n",
      "Iteration 38, loss = 0.26666474\n",
      "Iteration 39, loss = 0.26498835\n",
      "Iteration 40, loss = 0.26086795\n",
      "Iteration 41, loss = 0.25431820\n",
      "Iteration 42, loss = 0.26474429\n",
      "Iteration 43, loss = 0.26276577\n",
      "Iteration 44, loss = 0.28541828\n",
      "Iteration 45, loss = 0.27462275\n",
      "Iteration 46, loss = 0.25632367\n",
      "Iteration 47, loss = 0.25572915\n",
      "Iteration 48, loss = 0.25535605\n",
      "Iteration 49, loss = 0.25726677\n",
      "Iteration 50, loss = 0.25202220\n",
      "Iteration 51, loss = 0.24511591\n",
      "Iteration 52, loss = 0.24691893\n",
      "Iteration 53, loss = 0.25999465\n",
      "Iteration 54, loss = 0.27606240\n",
      "Iteration 55, loss = 0.25002802\n",
      "Iteration 56, loss = 0.24805146\n",
      "Iteration 57, loss = 0.24400974\n",
      "Iteration 58, loss = 0.25327083\n",
      "Iteration 59, loss = 0.25623499\n",
      "Iteration 60, loss = 0.24548985\n",
      "Iteration 61, loss = 0.24322793\n",
      "Iteration 62, loss = 0.24277308\n",
      "Iteration 63, loss = 0.23964906\n",
      "Iteration 64, loss = 0.24025941\n",
      "Iteration 65, loss = 0.24015944\n",
      "Iteration 66, loss = 0.23561937\n",
      "Iteration 67, loss = 0.23867914\n",
      "Iteration 68, loss = 0.23340286\n",
      "Iteration 69, loss = 0.23624468\n",
      "Iteration 70, loss = 0.23921341\n",
      "Iteration 71, loss = 0.23544900\n",
      "Iteration 72, loss = 0.23601093\n",
      "Iteration 73, loss = 0.25896130\n",
      "Iteration 74, loss = 0.24385275\n",
      "Iteration 75, loss = 0.24369691\n",
      "Iteration 76, loss = 0.24326570\n",
      "Iteration 77, loss = 0.24105778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 78, loss = 0.24457301\n",
      "Iteration 79, loss = 0.24803476\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.10228016\n",
      "Iteration 2, loss = 3.21608928\n",
      "Iteration 3, loss = 2.51338257\n",
      "Iteration 4, loss = 1.98573863\n",
      "Iteration 5, loss = 1.61049158\n",
      "Iteration 6, loss = 1.32860546\n",
      "Iteration 7, loss = 1.11360804\n",
      "Iteration 8, loss = 0.96742201\n",
      "Iteration 9, loss = 0.86330464\n",
      "Iteration 10, loss = 0.74517211\n",
      "Iteration 11, loss = 0.67524280\n",
      "Iteration 12, loss = 0.60206031\n",
      "Iteration 13, loss = 0.56514217\n",
      "Iteration 14, loss = 0.54020719\n",
      "Iteration 15, loss = 0.50537373\n",
      "Iteration 16, loss = 0.49335072\n",
      "Iteration 17, loss = 0.44747514\n",
      "Iteration 18, loss = 0.42350273\n",
      "Iteration 19, loss = 0.41715851\n",
      "Iteration 20, loss = 0.42015901\n",
      "Iteration 21, loss = 0.38133491\n",
      "Iteration 22, loss = 0.35396736\n",
      "Iteration 23, loss = 0.33830664\n",
      "Iteration 24, loss = 0.33048185\n",
      "Iteration 25, loss = 0.33213297\n",
      "Iteration 26, loss = 0.32205723\n",
      "Iteration 27, loss = 0.33565463\n",
      "Iteration 28, loss = 0.33301863\n",
      "Iteration 29, loss = 0.32321983\n",
      "Iteration 30, loss = 0.34079762\n",
      "Iteration 31, loss = 0.31706121\n",
      "Iteration 32, loss = 0.30715107\n",
      "Iteration 33, loss = 0.30361231\n",
      "Iteration 34, loss = 0.29535261\n",
      "Iteration 35, loss = 0.27742707\n",
      "Iteration 36, loss = 0.28490960\n",
      "Iteration 37, loss = 0.27434073\n",
      "Iteration 38, loss = 0.27846771\n",
      "Iteration 39, loss = 0.27725464\n",
      "Iteration 40, loss = 0.27119557\n",
      "Iteration 41, loss = 0.26559600\n",
      "Iteration 42, loss = 0.27723467\n",
      "Iteration 43, loss = 0.27147277\n",
      "Iteration 44, loss = 0.29124043\n",
      "Iteration 45, loss = 0.28453286\n",
      "Iteration 46, loss = 0.26760669\n",
      "Iteration 47, loss = 0.26342887\n",
      "Iteration 48, loss = 0.26608321\n",
      "Iteration 49, loss = 0.26665633\n",
      "Iteration 50, loss = 0.26178838\n",
      "Iteration 51, loss = 0.25363017\n",
      "Iteration 52, loss = 0.25484987\n",
      "Iteration 53, loss = 0.26657965\n",
      "Iteration 54, loss = 0.28444413\n",
      "Iteration 55, loss = 0.26285211\n",
      "Iteration 56, loss = 0.25624695\n",
      "Iteration 57, loss = 0.25208904\n",
      "Iteration 58, loss = 0.26285183\n",
      "Iteration 59, loss = 0.26854934\n",
      "Iteration 60, loss = 0.26015751\n",
      "Iteration 61, loss = 0.25521713\n",
      "Iteration 62, loss = 0.25454328\n",
      "Iteration 63, loss = 0.25229190\n",
      "Iteration 64, loss = 0.24768341\n",
      "Iteration 65, loss = 0.24836002\n",
      "Iteration 66, loss = 0.24612073\n",
      "Iteration 67, loss = 0.24827094\n",
      "Iteration 68, loss = 0.24351423\n",
      "Iteration 69, loss = 0.24867964\n",
      "Iteration 70, loss = 0.25001609\n",
      "Iteration 71, loss = 0.24639323\n",
      "Iteration 72, loss = 0.24898922\n",
      "Iteration 73, loss = 0.27026017\n",
      "Iteration 74, loss = 0.25527059\n",
      "Iteration 75, loss = 0.25743769\n",
      "Iteration 76, loss = 0.25897376\n",
      "Iteration 77, loss = 0.25290309\n",
      "Iteration 78, loss = 0.25335236\n",
      "Iteration 79, loss = 0.25282600\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.29382285\n",
      "Iteration 2, loss = 3.33919900\n",
      "Iteration 3, loss = 2.58593008\n",
      "Iteration 4, loss = 2.02695360\n",
      "Iteration 5, loss = 1.63389980\n",
      "Iteration 6, loss = 1.34236467\n",
      "Iteration 7, loss = 1.12124883\n",
      "Iteration 8, loss = 0.97467974\n",
      "Iteration 9, loss = 0.85402366\n",
      "Iteration 10, loss = 0.74797432\n",
      "Iteration 11, loss = 0.67098938\n",
      "Iteration 12, loss = 0.60592277\n",
      "Iteration 13, loss = 0.56284385\n",
      "Iteration 14, loss = 0.54458319\n",
      "Iteration 15, loss = 0.51167358\n",
      "Iteration 16, loss = 0.50682107\n",
      "Iteration 17, loss = 0.45426724\n",
      "Iteration 18, loss = 0.43434462\n",
      "Iteration 19, loss = 0.40854999\n",
      "Iteration 20, loss = 0.41185289\n",
      "Iteration 21, loss = 0.38705615\n",
      "Iteration 22, loss = 0.35950709\n",
      "Iteration 23, loss = 0.34800715\n",
      "Iteration 24, loss = 0.33817911\n",
      "Iteration 25, loss = 0.34226769\n",
      "Iteration 26, loss = 0.32712572\n",
      "Iteration 27, loss = 0.34232816\n",
      "Iteration 28, loss = 0.33500319\n",
      "Iteration 29, loss = 0.32578437\n",
      "Iteration 30, loss = 0.34614139\n",
      "Iteration 31, loss = 0.32435564\n",
      "Iteration 32, loss = 0.30900678\n",
      "Iteration 33, loss = 0.30667840\n",
      "Iteration 34, loss = 0.30502703\n",
      "Iteration 35, loss = 0.29071092\n",
      "Iteration 36, loss = 0.30405590\n",
      "Iteration 37, loss = 0.28729866\n",
      "Iteration 38, loss = 0.29059060\n",
      "Iteration 39, loss = 0.28432999\n",
      "Iteration 40, loss = 0.28015128\n",
      "Iteration 41, loss = 0.27549302\n",
      "Iteration 42, loss = 0.28650279\n",
      "Iteration 43, loss = 0.28804534\n",
      "Iteration 44, loss = 0.30374134\n",
      "Iteration 45, loss = 0.29280008\n",
      "Iteration 46, loss = 0.27504440\n",
      "Iteration 47, loss = 0.27316235\n",
      "Iteration 48, loss = 0.27677843\n",
      "Iteration 49, loss = 0.27693001\n",
      "Iteration 50, loss = 0.27351754\n",
      "Iteration 51, loss = 0.26764265\n",
      "Iteration 52, loss = 0.27079285\n",
      "Iteration 53, loss = 0.28203225\n",
      "Iteration 54, loss = 0.29734359\n",
      "Iteration 55, loss = 0.27029607\n",
      "Iteration 56, loss = 0.26547639\n",
      "Iteration 57, loss = 0.26153636\n",
      "Iteration 58, loss = 0.27107531\n",
      "Iteration 59, loss = 0.27563091\n",
      "Iteration 60, loss = 0.26541649\n",
      "Iteration 61, loss = 0.26452730\n",
      "Iteration 62, loss = 0.26582623\n",
      "Iteration 63, loss = 0.26447784\n",
      "Iteration 64, loss = 0.26147894\n",
      "Iteration 65, loss = 0.26265394\n",
      "Iteration 66, loss = 0.25781758\n",
      "Iteration 67, loss = 0.25790598\n",
      "Iteration 68, loss = 0.25315243\n",
      "Iteration 69, loss = 0.26143768\n",
      "Iteration 70, loss = 0.26972975\n",
      "Iteration 71, loss = 0.26097881\n",
      "Iteration 72, loss = 0.26125271\n",
      "Iteration 73, loss = 0.28411338\n",
      "Iteration 74, loss = 0.26886543\n",
      "Iteration 75, loss = 0.27224017\n",
      "Iteration 76, loss = 0.27439654\n",
      "Iteration 77, loss = 0.26848381\n",
      "Iteration 78, loss = 0.26552377\n",
      "Iteration 79, loss = 0.26748901\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.48304553\n",
      "Iteration 2, loss = 3.45746561\n",
      "Iteration 3, loss = 2.65332059\n",
      "Iteration 4, loss = 2.06240171\n",
      "Iteration 5, loss = 1.64906661\n",
      "Iteration 6, loss = 1.34620855\n",
      "Iteration 7, loss = 1.11974269\n",
      "Iteration 8, loss = 0.96737729\n",
      "Iteration 9, loss = 0.86850978\n",
      "Iteration 10, loss = 0.74614243\n",
      "Iteration 11, loss = 0.67635871\n",
      "Iteration 12, loss = 0.60575680\n",
      "Iteration 13, loss = 0.57104754\n",
      "Iteration 14, loss = 0.55147621\n",
      "Iteration 15, loss = 0.50848747\n",
      "Iteration 16, loss = 0.50155883\n",
      "Iteration 17, loss = 0.45751099\n",
      "Iteration 18, loss = 0.43751643\n",
      "Iteration 19, loss = 0.41605806\n",
      "Iteration 20, loss = 0.41828106\n",
      "Iteration 21, loss = 0.39261984\n",
      "Iteration 22, loss = 0.36689259\n",
      "Iteration 23, loss = 0.35562223\n",
      "Iteration 24, loss = 0.34313091\n",
      "Iteration 25, loss = 0.35165918\n",
      "Iteration 26, loss = 0.33600228\n",
      "Iteration 27, loss = 0.35380763\n",
      "Iteration 28, loss = 0.34463835\n",
      "Iteration 29, loss = 0.33528712\n",
      "Iteration 30, loss = 0.34643311\n",
      "Iteration 31, loss = 0.32553539\n",
      "Iteration 32, loss = 0.32076285\n",
      "Iteration 33, loss = 0.32318762\n",
      "Iteration 34, loss = 0.31523660\n",
      "Iteration 35, loss = 0.30275733\n",
      "Iteration 36, loss = 0.31834548\n",
      "Iteration 37, loss = 0.30271172\n",
      "Iteration 38, loss = 0.30342214\n",
      "Iteration 39, loss = 0.29512710\n",
      "Iteration 40, loss = 0.29581638\n",
      "Iteration 41, loss = 0.28637139\n",
      "Iteration 42, loss = 0.29403687\n",
      "Iteration 43, loss = 0.29438170\n",
      "Iteration 44, loss = 0.30965621\n",
      "Iteration 45, loss = 0.30128189\n",
      "Iteration 46, loss = 0.28534423\n",
      "Iteration 47, loss = 0.28418629\n",
      "Iteration 48, loss = 0.28897913\n",
      "Iteration 49, loss = 0.29357538\n",
      "Iteration 50, loss = 0.28366240\n",
      "Iteration 51, loss = 0.27807634\n",
      "Iteration 52, loss = 0.28151123\n",
      "Iteration 53, loss = 0.29432143\n",
      "Iteration 54, loss = 0.30691395\n",
      "Iteration 55, loss = 0.28354914\n",
      "Iteration 56, loss = 0.27773145\n",
      "Iteration 57, loss = 0.27380448\n",
      "Iteration 58, loss = 0.28562326\n",
      "Iteration 59, loss = 0.28892356\n",
      "Iteration 60, loss = 0.27786024\n",
      "Iteration 61, loss = 0.27483170\n",
      "Iteration 62, loss = 0.27611521\n",
      "Iteration 63, loss = 0.27410654\n",
      "Iteration 64, loss = 0.27296642\n",
      "Iteration 65, loss = 0.27138816\n",
      "Iteration 66, loss = 0.26770789\n",
      "Iteration 67, loss = 0.26988861\n",
      "Iteration 68, loss = 0.26378053\n",
      "Iteration 69, loss = 0.27043476\n",
      "Iteration 70, loss = 0.27710173\n",
      "Iteration 71, loss = 0.26813151\n",
      "Iteration 72, loss = 0.27100747\n",
      "Iteration 73, loss = 0.29523207\n",
      "Iteration 74, loss = 0.27900680\n",
      "Iteration 75, loss = 0.28403449\n",
      "Iteration 76, loss = 0.28367186\n",
      "Iteration 77, loss = 0.27815691\n",
      "Iteration 78, loss = 0.27544627\n",
      "Iteration 79, loss = 0.27349723\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.67237518\n",
      "Iteration 2, loss = 3.57722991\n",
      "Iteration 3, loss = 2.72641104\n",
      "Iteration 4, loss = 2.10504223\n",
      "Iteration 5, loss = 1.67547481\n",
      "Iteration 6, loss = 1.36515551\n",
      "Iteration 7, loss = 1.13355241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.97981794\n",
      "Iteration 9, loss = 0.85608082\n",
      "Iteration 10, loss = 0.75034497\n",
      "Iteration 11, loss = 0.67630598\n",
      "Iteration 12, loss = 0.60689935\n",
      "Iteration 13, loss = 0.56783611\n",
      "Iteration 14, loss = 0.55005968\n",
      "Iteration 15, loss = 0.51287612\n",
      "Iteration 16, loss = 0.50883883\n",
      "Iteration 17, loss = 0.46057242\n",
      "Iteration 18, loss = 0.44346774\n",
      "Iteration 19, loss = 0.41804885\n",
      "Iteration 20, loss = 0.42646818\n",
      "Iteration 21, loss = 0.40133999\n",
      "Iteration 22, loss = 0.37528637\n",
      "Iteration 23, loss = 0.36282409\n",
      "Iteration 24, loss = 0.35224568\n",
      "Iteration 25, loss = 0.35914094\n",
      "Iteration 26, loss = 0.34421360\n",
      "Iteration 27, loss = 0.35984136\n",
      "Iteration 28, loss = 0.35095045\n",
      "Iteration 29, loss = 0.34597223\n",
      "Iteration 30, loss = 0.36005355\n",
      "Iteration 31, loss = 0.34046870\n",
      "Iteration 32, loss = 0.33705397\n",
      "Iteration 33, loss = 0.33298952\n",
      "Iteration 34, loss = 0.32713096\n",
      "Iteration 35, loss = 0.30845450\n",
      "Iteration 36, loss = 0.31695130\n",
      "Iteration 37, loss = 0.30893018\n",
      "Iteration 38, loss = 0.30848519\n",
      "Iteration 39, loss = 0.30227623\n",
      "Iteration 40, loss = 0.30552653\n",
      "Iteration 41, loss = 0.29381933\n",
      "Iteration 42, loss = 0.30532490\n",
      "Iteration 43, loss = 0.30475424\n",
      "Iteration 44, loss = 0.32065645\n",
      "Iteration 45, loss = 0.30858479\n",
      "Iteration 46, loss = 0.29504788\n",
      "Iteration 47, loss = 0.29455682\n",
      "Iteration 48, loss = 0.29865897\n",
      "Iteration 49, loss = 0.29897618\n",
      "Iteration 50, loss = 0.29286237\n",
      "Iteration 51, loss = 0.28510986\n",
      "Iteration 52, loss = 0.28800862\n",
      "Iteration 53, loss = 0.30381285\n",
      "Iteration 54, loss = 0.31549100\n",
      "Iteration 55, loss = 0.29200509\n",
      "Iteration 56, loss = 0.28649028\n",
      "Iteration 57, loss = 0.28267162\n",
      "Iteration 58, loss = 0.29386316\n",
      "Iteration 59, loss = 0.30256685\n",
      "Iteration 60, loss = 0.28866209\n",
      "Iteration 61, loss = 0.28155608\n",
      "Iteration 62, loss = 0.28746277\n",
      "Iteration 63, loss = 0.28197122\n",
      "Iteration 64, loss = 0.27833532\n",
      "Iteration 65, loss = 0.28519014\n",
      "Iteration 66, loss = 0.28094704\n",
      "Iteration 67, loss = 0.27834847\n",
      "Iteration 68, loss = 0.27552765\n",
      "Iteration 69, loss = 0.28268294\n",
      "Iteration 70, loss = 0.28982753\n",
      "Iteration 71, loss = 0.28339763\n",
      "Iteration 72, loss = 0.27990203\n",
      "Iteration 73, loss = 0.31115311\n",
      "Iteration 74, loss = 0.29500398\n",
      "Iteration 75, loss = 0.28860216\n",
      "Iteration 76, loss = 0.29000381\n",
      "Iteration 77, loss = 0.29126155\n",
      "Iteration 78, loss = 0.28857579\n",
      "Iteration 79, loss = 0.28405350\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.86047370\n",
      "Iteration 2, loss = 3.69166539\n",
      "Iteration 3, loss = 2.78812267\n",
      "Iteration 4, loss = 2.13681245\n",
      "Iteration 5, loss = 1.68917637\n",
      "Iteration 6, loss = 1.36741869\n",
      "Iteration 7, loss = 1.13192617\n",
      "Iteration 8, loss = 0.97379804\n",
      "Iteration 9, loss = 0.85970884\n",
      "Iteration 10, loss = 0.74898045\n",
      "Iteration 11, loss = 0.67546529\n",
      "Iteration 12, loss = 0.60754090\n",
      "Iteration 13, loss = 0.56591050\n",
      "Iteration 14, loss = 0.55232348\n",
      "Iteration 15, loss = 0.51377161\n",
      "Iteration 16, loss = 0.51212068\n",
      "Iteration 17, loss = 0.46290633\n",
      "Iteration 18, loss = 0.44595795\n",
      "Iteration 19, loss = 0.42294795\n",
      "Iteration 20, loss = 0.42989390\n",
      "Iteration 21, loss = 0.40498611\n",
      "Iteration 22, loss = 0.38207989\n",
      "Iteration 23, loss = 0.36852363\n",
      "Iteration 24, loss = 0.35764381\n",
      "Iteration 25, loss = 0.36997969\n",
      "Iteration 26, loss = 0.35494124\n",
      "Iteration 27, loss = 0.36940953\n",
      "Iteration 28, loss = 0.35198160\n",
      "Iteration 29, loss = 0.35066836\n",
      "Iteration 30, loss = 0.35833338\n",
      "Iteration 31, loss = 0.34177316\n",
      "Iteration 32, loss = 0.34103731\n",
      "Iteration 33, loss = 0.34583469\n",
      "Iteration 34, loss = 0.34186726\n",
      "Iteration 35, loss = 0.32171319\n",
      "Iteration 36, loss = 0.33454818\n",
      "Iteration 37, loss = 0.33182903\n",
      "Iteration 38, loss = 0.32128968\n",
      "Iteration 39, loss = 0.31716662\n",
      "Iteration 40, loss = 0.32261911\n",
      "Iteration 41, loss = 0.30535112\n",
      "Iteration 42, loss = 0.30688155\n",
      "Iteration 43, loss = 0.31470029\n",
      "Iteration 44, loss = 0.32011779\n",
      "Iteration 45, loss = 0.31191822\n",
      "Iteration 46, loss = 0.30610434\n",
      "Iteration 47, loss = 0.30342283\n",
      "Iteration 48, loss = 0.31211289\n",
      "Iteration 49, loss = 0.31359694\n",
      "Iteration 50, loss = 0.30774205\n",
      "Iteration 51, loss = 0.29641651\n",
      "Iteration 52, loss = 0.29892673\n",
      "Iteration 53, loss = 0.31164953\n",
      "Iteration 54, loss = 0.32052330\n",
      "Iteration 55, loss = 0.30176621\n",
      "Iteration 56, loss = 0.29685336\n",
      "Iteration 57, loss = 0.29187694\n",
      "Iteration 58, loss = 0.30500042\n",
      "Iteration 59, loss = 0.31471743\n",
      "Iteration 60, loss = 0.29990599\n",
      "Iteration 61, loss = 0.29283358\n",
      "Iteration 62, loss = 0.29485739\n",
      "Iteration 63, loss = 0.29142063\n",
      "Iteration 64, loss = 0.28968970\n",
      "Iteration 65, loss = 0.29269737\n",
      "Iteration 66, loss = 0.28737102\n",
      "Iteration 67, loss = 0.28778013\n",
      "Iteration 68, loss = 0.28398145\n",
      "Iteration 69, loss = 0.29294939\n",
      "Iteration 70, loss = 0.30158404\n",
      "Iteration 71, loss = 0.29362374\n",
      "Iteration 72, loss = 0.28879531\n",
      "Iteration 73, loss = 0.31817187\n",
      "Iteration 74, loss = 0.30749061\n",
      "Iteration 75, loss = 0.29808042\n",
      "Iteration 76, loss = 0.29934980\n",
      "Iteration 77, loss = 0.29849843\n",
      "Iteration 78, loss = 0.29668254\n",
      "Iteration 79, loss = 0.29230800\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.04721139\n",
      "Iteration 2, loss = 3.80699352\n",
      "Iteration 3, loss = 2.85481798\n",
      "Iteration 4, loss = 2.17480005\n",
      "Iteration 5, loss = 1.70889646\n",
      "Iteration 6, loss = 1.37919768\n",
      "Iteration 7, loss = 1.13812537\n",
      "Iteration 8, loss = 0.97656838\n",
      "Iteration 9, loss = 0.86353716\n",
      "Iteration 10, loss = 0.74579670\n",
      "Iteration 11, loss = 0.67711019\n",
      "Iteration 12, loss = 0.60959653\n",
      "Iteration 13, loss = 0.57270819\n",
      "Iteration 14, loss = 0.56121334\n",
      "Iteration 15, loss = 0.51331787\n",
      "Iteration 16, loss = 0.50663653\n",
      "Iteration 17, loss = 0.46584949\n",
      "Iteration 18, loss = 0.44858331\n",
      "Iteration 19, loss = 0.43172023\n",
      "Iteration 20, loss = 0.43226842\n",
      "Iteration 21, loss = 0.40530350\n",
      "Iteration 22, loss = 0.38575046\n",
      "Iteration 23, loss = 0.37472223\n",
      "Iteration 24, loss = 0.36481720\n",
      "Iteration 25, loss = 0.38194324\n",
      "Iteration 26, loss = 0.36620328\n",
      "Iteration 27, loss = 0.37668037\n",
      "Iteration 28, loss = 0.35907823\n",
      "Iteration 29, loss = 0.35904567\n",
      "Iteration 30, loss = 0.37356611\n",
      "Iteration 31, loss = 0.35439589\n",
      "Iteration 32, loss = 0.35089149\n",
      "Iteration 33, loss = 0.35548778\n",
      "Iteration 34, loss = 0.34998144\n",
      "Iteration 35, loss = 0.33010429\n",
      "Iteration 36, loss = 0.33976393\n",
      "Iteration 37, loss = 0.33674088\n",
      "Iteration 38, loss = 0.32908802\n",
      "Iteration 39, loss = 0.32910028\n",
      "Iteration 40, loss = 0.33781799\n",
      "Iteration 41, loss = 0.31550213\n",
      "Iteration 42, loss = 0.31808494\n",
      "Iteration 43, loss = 0.32539104\n",
      "Iteration 44, loss = 0.32833964\n",
      "Iteration 45, loss = 0.31949772\n",
      "Iteration 46, loss = 0.31180516\n",
      "Iteration 47, loss = 0.31135903\n",
      "Iteration 48, loss = 0.32163582\n",
      "Iteration 49, loss = 0.32497028\n",
      "Iteration 50, loss = 0.31468744\n",
      "Iteration 51, loss = 0.30691368\n",
      "Iteration 52, loss = 0.31490712\n",
      "Iteration 53, loss = 0.32148751\n",
      "Iteration 54, loss = 0.32839811\n",
      "Iteration 55, loss = 0.31251508\n",
      "Iteration 56, loss = 0.30661371\n",
      "Iteration 57, loss = 0.30165052\n",
      "Iteration 58, loss = 0.31395416\n",
      "Iteration 59, loss = 0.32548942\n",
      "Iteration 60, loss = 0.30820955\n",
      "Iteration 61, loss = 0.30229509\n",
      "Iteration 62, loss = 0.30755065\n",
      "Iteration 63, loss = 0.30647146\n",
      "Iteration 64, loss = 0.30085042\n",
      "Iteration 65, loss = 0.30544071\n",
      "Iteration 66, loss = 0.29738519\n",
      "Iteration 67, loss = 0.30111811\n",
      "Iteration 68, loss = 0.29546101\n",
      "Iteration 69, loss = 0.29954761\n",
      "Iteration 70, loss = 0.30484929\n",
      "Iteration 71, loss = 0.29974398\n",
      "Iteration 72, loss = 0.30270743\n",
      "Iteration 73, loss = 0.34001617\n",
      "Iteration 74, loss = 0.32670219\n",
      "Iteration 75, loss = 0.30834173\n",
      "Iteration 76, loss = 0.30552949\n",
      "Iteration 77, loss = 0.30604908\n",
      "Iteration 78, loss = 0.30616082\n",
      "Iteration 79, loss = 0.30231346\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.23444855\n",
      "Iteration 2, loss = 3.91907350\n",
      "Iteration 3, loss = 2.91570828\n",
      "Iteration 4, loss = 2.20522939\n",
      "Iteration 5, loss = 1.72381345\n",
      "Iteration 6, loss = 1.38495944\n",
      "Iteration 7, loss = 1.13816958\n",
      "Iteration 8, loss = 0.97418774\n",
      "Iteration 9, loss = 0.86343503\n",
      "Iteration 10, loss = 0.74415864\n",
      "Iteration 11, loss = 0.67737616\n",
      "Iteration 12, loss = 0.60843666\n",
      "Iteration 13, loss = 0.57146900\n",
      "Iteration 14, loss = 0.56257626\n",
      "Iteration 15, loss = 0.50893369\n",
      "Iteration 16, loss = 0.50593253\n",
      "Iteration 17, loss = 0.47252142\n",
      "Iteration 18, loss = 0.45621915\n",
      "Iteration 19, loss = 0.43634360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.43453061\n",
      "Iteration 21, loss = 0.40917587\n",
      "Iteration 22, loss = 0.39270709\n",
      "Iteration 23, loss = 0.38269812\n",
      "Iteration 24, loss = 0.37081834\n",
      "Iteration 25, loss = 0.39009062\n",
      "Iteration 26, loss = 0.37275897\n",
      "Iteration 27, loss = 0.37788572\n",
      "Iteration 28, loss = 0.36345395\n",
      "Iteration 29, loss = 0.36463422\n",
      "Iteration 30, loss = 0.37605356\n",
      "Iteration 31, loss = 0.36279490\n",
      "Iteration 32, loss = 0.36315412\n",
      "Iteration 33, loss = 0.36486418\n",
      "Iteration 34, loss = 0.36140039\n",
      "Iteration 35, loss = 0.34188079\n",
      "Iteration 36, loss = 0.35001837\n",
      "Iteration 37, loss = 0.35361125\n",
      "Iteration 38, loss = 0.34551797\n",
      "Iteration 39, loss = 0.34010585\n",
      "Iteration 40, loss = 0.34923309\n",
      "Iteration 41, loss = 0.33045424\n",
      "Iteration 42, loss = 0.33510930\n",
      "Iteration 43, loss = 0.33559654\n",
      "Iteration 44, loss = 0.33370370\n",
      "Iteration 45, loss = 0.33388823\n",
      "Iteration 46, loss = 0.32436631\n",
      "Iteration 47, loss = 0.31944046\n",
      "Iteration 48, loss = 0.32558510\n",
      "Iteration 49, loss = 0.33584999\n",
      "Iteration 50, loss = 0.32458463\n",
      "Iteration 51, loss = 0.31505699\n",
      "Iteration 52, loss = 0.32172626\n",
      "Iteration 53, loss = 0.33070956\n",
      "Iteration 54, loss = 0.33535923\n",
      "Iteration 55, loss = 0.32455937\n",
      "Iteration 56, loss = 0.31914652\n",
      "Iteration 57, loss = 0.31196331\n",
      "Iteration 58, loss = 0.32469086\n",
      "Iteration 59, loss = 0.33282530\n",
      "Iteration 60, loss = 0.31611419\n",
      "Iteration 61, loss = 0.31114730\n",
      "Iteration 62, loss = 0.31784850\n",
      "Iteration 63, loss = 0.31203130\n",
      "Iteration 64, loss = 0.30621578\n",
      "Iteration 65, loss = 0.31317839\n",
      "Iteration 66, loss = 0.30434327\n",
      "Iteration 67, loss = 0.30776069\n",
      "Iteration 68, loss = 0.30391523\n",
      "Iteration 69, loss = 0.31266150\n",
      "Iteration 70, loss = 0.31524495\n",
      "Iteration 71, loss = 0.30955286\n",
      "Iteration 72, loss = 0.30765415\n",
      "Iteration 73, loss = 0.33994184\n",
      "Iteration 74, loss = 0.32656026\n",
      "Iteration 75, loss = 0.31287392\n",
      "Iteration 76, loss = 0.31333827\n",
      "Iteration 77, loss = 0.31428591\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.41939284\n",
      "Iteration 2, loss = 4.02981291\n",
      "Iteration 3, loss = 2.97464210\n",
      "Iteration 4, loss = 2.23641178\n",
      "Iteration 5, loss = 1.74223233\n",
      "Iteration 6, loss = 1.39989676\n",
      "Iteration 7, loss = 1.14507927\n",
      "Iteration 8, loss = 0.97555607\n",
      "Iteration 9, loss = 0.85871829\n",
      "Iteration 10, loss = 0.75199202\n",
      "Iteration 11, loss = 0.67576268\n",
      "Iteration 12, loss = 0.60987918\n",
      "Iteration 13, loss = 0.57488114\n",
      "Iteration 14, loss = 0.56819955\n",
      "Iteration 15, loss = 0.51320346\n",
      "Iteration 16, loss = 0.51414294\n",
      "Iteration 17, loss = 0.47714814\n",
      "Iteration 18, loss = 0.46462006\n",
      "Iteration 19, loss = 0.44455386\n",
      "Iteration 20, loss = 0.42908287\n",
      "Iteration 21, loss = 0.41453621\n",
      "Iteration 22, loss = 0.39848949\n",
      "Iteration 23, loss = 0.39397791\n",
      "Iteration 24, loss = 0.38253790\n",
      "Iteration 25, loss = 0.39852021\n",
      "Iteration 26, loss = 0.38613874\n",
      "Iteration 27, loss = 0.38599777\n",
      "Iteration 28, loss = 0.37194664\n",
      "Iteration 29, loss = 0.37559516\n",
      "Iteration 30, loss = 0.39027884\n",
      "Iteration 31, loss = 0.38290464\n",
      "Iteration 32, loss = 0.38738763\n",
      "Iteration 33, loss = 0.36708026\n",
      "Iteration 34, loss = 0.36502544\n",
      "Iteration 35, loss = 0.34827055\n",
      "Iteration 36, loss = 0.35563354\n",
      "Iteration 37, loss = 0.36102479\n",
      "Iteration 38, loss = 0.35714237\n",
      "Iteration 39, loss = 0.35728642\n",
      "Iteration 40, loss = 0.36368327\n",
      "Iteration 41, loss = 0.34661542\n",
      "Iteration 42, loss = 0.34497299\n",
      "Iteration 43, loss = 0.34680111\n",
      "Iteration 44, loss = 0.34684664\n",
      "Iteration 45, loss = 0.34111215\n",
      "Iteration 46, loss = 0.33155049\n",
      "Iteration 47, loss = 0.32724771\n",
      "Iteration 48, loss = 0.33870955\n",
      "Iteration 49, loss = 0.35095307\n",
      "Iteration 50, loss = 0.33839057\n",
      "Iteration 51, loss = 0.33110531\n",
      "Iteration 52, loss = 0.33274600\n",
      "Iteration 53, loss = 0.34169498\n",
      "Iteration 54, loss = 0.34255578\n",
      "Iteration 55, loss = 0.33256392\n",
      "Iteration 56, loss = 0.32888566\n",
      "Iteration 57, loss = 0.32408312\n",
      "Iteration 58, loss = 0.33039042\n",
      "Iteration 59, loss = 0.33365577\n",
      "Iteration 60, loss = 0.32085851\n",
      "Iteration 61, loss = 0.32081447\n",
      "Iteration 62, loss = 0.32798312\n",
      "Iteration 63, loss = 0.32015167\n",
      "Iteration 64, loss = 0.31572017\n",
      "Iteration 65, loss = 0.32119571\n",
      "Iteration 66, loss = 0.31590262\n",
      "Iteration 67, loss = 0.31795224\n",
      "Iteration 68, loss = 0.31429062\n",
      "Iteration 69, loss = 0.32389113\n",
      "Iteration 70, loss = 0.32751160\n",
      "Iteration 71, loss = 0.32012686\n",
      "Iteration 72, loss = 0.31856996\n",
      "Iteration 73, loss = 0.34190007\n",
      "Iteration 74, loss = 0.33238649\n",
      "Iteration 75, loss = 0.32076138\n",
      "Iteration 76, loss = 0.32241745\n",
      "Iteration 77, loss = 0.32443020\n",
      "Iteration 78, loss = 0.32319578\n",
      "Iteration 79, loss = 0.32259829\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66657302\n",
      "Iteration 2, loss = 0.44495959\n",
      "Iteration 3, loss = 0.36791049\n",
      "Iteration 4, loss = 0.31465601\n",
      "Iteration 5, loss = 0.25303693\n",
      "Iteration 6, loss = 0.20695132\n",
      "Iteration 7, loss = 0.16792561\n",
      "Iteration 8, loss = 0.12980670\n",
      "Iteration 9, loss = 0.11175740\n",
      "Iteration 10, loss = 0.07716435\n",
      "Iteration 11, loss = 0.06219278\n",
      "Iteration 12, loss = 0.04245955\n",
      "Iteration 13, loss = 0.03310490\n",
      "Iteration 14, loss = 0.02807588\n",
      "Iteration 15, loss = 0.02729558\n",
      "Iteration 16, loss = 0.01557748\n",
      "Iteration 17, loss = 0.01036304\n",
      "Iteration 18, loss = 0.00658701\n",
      "Iteration 19, loss = 0.00826282\n",
      "Iteration 20, loss = 0.00576172\n",
      "Iteration 21, loss = 0.00440988\n",
      "Iteration 22, loss = 0.00305695\n",
      "Iteration 23, loss = 0.00231239\n",
      "Iteration 24, loss = 0.00193880\n",
      "Iteration 25, loss = 0.00164876\n",
      "Iteration 26, loss = 0.00152847\n",
      "Iteration 27, loss = 0.00138381\n",
      "Iteration 28, loss = 0.00122475\n",
      "Iteration 29, loss = 0.00108741\n",
      "Iteration 30, loss = 0.00098355\n",
      "Iteration 31, loss = 0.00091164\n",
      "Iteration 32, loss = 0.00083359\n",
      "Iteration 33, loss = 0.00077572\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.88407395\n",
      "Iteration 2, loss = 0.65982346\n",
      "Iteration 3, loss = 0.57996641\n",
      "Iteration 4, loss = 0.52617396\n",
      "Iteration 5, loss = 0.46294484\n",
      "Iteration 6, loss = 0.41299588\n",
      "Iteration 7, loss = 0.37403789\n",
      "Iteration 8, loss = 0.33699953\n",
      "Iteration 9, loss = 0.31368823\n",
      "Iteration 10, loss = 0.27794027\n",
      "Iteration 11, loss = 0.26079850\n",
      "Iteration 12, loss = 0.23635916\n",
      "Iteration 13, loss = 0.21980241\n",
      "Iteration 14, loss = 0.20936835\n",
      "Iteration 15, loss = 0.20292587\n",
      "Iteration 16, loss = 0.18941943\n",
      "Iteration 17, loss = 0.18413735\n",
      "Iteration 18, loss = 0.19285262\n",
      "Iteration 19, loss = 0.17823619\n",
      "Iteration 20, loss = 0.16714796\n",
      "Iteration 21, loss = 0.15812644\n",
      "Iteration 22, loss = 0.15254109\n",
      "Iteration 23, loss = 0.14779626\n",
      "Iteration 24, loss = 0.14346360\n",
      "Iteration 25, loss = 0.13915354\n",
      "Iteration 26, loss = 0.13498276\n",
      "Iteration 27, loss = 0.13103051\n",
      "Iteration 28, loss = 0.12712992\n",
      "Iteration 29, loss = 0.12356388\n",
      "Iteration 30, loss = 0.12016806\n",
      "Iteration 31, loss = 0.11715889\n",
      "Iteration 32, loss = 0.11391884\n",
      "Iteration 33, loss = 0.11063431\n",
      "Iteration 34, loss = 0.10726192\n",
      "Iteration 35, loss = 0.10421437\n",
      "Iteration 36, loss = 0.10130495\n",
      "Iteration 37, loss = 0.09899905\n",
      "Iteration 38, loss = 0.09592338\n",
      "Iteration 39, loss = 0.09320497\n",
      "Iteration 40, loss = 0.09084060\n",
      "Iteration 41, loss = 0.08872657\n",
      "Iteration 42, loss = 0.08635991\n",
      "Iteration 43, loss = 0.08389920\n",
      "Iteration 44, loss = 0.08177106\n",
      "Iteration 45, loss = 0.07953737\n",
      "Iteration 46, loss = 0.07724891\n",
      "Iteration 47, loss = 0.07553369\n",
      "Iteration 48, loss = 0.07405809\n",
      "Iteration 49, loss = 0.07279001\n",
      "Iteration 50, loss = 0.07046401\n",
      "Iteration 51, loss = 0.06860122\n",
      "Iteration 52, loss = 0.06641594\n",
      "Iteration 53, loss = 0.06477991\n",
      "Iteration 54, loss = 0.06358662\n",
      "Iteration 55, loss = 0.06310793\n",
      "Iteration 56, loss = 0.06117979\n",
      "Iteration 57, loss = 0.05981542\n",
      "Iteration 58, loss = 0.06001231\n",
      "Iteration 59, loss = 0.81558787\n",
      "Iteration 60, loss = 0.45246689\n",
      "Iteration 61, loss = 0.31492565\n",
      "Iteration 62, loss = 0.24655664\n",
      "Iteration 63, loss = 0.18389876\n",
      "Iteration 64, loss = 0.15912938\n",
      "Iteration 65, loss = 0.13955358\n",
      "Iteration 66, loss = 0.11428015\n",
      "Iteration 67, loss = 0.10143433\n",
      "Iteration 68, loss = 0.09444728\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09924373\n",
      "Iteration 2, loss = 0.86444961\n",
      "Iteration 3, loss = 0.77251877\n",
      "Iteration 4, loss = 0.70853811\n",
      "Iteration 5, loss = 0.63711159\n",
      "Iteration 6, loss = 0.57799842\n",
      "Iteration 7, loss = 0.52851530\n",
      "Iteration 8, loss = 0.48521591\n",
      "Iteration 9, loss = 0.45449697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.40723366\n",
      "Iteration 11, loss = 0.38066026\n",
      "Iteration 12, loss = 0.35331065\n",
      "Iteration 13, loss = 0.32675471\n",
      "Iteration 14, loss = 0.30919621\n",
      "Iteration 15, loss = 0.30222208\n",
      "Iteration 16, loss = 0.28087228\n",
      "Iteration 17, loss = 0.27073883\n",
      "Iteration 18, loss = 0.25643518\n",
      "Iteration 19, loss = 0.24734057\n",
      "Iteration 20, loss = 0.23331559\n",
      "Iteration 21, loss = 0.21959155\n",
      "Iteration 22, loss = 0.20830782\n",
      "Iteration 23, loss = 0.19841577\n",
      "Iteration 24, loss = 0.19062519\n",
      "Iteration 25, loss = 0.18355738\n",
      "Iteration 26, loss = 0.17558302\n",
      "Iteration 27, loss = 0.16954568\n",
      "Iteration 28, loss = 0.16225662\n",
      "Iteration 29, loss = 0.15665918\n",
      "Iteration 30, loss = 0.15144332\n",
      "Iteration 31, loss = 0.14573265\n",
      "Iteration 32, loss = 0.14284232\n",
      "Iteration 33, loss = 0.14357964\n",
      "Iteration 34, loss = 0.14703913\n",
      "Iteration 35, loss = 0.25523897\n",
      "Iteration 36, loss = 0.25230316\n",
      "Iteration 37, loss = 0.21427180\n",
      "Iteration 38, loss = 0.20589556\n",
      "Iteration 39, loss = 0.16620505\n",
      "Iteration 40, loss = 0.16334891\n",
      "Iteration 41, loss = 0.15797461\n",
      "Iteration 42, loss = 0.14102216\n",
      "Iteration 43, loss = 0.13130107\n",
      "Iteration 44, loss = 0.12562145\n",
      "Iteration 45, loss = 0.12102874\n",
      "Iteration 46, loss = 0.11690672\n",
      "Iteration 47, loss = 0.11307365\n",
      "Iteration 48, loss = 0.10954601\n",
      "Iteration 49, loss = 0.10628110\n",
      "Iteration 50, loss = 0.10337183\n",
      "Iteration 51, loss = 0.10040549\n",
      "Iteration 52, loss = 0.09806629\n",
      "Iteration 53, loss = 0.09524789\n",
      "Iteration 54, loss = 0.09280046\n",
      "Iteration 55, loss = 0.09050527\n",
      "Iteration 56, loss = 0.08930750\n",
      "Iteration 57, loss = 0.08824163\n",
      "Iteration 58, loss = 0.08468711\n",
      "Iteration 59, loss = 0.08236415\n",
      "Iteration 60, loss = 0.08021903\n",
      "Iteration 61, loss = 0.07887644\n",
      "Iteration 62, loss = 0.07768478\n",
      "Iteration 63, loss = 0.07619531\n",
      "Iteration 64, loss = 0.07396553\n",
      "Iteration 65, loss = 0.07274237\n",
      "Iteration 66, loss = 0.07134045\n",
      "Iteration 67, loss = 0.06968380\n",
      "Iteration 68, loss = 0.06860160\n",
      "Iteration 69, loss = 0.06758954\n",
      "Iteration 70, loss = 0.06645582\n",
      "Iteration 71, loss = 0.06515012\n",
      "Iteration 72, loss = 0.06436187\n",
      "Iteration 73, loss = 0.06328575\n",
      "Iteration 74, loss = 0.06265090\n",
      "Iteration 75, loss = 0.06228663\n",
      "Iteration 76, loss = 0.06104413\n",
      "Iteration 77, loss = 0.06046603\n",
      "Iteration 78, loss = 0.06045460\n",
      "Iteration 79, loss = 0.05895383\n",
      "Iteration 80, loss = 0.69556668\n",
      "Iteration 81, loss = 0.57539656\n",
      "Iteration 82, loss = 0.45282160\n",
      "Iteration 83, loss = 0.27685162\n",
      "Iteration 84, loss = 0.18779432\n",
      "Iteration 85, loss = 0.16930650\n",
      "Iteration 86, loss = 0.13359806\n",
      "Iteration 87, loss = 0.12164295\n",
      "Iteration 88, loss = 0.10617552\n",
      "Iteration 89, loss = 0.09947178\n",
      "Iteration 90, loss = 0.09474070\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.31164028\n",
      "Iteration 2, loss = 1.06073075\n",
      "Iteration 3, loss = 0.94987322\n",
      "Iteration 4, loss = 0.86517234\n",
      "Iteration 5, loss = 0.77800056\n",
      "Iteration 6, loss = 0.70648775\n",
      "Iteration 7, loss = 0.64153898\n",
      "Iteration 8, loss = 0.58849725\n",
      "Iteration 9, loss = 0.54679217\n",
      "Iteration 10, loss = 0.49099369\n",
      "Iteration 11, loss = 0.45981380\n",
      "Iteration 12, loss = 0.42810200\n",
      "Iteration 13, loss = 0.39472312\n",
      "Iteration 14, loss = 0.37098486\n",
      "Iteration 15, loss = 0.36431847\n",
      "Iteration 16, loss = 0.33454632\n",
      "Iteration 17, loss = 0.31783085\n",
      "Iteration 18, loss = 0.30909877\n",
      "Iteration 19, loss = 0.28226852\n",
      "Iteration 20, loss = 0.26314568\n",
      "Iteration 21, loss = 0.24721177\n",
      "Iteration 22, loss = 0.23427616\n",
      "Iteration 23, loss = 0.22207752\n",
      "Iteration 24, loss = 0.21326186\n",
      "Iteration 25, loss = 0.20451786\n",
      "Iteration 26, loss = 0.19599177\n",
      "Iteration 27, loss = 0.19004996\n",
      "Iteration 28, loss = 0.18464908\n",
      "Iteration 29, loss = 0.18868657\n",
      "Iteration 30, loss = 0.25071388\n",
      "Iteration 31, loss = 0.28027122\n",
      "Iteration 32, loss = 0.23918115\n",
      "Iteration 33, loss = 0.19715693\n",
      "Iteration 34, loss = 0.18444544\n",
      "Iteration 35, loss = 0.17016682\n",
      "Iteration 36, loss = 0.16059567\n",
      "Iteration 37, loss = 0.15335712\n",
      "Iteration 38, loss = 0.14580415\n",
      "Iteration 39, loss = 0.13989101\n",
      "Iteration 40, loss = 0.13403877\n",
      "Iteration 41, loss = 0.13123999\n",
      "Iteration 42, loss = 0.12671369\n",
      "Iteration 43, loss = 0.12178187\n",
      "Iteration 44, loss = 0.11840279\n",
      "Iteration 45, loss = 0.11497840\n",
      "Iteration 46, loss = 0.11289968\n",
      "Iteration 47, loss = 0.10975091\n",
      "Iteration 48, loss = 0.10604127\n",
      "Iteration 49, loss = 0.10431991\n",
      "Iteration 50, loss = 0.10183653\n",
      "Iteration 51, loss = 0.09933526\n",
      "Iteration 52, loss = 0.09728632\n",
      "Iteration 53, loss = 0.09606546\n",
      "Iteration 54, loss = 0.09436829\n",
      "Iteration 55, loss = 0.09580219\n",
      "Iteration 56, loss = 0.09301979\n",
      "Iteration 57, loss = 0.09250812\n",
      "Iteration 58, loss = 0.09413847\n",
      "Iteration 59, loss = 0.09559561\n",
      "Iteration 60, loss = 0.09431963\n",
      "Iteration 61, loss = 0.17026384\n",
      "Iteration 62, loss = 0.17036288\n",
      "Iteration 63, loss = 0.15995830\n",
      "Iteration 64, loss = 0.19247056\n",
      "Iteration 65, loss = 0.16559294\n",
      "Iteration 66, loss = 0.14898894\n",
      "Iteration 67, loss = 0.11952955\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.52208488\n",
      "Iteration 2, loss = 1.24823980\n",
      "Iteration 3, loss = 1.11272058\n",
      "Iteration 4, loss = 1.00617559\n",
      "Iteration 5, loss = 0.90146687\n",
      "Iteration 6, loss = 0.81166024\n",
      "Iteration 7, loss = 0.73506136\n",
      "Iteration 8, loss = 0.67028475\n",
      "Iteration 9, loss = 0.62590006\n",
      "Iteration 10, loss = 0.55510619\n",
      "Iteration 11, loss = 0.52157999\n",
      "Iteration 12, loss = 0.47717852\n",
      "Iteration 13, loss = 0.43761332\n",
      "Iteration 14, loss = 0.40708656\n",
      "Iteration 15, loss = 0.40460527\n",
      "Iteration 16, loss = 0.37135441\n",
      "Iteration 17, loss = 0.35573602\n",
      "Iteration 18, loss = 0.33044948\n",
      "Iteration 19, loss = 0.32005523\n",
      "Iteration 20, loss = 0.29823282\n",
      "Iteration 21, loss = 0.27103891\n",
      "Iteration 22, loss = 0.25490888\n",
      "Iteration 23, loss = 0.24083068\n",
      "Iteration 24, loss = 0.22901164\n",
      "Iteration 25, loss = 0.21824654\n",
      "Iteration 26, loss = 0.20892827\n",
      "Iteration 27, loss = 0.20393301\n",
      "Iteration 28, loss = 0.20370548\n",
      "Iteration 29, loss = 0.22046403\n",
      "Iteration 30, loss = 0.28652370\n",
      "Iteration 31, loss = 0.24917669\n",
      "Iteration 32, loss = 0.21866823\n",
      "Iteration 33, loss = 0.20025188\n",
      "Iteration 34, loss = 0.18827003\n",
      "Iteration 35, loss = 0.17109715\n",
      "Iteration 36, loss = 0.16227542\n",
      "Iteration 37, loss = 0.15636450\n",
      "Iteration 38, loss = 0.15073573\n",
      "Iteration 39, loss = 0.14464229\n",
      "Iteration 40, loss = 0.14002238\n",
      "Iteration 41, loss = 0.13610916\n",
      "Iteration 42, loss = 0.13122898\n",
      "Iteration 43, loss = 0.13084473\n",
      "Iteration 44, loss = 0.13664568\n",
      "Iteration 45, loss = 0.12855491\n",
      "Iteration 46, loss = 0.12129005\n",
      "Iteration 47, loss = 0.11741158\n",
      "Iteration 48, loss = 0.11437679\n",
      "Iteration 49, loss = 0.11330816\n",
      "Iteration 50, loss = 0.11122278\n",
      "Iteration 51, loss = 0.10799039\n",
      "Iteration 52, loss = 0.10523043\n",
      "Iteration 53, loss = 0.10351986\n",
      "Iteration 54, loss = 0.10273865\n",
      "Iteration 55, loss = 0.12360910\n",
      "Iteration 56, loss = 0.15057067\n",
      "Iteration 57, loss = 0.20039540\n",
      "Iteration 58, loss = 0.22787535\n",
      "Iteration 59, loss = 0.21919459\n",
      "Iteration 60, loss = 0.16745228\n",
      "Iteration 61, loss = 0.14819220\n",
      "Iteration 62, loss = 0.12981796\n",
      "Iteration 63, loss = 0.12452495\n",
      "Iteration 64, loss = 0.11463320\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.73174687\n",
      "Iteration 2, loss = 1.42844884\n",
      "Iteration 3, loss = 1.26044372\n",
      "Iteration 4, loss = 1.13195756\n",
      "Iteration 5, loss = 1.00416742\n",
      "Iteration 6, loss = 0.89776345\n",
      "Iteration 7, loss = 0.80308410\n",
      "Iteration 8, loss = 0.73066657\n",
      "Iteration 9, loss = 0.67805823\n",
      "Iteration 10, loss = 0.59760745\n",
      "Iteration 11, loss = 0.55637083\n",
      "Iteration 12, loss = 0.50796806\n",
      "Iteration 13, loss = 0.46364672\n",
      "Iteration 14, loss = 0.43282339\n",
      "Iteration 15, loss = 0.43515948\n",
      "Iteration 16, loss = 0.39094832\n",
      "Iteration 17, loss = 0.37586747\n",
      "Iteration 18, loss = 0.34162337\n",
      "Iteration 19, loss = 0.32257354\n",
      "Iteration 20, loss = 0.30468251\n",
      "Iteration 21, loss = 0.31292973\n",
      "Iteration 22, loss = 0.28495489\n",
      "Iteration 23, loss = 0.26014300\n",
      "Iteration 24, loss = 0.24666225\n",
      "Iteration 25, loss = 0.23181653\n",
      "Iteration 26, loss = 0.22025756\n",
      "Iteration 27, loss = 0.21348693\n",
      "Iteration 28, loss = 0.20874101\n",
      "Iteration 29, loss = 0.20613226\n",
      "Iteration 30, loss = 0.19389273\n",
      "Iteration 31, loss = 0.18437116\n",
      "Iteration 32, loss = 0.18513474\n",
      "Iteration 33, loss = 0.17396915\n",
      "Iteration 34, loss = 0.17424397\n",
      "Iteration 35, loss = 0.17606402\n",
      "Iteration 36, loss = 0.16179972\n",
      "Iteration 37, loss = 0.15949147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 0.15317752\n",
      "Iteration 39, loss = 0.14776957\n",
      "Iteration 40, loss = 0.14155560\n",
      "Iteration 41, loss = 0.13668775\n",
      "Iteration 42, loss = 0.13874372\n",
      "Iteration 43, loss = 0.13369695\n",
      "Iteration 44, loss = 0.13313023\n",
      "Iteration 45, loss = 0.12904896\n",
      "Iteration 46, loss = 0.13171811\n",
      "Iteration 47, loss = 0.20877223\n",
      "Iteration 48, loss = 0.20304637\n",
      "Iteration 49, loss = 0.18185846\n",
      "Iteration 50, loss = 0.18881075\n",
      "Iteration 51, loss = 0.16480081\n",
      "Iteration 52, loss = 0.15256743\n",
      "Iteration 53, loss = 0.16131226\n",
      "Iteration 54, loss = 0.13628068\n",
      "Iteration 55, loss = 0.13457059\n",
      "Iteration 56, loss = 0.14401894\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.93875009\n",
      "Iteration 2, loss = 1.60272900\n",
      "Iteration 3, loss = 1.39929565\n",
      "Iteration 4, loss = 1.23937863\n",
      "Iteration 5, loss = 1.08699840\n",
      "Iteration 6, loss = 0.96121226\n",
      "Iteration 7, loss = 0.85540880\n",
      "Iteration 8, loss = 0.77231658\n",
      "Iteration 9, loss = 0.71309162\n",
      "Iteration 10, loss = 0.63205760\n",
      "Iteration 11, loss = 0.57968266\n",
      "Iteration 12, loss = 0.54010995\n",
      "Iteration 13, loss = 0.49899277\n",
      "Iteration 14, loss = 0.45802850\n",
      "Iteration 15, loss = 0.43848499\n",
      "Iteration 16, loss = 0.40741014\n",
      "Iteration 17, loss = 0.38466762\n",
      "Iteration 18, loss = 0.38178450\n",
      "Iteration 19, loss = 0.33620030\n",
      "Iteration 20, loss = 0.31192339\n",
      "Iteration 21, loss = 0.28636603\n",
      "Iteration 22, loss = 0.26786991\n",
      "Iteration 23, loss = 0.25321539\n",
      "Iteration 24, loss = 0.24393688\n",
      "Iteration 25, loss = 0.25558238\n",
      "Iteration 26, loss = 0.24132595\n",
      "Iteration 27, loss = 0.23709005\n",
      "Iteration 28, loss = 0.21986400\n",
      "Iteration 29, loss = 0.20848316\n",
      "Iteration 30, loss = 0.22801948\n",
      "Iteration 31, loss = 0.21064659\n",
      "Iteration 32, loss = 0.23136797\n",
      "Iteration 33, loss = 0.19482428\n",
      "Iteration 34, loss = 0.19530099\n",
      "Iteration 35, loss = 0.18271341\n",
      "Iteration 36, loss = 0.17051629\n",
      "Iteration 37, loss = 0.16437324\n",
      "Iteration 38, loss = 0.15795583\n",
      "Iteration 39, loss = 0.15523192\n",
      "Iteration 40, loss = 0.14998875\n",
      "Iteration 41, loss = 0.14585644\n",
      "Iteration 42, loss = 0.14711819\n",
      "Iteration 43, loss = 0.14826144\n",
      "Iteration 44, loss = 0.14623187\n",
      "Iteration 45, loss = 0.14473758\n",
      "Iteration 46, loss = 0.13890017\n",
      "Iteration 47, loss = 0.14250160\n",
      "Iteration 48, loss = 0.14567656\n",
      "Iteration 49, loss = 0.13985915\n",
      "Iteration 50, loss = 0.14357957\n",
      "Iteration 51, loss = 0.14286426\n",
      "Iteration 52, loss = 0.14957713\n",
      "Iteration 53, loss = 0.15434588\n",
      "Iteration 54, loss = 0.14695191\n",
      "Iteration 55, loss = 0.14656221\n",
      "Iteration 56, loss = 0.14968220\n",
      "Iteration 57, loss = 0.14292352\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14391526\n",
      "Iteration 2, loss = 1.76864554\n",
      "Iteration 3, loss = 1.52877182\n",
      "Iteration 4, loss = 1.33996002\n",
      "Iteration 5, loss = 1.16350728\n",
      "Iteration 6, loss = 1.02306810\n",
      "Iteration 7, loss = 0.90251743\n",
      "Iteration 8, loss = 0.81202175\n",
      "Iteration 9, loss = 0.74259832\n",
      "Iteration 10, loss = 0.65220590\n",
      "Iteration 11, loss = 0.59651496\n",
      "Iteration 12, loss = 0.55280590\n",
      "Iteration 13, loss = 0.50551486\n",
      "Iteration 14, loss = 0.46085504\n",
      "Iteration 15, loss = 0.45525740\n",
      "Iteration 16, loss = 0.41499455\n",
      "Iteration 17, loss = 0.39478227\n",
      "Iteration 18, loss = 0.35785964\n",
      "Iteration 19, loss = 0.34344996\n",
      "Iteration 20, loss = 0.32336764\n",
      "Iteration 21, loss = 0.30234290\n",
      "Iteration 22, loss = 0.28369400\n",
      "Iteration 23, loss = 0.26342013\n",
      "Iteration 24, loss = 0.25161779\n",
      "Iteration 25, loss = 0.24569966\n",
      "Iteration 26, loss = 0.23193197\n",
      "Iteration 27, loss = 0.24316076\n",
      "Iteration 28, loss = 0.22325649\n",
      "Iteration 29, loss = 0.22826460\n",
      "Iteration 30, loss = 0.26686943\n",
      "Iteration 31, loss = 0.27503015\n",
      "Iteration 32, loss = 0.23449308\n",
      "Iteration 33, loss = 0.24819981\n",
      "Iteration 34, loss = 0.23344492\n",
      "Iteration 35, loss = 0.21208567\n",
      "Iteration 36, loss = 0.19024602\n",
      "Iteration 37, loss = 0.18287203\n",
      "Iteration 38, loss = 0.17382626\n",
      "Iteration 39, loss = 0.16877392\n",
      "Iteration 40, loss = 0.16339700\n",
      "Iteration 41, loss = 0.16134393\n",
      "Iteration 42, loss = 0.16972482\n",
      "Iteration 43, loss = 0.16332113\n",
      "Iteration 44, loss = 0.15863103\n",
      "Iteration 45, loss = 0.15701042\n",
      "Iteration 46, loss = 0.14877520\n",
      "Iteration 47, loss = 0.14833237\n",
      "Iteration 48, loss = 0.14693603\n",
      "Iteration 49, loss = 0.14481521\n",
      "Iteration 50, loss = 0.14679933\n",
      "Iteration 51, loss = 0.14533135\n",
      "Iteration 52, loss = 0.15140461\n",
      "Iteration 53, loss = 0.18170625\n",
      "Iteration 54, loss = 0.18618391\n",
      "Iteration 55, loss = 0.16260058\n",
      "Iteration 56, loss = 0.16269609\n",
      "Iteration 57, loss = 0.15398148\n",
      "Iteration 58, loss = 0.14506732\n",
      "Iteration 59, loss = 0.13754924\n",
      "Iteration 60, loss = 0.13359146\n",
      "Iteration 61, loss = 0.13482655\n",
      "Iteration 62, loss = 0.13771317\n",
      "Iteration 63, loss = 0.13470941\n",
      "Iteration 64, loss = 0.13254194\n",
      "Iteration 65, loss = 0.13074930\n",
      "Iteration 66, loss = 0.13771791\n",
      "Iteration 67, loss = 0.13326683\n",
      "Iteration 68, loss = 0.12980829\n",
      "Iteration 69, loss = 0.13000524\n",
      "Iteration 70, loss = 0.12688084\n",
      "Iteration 71, loss = 0.12832946\n",
      "Iteration 72, loss = 0.13408870\n",
      "Iteration 73, loss = 0.14165121\n",
      "Iteration 74, loss = 0.14850432\n",
      "Iteration 75, loss = 0.14411143\n",
      "Iteration 76, loss = 0.13424204\n",
      "Iteration 77, loss = 0.13309970\n",
      "Iteration 78, loss = 0.13965178\n",
      "Iteration 79, loss = 0.13423916\n",
      "Iteration 80, loss = 0.16687459\n",
      "Iteration 81, loss = 0.17534103\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.34777988\n",
      "Iteration 2, loss = 1.93031263\n",
      "Iteration 3, loss = 1.64907350\n",
      "Iteration 4, loss = 1.42568156\n",
      "Iteration 5, loss = 1.22592473\n",
      "Iteration 6, loss = 1.06875568\n",
      "Iteration 7, loss = 0.93576391\n",
      "Iteration 8, loss = 0.83718109\n",
      "Iteration 9, loss = 0.76090018\n",
      "Iteration 10, loss = 0.67447922\n",
      "Iteration 11, loss = 0.61172887\n",
      "Iteration 12, loss = 0.56024526\n",
      "Iteration 13, loss = 0.50992749\n",
      "Iteration 14, loss = 0.46861624\n",
      "Iteration 15, loss = 0.45449542\n",
      "Iteration 16, loss = 0.41423837\n",
      "Iteration 17, loss = 0.38712567\n",
      "Iteration 18, loss = 0.36873883\n",
      "Iteration 19, loss = 0.35591918\n",
      "Iteration 20, loss = 0.33019455\n",
      "Iteration 21, loss = 0.31534406\n",
      "Iteration 22, loss = 0.31858755\n",
      "Iteration 23, loss = 0.27761322\n",
      "Iteration 24, loss = 0.26383364\n",
      "Iteration 25, loss = 0.25498870\n",
      "Iteration 26, loss = 0.24124279\n",
      "Iteration 27, loss = 0.23359446\n",
      "Iteration 28, loss = 0.22319999\n",
      "Iteration 29, loss = 0.21805242\n",
      "Iteration 30, loss = 0.22460530\n",
      "Iteration 31, loss = 0.21184401\n",
      "Iteration 32, loss = 0.20459256\n",
      "Iteration 33, loss = 0.19534396\n",
      "Iteration 34, loss = 0.20112597\n",
      "Iteration 35, loss = 0.20007250\n",
      "Iteration 36, loss = 0.18570892\n",
      "Iteration 37, loss = 0.18443984\n",
      "Iteration 38, loss = 0.17663394\n",
      "Iteration 39, loss = 0.17544643\n",
      "Iteration 40, loss = 0.17003894\n",
      "Iteration 41, loss = 0.16706950\n",
      "Iteration 42, loss = 0.17349195\n",
      "Iteration 43, loss = 0.17085898\n",
      "Iteration 44, loss = 0.17506142\n",
      "Iteration 45, loss = 0.17879457\n",
      "Iteration 46, loss = 0.18138728\n",
      "Iteration 47, loss = 0.17478703\n",
      "Iteration 48, loss = 0.17344454\n",
      "Iteration 49, loss = 0.16310524\n",
      "Iteration 50, loss = 0.17144701\n",
      "Iteration 51, loss = 0.16454378\n",
      "Iteration 52, loss = 0.17786131\n",
      "Iteration 53, loss = 0.19041061\n",
      "Iteration 54, loss = 0.16969445\n",
      "Iteration 55, loss = 0.18029760\n",
      "Iteration 56, loss = 0.16418686\n",
      "Iteration 57, loss = 0.16274347\n",
      "Iteration 58, loss = 0.15037388\n",
      "Iteration 59, loss = 0.14512217\n",
      "Iteration 60, loss = 0.14519139\n",
      "Iteration 61, loss = 0.14434334\n",
      "Iteration 62, loss = 0.14538140\n",
      "Iteration 63, loss = 0.14145262\n",
      "Iteration 64, loss = 0.14203258\n",
      "Iteration 65, loss = 0.14423051\n",
      "Iteration 66, loss = 0.16305364\n",
      "Iteration 67, loss = 0.16071405\n",
      "Iteration 68, loss = 0.15347363\n",
      "Iteration 69, loss = 0.14766865\n",
      "Iteration 70, loss = 0.14687386\n",
      "Iteration 71, loss = 0.14819128\n",
      "Iteration 72, loss = 0.15286023\n",
      "Iteration 73, loss = 0.14590317\n",
      "Iteration 74, loss = 0.15778708\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.54978342\n",
      "Iteration 2, loss = 2.08478447\n",
      "Iteration 3, loss = 1.76004685\n",
      "Iteration 4, loss = 1.50439648\n",
      "Iteration 5, loss = 1.28262657\n",
      "Iteration 6, loss = 1.10372948\n",
      "Iteration 7, loss = 0.96206265\n",
      "Iteration 8, loss = 0.85278400\n",
      "Iteration 9, loss = 0.77225376\n",
      "Iteration 10, loss = 0.68172265\n",
      "Iteration 11, loss = 0.63244586\n",
      "Iteration 12, loss = 0.56003926\n",
      "Iteration 13, loss = 0.51836764\n",
      "Iteration 14, loss = 0.47883040\n",
      "Iteration 15, loss = 0.44189965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.41975612\n",
      "Iteration 17, loss = 0.39664036\n",
      "Iteration 18, loss = 0.39492782\n",
      "Iteration 19, loss = 0.37168808\n",
      "Iteration 20, loss = 0.33879035\n",
      "Iteration 21, loss = 0.31958895\n",
      "Iteration 22, loss = 0.29629446\n",
      "Iteration 23, loss = 0.27580134\n",
      "Iteration 24, loss = 0.26218496\n",
      "Iteration 25, loss = 0.26049276\n",
      "Iteration 26, loss = 0.24848861\n",
      "Iteration 27, loss = 0.26273885\n",
      "Iteration 28, loss = 0.24858063\n",
      "Iteration 29, loss = 0.24981972\n",
      "Iteration 30, loss = 0.24887945\n",
      "Iteration 31, loss = 0.22441219\n",
      "Iteration 32, loss = 0.21677456\n",
      "Iteration 33, loss = 0.21623142\n",
      "Iteration 34, loss = 0.22227645\n",
      "Iteration 35, loss = 0.23994909\n",
      "Iteration 36, loss = 0.21383944\n",
      "Iteration 37, loss = 0.20384836\n",
      "Iteration 38, loss = 0.19075811\n",
      "Iteration 39, loss = 0.19137509\n",
      "Iteration 40, loss = 0.18286226\n",
      "Iteration 41, loss = 0.17860178\n",
      "Iteration 42, loss = 0.18625052\n",
      "Iteration 43, loss = 0.17775456\n",
      "Iteration 44, loss = 0.17498129\n",
      "Iteration 45, loss = 0.17652589\n",
      "Iteration 46, loss = 0.17256015\n",
      "Iteration 47, loss = 0.17938613\n",
      "Iteration 48, loss = 0.18751206\n",
      "Iteration 49, loss = 0.19118406\n",
      "Iteration 50, loss = 0.18273512\n",
      "Iteration 51, loss = 0.17861245\n",
      "Iteration 52, loss = 0.18812955\n",
      "Iteration 53, loss = 0.22826309\n",
      "Iteration 54, loss = 0.22478241\n",
      "Iteration 55, loss = 0.24062037\n",
      "Iteration 56, loss = 0.22577708\n",
      "Iteration 57, loss = 0.19914303\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.75069965\n",
      "Iteration 2, loss = 2.23586271\n",
      "Iteration 3, loss = 1.86574504\n",
      "Iteration 4, loss = 1.57665211\n",
      "Iteration 5, loss = 1.32851288\n",
      "Iteration 6, loss = 1.13403852\n",
      "Iteration 7, loss = 0.98242975\n",
      "Iteration 8, loss = 0.86441353\n",
      "Iteration 9, loss = 0.78628381\n",
      "Iteration 10, loss = 0.68442860\n",
      "Iteration 11, loss = 0.62689779\n",
      "Iteration 12, loss = 0.57780347\n",
      "Iteration 13, loss = 0.53578662\n",
      "Iteration 14, loss = 0.49625574\n",
      "Iteration 15, loss = 0.44772429\n",
      "Iteration 16, loss = 0.42430034\n",
      "Iteration 17, loss = 0.39667375\n",
      "Iteration 18, loss = 0.39924257\n",
      "Iteration 19, loss = 0.36787000\n",
      "Iteration 20, loss = 0.33765291\n",
      "Iteration 21, loss = 0.33324441\n",
      "Iteration 22, loss = 0.30828384\n",
      "Iteration 23, loss = 0.28209689\n",
      "Iteration 24, loss = 0.26923577\n",
      "Iteration 25, loss = 0.27978145\n",
      "Iteration 26, loss = 0.26687092\n",
      "Iteration 27, loss = 0.26874334\n",
      "Iteration 28, loss = 0.26110134\n",
      "Iteration 29, loss = 0.24369015\n",
      "Iteration 30, loss = 0.24647028\n",
      "Iteration 31, loss = 0.23639593\n",
      "Iteration 32, loss = 0.23778778\n",
      "Iteration 33, loss = 0.24009382\n",
      "Iteration 34, loss = 0.22428996\n",
      "Iteration 35, loss = 0.21970220\n",
      "Iteration 36, loss = 0.20501170\n",
      "Iteration 37, loss = 0.20050907\n",
      "Iteration 38, loss = 0.20087158\n",
      "Iteration 39, loss = 0.19902993\n",
      "Iteration 40, loss = 0.18971707\n",
      "Iteration 41, loss = 0.18603540\n",
      "Iteration 42, loss = 0.19264764\n",
      "Iteration 43, loss = 0.19142596\n",
      "Iteration 44, loss = 0.18624058\n",
      "Iteration 45, loss = 0.18500962\n",
      "Iteration 46, loss = 0.18067767\n",
      "Iteration 47, loss = 0.17958471\n",
      "Iteration 48, loss = 0.19297636\n",
      "Iteration 49, loss = 0.18452342\n",
      "Iteration 50, loss = 0.18973332\n",
      "Iteration 51, loss = 0.18864207\n",
      "Iteration 52, loss = 0.19623684\n",
      "Iteration 53, loss = 0.23065834\n",
      "Iteration 54, loss = 0.22209022\n",
      "Iteration 55, loss = 0.22716741\n",
      "Iteration 56, loss = 0.22941474\n",
      "Iteration 57, loss = 0.19857725\n",
      "Iteration 58, loss = 0.18357627\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.94939084\n",
      "Iteration 2, loss = 2.37947011\n",
      "Iteration 3, loss = 1.96375559\n",
      "Iteration 4, loss = 1.64610116\n",
      "Iteration 5, loss = 1.37471869\n",
      "Iteration 6, loss = 1.16605592\n",
      "Iteration 7, loss = 1.00640323\n",
      "Iteration 8, loss = 0.88342881\n",
      "Iteration 9, loss = 0.79669295\n",
      "Iteration 10, loss = 0.69827945\n",
      "Iteration 11, loss = 0.63798504\n",
      "Iteration 12, loss = 0.57497869\n",
      "Iteration 13, loss = 0.53177340\n",
      "Iteration 14, loss = 0.49167515\n",
      "Iteration 15, loss = 0.45243665\n",
      "Iteration 16, loss = 0.42545934\n",
      "Iteration 17, loss = 0.40940418\n",
      "Iteration 18, loss = 0.40944919\n",
      "Iteration 19, loss = 0.37668673\n",
      "Iteration 20, loss = 0.34126426\n",
      "Iteration 21, loss = 0.34091659\n",
      "Iteration 22, loss = 0.32302810\n",
      "Iteration 23, loss = 0.29885160\n",
      "Iteration 24, loss = 0.28217229\n",
      "Iteration 25, loss = 0.29957422\n",
      "Iteration 26, loss = 0.28483292\n",
      "Iteration 27, loss = 0.28763881\n",
      "Iteration 28, loss = 0.27382381\n",
      "Iteration 29, loss = 0.25344170\n",
      "Iteration 30, loss = 0.25834916\n",
      "Iteration 31, loss = 0.24101335\n",
      "Iteration 32, loss = 0.24528883\n",
      "Iteration 33, loss = 0.24428192\n",
      "Iteration 34, loss = 0.23713827\n",
      "Iteration 35, loss = 0.24108224\n",
      "Iteration 36, loss = 0.22186855\n",
      "Iteration 37, loss = 0.21888264\n",
      "Iteration 38, loss = 0.21315273\n",
      "Iteration 39, loss = 0.21270727\n",
      "Iteration 40, loss = 0.20140794\n",
      "Iteration 41, loss = 0.19791686\n",
      "Iteration 42, loss = 0.20658275\n",
      "Iteration 43, loss = 0.20620535\n",
      "Iteration 44, loss = 0.19938110\n",
      "Iteration 45, loss = 0.20526861\n",
      "Iteration 46, loss = 0.19696615\n",
      "Iteration 47, loss = 0.19889310\n",
      "Iteration 48, loss = 0.19917533\n",
      "Iteration 49, loss = 0.20113823\n",
      "Iteration 50, loss = 0.19812251\n",
      "Iteration 51, loss = 0.19618009\n",
      "Iteration 52, loss = 0.20752706\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.14583072\n",
      "Iteration 2, loss = 2.52222911\n",
      "Iteration 3, loss = 2.05731419\n",
      "Iteration 4, loss = 1.70848038\n",
      "Iteration 5, loss = 1.41596711\n",
      "Iteration 6, loss = 1.19361842\n",
      "Iteration 7, loss = 1.02182289\n",
      "Iteration 8, loss = 0.89598238\n",
      "Iteration 9, loss = 0.80376194\n",
      "Iteration 10, loss = 0.70799084\n",
      "Iteration 11, loss = 0.63866922\n",
      "Iteration 12, loss = 0.58721320\n",
      "Iteration 13, loss = 0.54263491\n",
      "Iteration 14, loss = 0.50461557\n",
      "Iteration 15, loss = 0.45609829\n",
      "Iteration 16, loss = 0.43551650\n",
      "Iteration 17, loss = 0.41118855\n",
      "Iteration 18, loss = 0.43016275\n",
      "Iteration 19, loss = 0.38649285\n",
      "Iteration 20, loss = 0.35738973\n",
      "Iteration 21, loss = 0.35895894\n",
      "Iteration 22, loss = 0.32546575\n",
      "Iteration 23, loss = 0.30058381\n",
      "Iteration 24, loss = 0.28870561\n",
      "Iteration 25, loss = 0.29443702\n",
      "Iteration 26, loss = 0.27793995\n",
      "Iteration 27, loss = 0.29191653\n",
      "Iteration 28, loss = 0.27914266\n",
      "Iteration 29, loss = 0.25984373\n",
      "Iteration 30, loss = 0.27230551\n",
      "Iteration 31, loss = 0.26242374\n",
      "Iteration 32, loss = 0.27555091\n",
      "Iteration 33, loss = 0.26291418\n",
      "Iteration 34, loss = 0.25801623\n",
      "Iteration 35, loss = 0.25359666\n",
      "Iteration 36, loss = 0.23372672\n",
      "Iteration 37, loss = 0.22834840\n",
      "Iteration 38, loss = 0.22173118\n",
      "Iteration 39, loss = 0.22219896\n",
      "Iteration 40, loss = 0.21238302\n",
      "Iteration 41, loss = 0.20969124\n",
      "Iteration 42, loss = 0.21982943\n",
      "Iteration 43, loss = 0.22203365\n",
      "Iteration 44, loss = 0.21168239\n",
      "Iteration 45, loss = 0.21488705\n",
      "Iteration 46, loss = 0.20778108\n",
      "Iteration 47, loss = 0.20605249\n",
      "Iteration 48, loss = 0.21102853\n",
      "Iteration 49, loss = 0.21508627\n",
      "Iteration 50, loss = 0.21171133\n",
      "Iteration 51, loss = 0.20865488\n",
      "Iteration 52, loss = 0.22603587\n",
      "Iteration 53, loss = 0.26473692\n",
      "Iteration 54, loss = 0.23357891\n",
      "Iteration 55, loss = 0.23877412\n",
      "Iteration 56, loss = 0.23890699\n",
      "Iteration 57, loss = 0.21713584\n",
      "Iteration 58, loss = 0.20386339\n",
      "Iteration 59, loss = 0.19795259\n",
      "Iteration 60, loss = 0.19838955\n",
      "Iteration 61, loss = 0.20010295\n",
      "Iteration 62, loss = 0.21281480\n",
      "Iteration 63, loss = 0.20974426\n",
      "Iteration 64, loss = 0.20190305\n",
      "Iteration 65, loss = 0.19855233\n",
      "Iteration 66, loss = 0.20118379\n",
      "Iteration 67, loss = 0.18906259\n",
      "Iteration 68, loss = 0.19111614\n",
      "Iteration 69, loss = 0.19190173\n",
      "Iteration 70, loss = 0.19088441\n",
      "Iteration 71, loss = 0.19645453\n",
      "Iteration 72, loss = 0.20416524\n",
      "Iteration 73, loss = 0.19869070\n",
      "Iteration 74, loss = 0.20995934\n",
      "Iteration 75, loss = 0.20416626\n",
      "Iteration 76, loss = 0.19947333\n",
      "Iteration 77, loss = 0.19952217\n",
      "Iteration 78, loss = 0.20761821\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.34237112\n",
      "Iteration 2, loss = 2.65912746\n",
      "Iteration 3, loss = 2.14790378\n",
      "Iteration 4, loss = 1.76298455\n",
      "Iteration 5, loss = 1.44707769\n",
      "Iteration 6, loss = 1.20942354\n",
      "Iteration 7, loss = 1.03468165\n",
      "Iteration 8, loss = 0.89830464\n",
      "Iteration 9, loss = 0.80320380\n",
      "Iteration 10, loss = 0.70700607\n",
      "Iteration 11, loss = 0.64221348\n",
      "Iteration 12, loss = 0.58845356\n",
      "Iteration 13, loss = 0.54948723\n",
      "Iteration 14, loss = 0.51376605\n",
      "Iteration 15, loss = 0.46088351\n",
      "Iteration 16, loss = 0.42748148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.40779631\n",
      "Iteration 18, loss = 0.41139693\n",
      "Iteration 19, loss = 0.38548095\n",
      "Iteration 20, loss = 0.36212000\n",
      "Iteration 21, loss = 0.34946158\n",
      "Iteration 22, loss = 0.33742812\n",
      "Iteration 23, loss = 0.30671016\n",
      "Iteration 24, loss = 0.29626680\n",
      "Iteration 25, loss = 0.30638761\n",
      "Iteration 26, loss = 0.29256996\n",
      "Iteration 27, loss = 0.29289962\n",
      "Iteration 28, loss = 0.29384858\n",
      "Iteration 29, loss = 0.26387733\n",
      "Iteration 30, loss = 0.28053458\n",
      "Iteration 31, loss = 0.27112901\n",
      "Iteration 32, loss = 0.27460441\n",
      "Iteration 33, loss = 0.25482499\n",
      "Iteration 34, loss = 0.25173331\n",
      "Iteration 35, loss = 0.26564173\n",
      "Iteration 36, loss = 0.24139768\n",
      "Iteration 37, loss = 0.23598301\n",
      "Iteration 38, loss = 0.22931440\n",
      "Iteration 39, loss = 0.23210755\n",
      "Iteration 40, loss = 0.22411438\n",
      "Iteration 41, loss = 0.21939444\n",
      "Iteration 42, loss = 0.22764323\n",
      "Iteration 43, loss = 0.22768738\n",
      "Iteration 44, loss = 0.22332299\n",
      "Iteration 45, loss = 0.22894508\n",
      "Iteration 46, loss = 0.21958696\n",
      "Iteration 47, loss = 0.22583928\n",
      "Iteration 48, loss = 0.22817176\n",
      "Iteration 49, loss = 0.23161331\n",
      "Iteration 50, loss = 0.22680925\n",
      "Iteration 51, loss = 0.21982800\n",
      "Iteration 52, loss = 0.24392277\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.53637970\n",
      "Iteration 2, loss = 2.79343799\n",
      "Iteration 3, loss = 2.23190762\n",
      "Iteration 4, loss = 1.81079239\n",
      "Iteration 5, loss = 1.47685780\n",
      "Iteration 6, loss = 1.23066642\n",
      "Iteration 7, loss = 1.04252329\n",
      "Iteration 8, loss = 0.90861364\n",
      "Iteration 9, loss = 0.82214308\n",
      "Iteration 10, loss = 0.71645555\n",
      "Iteration 11, loss = 0.65190182\n",
      "Iteration 12, loss = 0.58188463\n",
      "Iteration 13, loss = 0.54015512\n",
      "Iteration 14, loss = 0.51089690\n",
      "Iteration 15, loss = 0.46236484\n",
      "Iteration 16, loss = 0.43887210\n",
      "Iteration 17, loss = 0.41744116\n",
      "Iteration 18, loss = 0.42337636\n",
      "Iteration 19, loss = 0.38989825\n",
      "Iteration 20, loss = 0.36303149\n",
      "Iteration 21, loss = 0.36012335\n",
      "Iteration 22, loss = 0.34464700\n",
      "Iteration 23, loss = 0.31810175\n",
      "Iteration 24, loss = 0.30264612\n",
      "Iteration 25, loss = 0.31023942\n",
      "Iteration 26, loss = 0.29843744\n",
      "Iteration 27, loss = 0.30447672\n",
      "Iteration 28, loss = 0.29930075\n",
      "Iteration 29, loss = 0.27252648\n",
      "Iteration 30, loss = 0.28355242\n",
      "Iteration 31, loss = 0.27575230\n",
      "Iteration 32, loss = 0.28959086\n",
      "Iteration 33, loss = 0.27180005\n",
      "Iteration 34, loss = 0.27053095\n",
      "Iteration 35, loss = 0.27872742\n",
      "Iteration 36, loss = 0.25769520\n",
      "Iteration 37, loss = 0.25985035\n",
      "Iteration 38, loss = 0.24363978\n",
      "Iteration 39, loss = 0.24949486\n",
      "Iteration 40, loss = 0.23493064\n",
      "Iteration 41, loss = 0.23068849\n",
      "Iteration 42, loss = 0.24556312\n",
      "Iteration 43, loss = 0.23820422\n",
      "Iteration 44, loss = 0.23620897\n",
      "Iteration 45, loss = 0.23367726\n",
      "Iteration 46, loss = 0.22702227\n",
      "Iteration 47, loss = 0.22694729\n",
      "Iteration 48, loss = 0.24292545\n",
      "Iteration 49, loss = 0.26783215\n",
      "Iteration 50, loss = 0.25030055\n",
      "Iteration 51, loss = 0.23828105\n",
      "Iteration 52, loss = 0.23779517\n",
      "Iteration 53, loss = 0.24393626\n",
      "Iteration 54, loss = 0.24016177\n",
      "Iteration 55, loss = 0.24339178\n",
      "Iteration 56, loss = 0.24922514\n",
      "Iteration 57, loss = 0.23485041\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.72986895\n",
      "Iteration 2, loss = 2.92304972\n",
      "Iteration 3, loss = 2.30904923\n",
      "Iteration 4, loss = 1.85713397\n",
      "Iteration 5, loss = 1.50197045\n",
      "Iteration 6, loss = 1.24148294\n",
      "Iteration 7, loss = 1.05042154\n",
      "Iteration 8, loss = 0.91222614\n",
      "Iteration 9, loss = 0.80957006\n",
      "Iteration 10, loss = 0.72235809\n",
      "Iteration 11, loss = 0.66964965\n",
      "Iteration 12, loss = 0.59206069\n",
      "Iteration 13, loss = 0.54800980\n",
      "Iteration 14, loss = 0.51554371\n",
      "Iteration 15, loss = 0.46584455\n",
      "Iteration 16, loss = 0.44333744\n",
      "Iteration 17, loss = 0.42354628\n",
      "Iteration 18, loss = 0.42233765\n",
      "Iteration 19, loss = 0.39140919\n",
      "Iteration 20, loss = 0.37155354\n",
      "Iteration 21, loss = 0.38121334\n",
      "Iteration 22, loss = 0.35000132\n",
      "Iteration 23, loss = 0.32305776\n",
      "Iteration 24, loss = 0.31035134\n",
      "Iteration 25, loss = 0.31137378\n",
      "Iteration 26, loss = 0.30376285\n",
      "Iteration 27, loss = 0.31282328\n",
      "Iteration 28, loss = 0.31073778\n",
      "Iteration 29, loss = 0.28354522\n",
      "Iteration 30, loss = 0.30033182\n",
      "Iteration 31, loss = 0.28493528\n",
      "Iteration 32, loss = 0.29318208\n",
      "Iteration 33, loss = 0.27981201\n",
      "Iteration 34, loss = 0.28437656\n",
      "Iteration 35, loss = 0.28862407\n",
      "Iteration 36, loss = 0.27569425\n",
      "Iteration 37, loss = 0.26242564\n",
      "Iteration 38, loss = 0.25484000\n",
      "Iteration 39, loss = 0.25793353\n",
      "Iteration 40, loss = 0.24777598\n",
      "Iteration 41, loss = 0.24379265\n",
      "Iteration 42, loss = 0.26426721\n",
      "Iteration 43, loss = 0.25478924\n",
      "Iteration 44, loss = 0.25000122\n",
      "Iteration 45, loss = 0.24957869\n",
      "Iteration 46, loss = 0.23808057\n",
      "Iteration 47, loss = 0.23960053\n",
      "Iteration 48, loss = 0.25204887\n",
      "Iteration 49, loss = 0.27454145\n",
      "Iteration 50, loss = 0.26434516\n",
      "Iteration 51, loss = 0.24913574\n",
      "Iteration 52, loss = 0.25056397\n",
      "Iteration 53, loss = 0.24988617\n",
      "Iteration 54, loss = 0.25298494\n",
      "Iteration 55, loss = 0.26030277\n",
      "Iteration 56, loss = 0.27011763\n",
      "Iteration 57, loss = 0.25136026\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.92138709\n",
      "Iteration 2, loss = 3.04912404\n",
      "Iteration 3, loss = 2.38519929\n",
      "Iteration 4, loss = 1.90369598\n",
      "Iteration 5, loss = 1.52913045\n",
      "Iteration 6, loss = 1.25612342\n",
      "Iteration 7, loss = 1.05964349\n",
      "Iteration 8, loss = 0.92432460\n",
      "Iteration 9, loss = 0.81946746\n",
      "Iteration 10, loss = 0.72380401\n",
      "Iteration 11, loss = 0.66126667\n",
      "Iteration 12, loss = 0.58975169\n",
      "Iteration 13, loss = 0.55474620\n",
      "Iteration 14, loss = 0.52102724\n",
      "Iteration 15, loss = 0.47420877\n",
      "Iteration 16, loss = 0.44543069\n",
      "Iteration 17, loss = 0.42666549\n",
      "Iteration 18, loss = 0.40697050\n",
      "Iteration 19, loss = 0.39548807\n",
      "Iteration 20, loss = 0.37909059\n",
      "Iteration 21, loss = 0.38529359\n",
      "Iteration 22, loss = 0.36313086\n",
      "Iteration 23, loss = 0.33452612\n",
      "Iteration 24, loss = 0.32260827\n",
      "Iteration 25, loss = 0.32317394\n",
      "Iteration 26, loss = 0.31447140\n",
      "Iteration 27, loss = 0.32928143\n",
      "Iteration 28, loss = 0.32917920\n",
      "Iteration 29, loss = 0.29948913\n",
      "Iteration 30, loss = 0.30861691\n",
      "Iteration 31, loss = 0.29258112\n",
      "Iteration 32, loss = 0.30310101\n",
      "Iteration 33, loss = 0.29425016\n",
      "Iteration 34, loss = 0.29865151\n",
      "Iteration 35, loss = 0.28008761\n",
      "Iteration 36, loss = 0.27215268\n",
      "Iteration 37, loss = 0.27601203\n",
      "Iteration 38, loss = 0.26479382\n",
      "Iteration 39, loss = 0.27081079\n",
      "Iteration 40, loss = 0.26113767\n",
      "Iteration 41, loss = 0.25491354\n",
      "Iteration 42, loss = 0.27685052\n",
      "Iteration 43, loss = 0.27005727\n",
      "Iteration 44, loss = 0.26142089\n",
      "Iteration 45, loss = 0.26016061\n",
      "Iteration 46, loss = 0.24792213\n",
      "Iteration 47, loss = 0.25115533\n",
      "Iteration 48, loss = 0.26512205\n",
      "Iteration 49, loss = 0.28670005\n",
      "Iteration 50, loss = 0.27647438\n",
      "Iteration 51, loss = 0.25985630\n",
      "Iteration 52, loss = 0.26684653\n",
      "Iteration 53, loss = 0.27498927\n",
      "Iteration 54, loss = 0.26890527\n",
      "Iteration 55, loss = 0.26562533\n",
      "Iteration 56, loss = 0.27782750\n",
      "Iteration 57, loss = 0.25805557\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.11209020\n",
      "Iteration 2, loss = 3.17318895\n",
      "Iteration 3, loss = 2.46168223\n",
      "Iteration 4, loss = 1.94302470\n",
      "Iteration 5, loss = 1.55418064\n",
      "Iteration 6, loss = 1.27143700\n",
      "Iteration 7, loss = 1.06573095\n",
      "Iteration 8, loss = 0.92541624\n",
      "Iteration 9, loss = 0.82215265\n",
      "Iteration 10, loss = 0.72486154\n",
      "Iteration 11, loss = 0.66325384\n",
      "Iteration 12, loss = 0.59151704\n",
      "Iteration 13, loss = 0.55163263\n",
      "Iteration 14, loss = 0.52109121\n",
      "Iteration 15, loss = 0.47839262\n",
      "Iteration 16, loss = 0.44870626\n",
      "Iteration 17, loss = 0.43116010\n",
      "Iteration 18, loss = 0.41298344\n",
      "Iteration 19, loss = 0.39941874\n",
      "Iteration 20, loss = 0.38544845\n",
      "Iteration 21, loss = 0.38314125\n",
      "Iteration 22, loss = 0.36260972\n",
      "Iteration 23, loss = 0.33561888\n",
      "Iteration 24, loss = 0.32729906\n",
      "Iteration 25, loss = 0.32340897\n",
      "Iteration 26, loss = 0.32548961\n",
      "Iteration 27, loss = 0.33724707\n",
      "Iteration 28, loss = 0.33928974\n",
      "Iteration 29, loss = 0.30548487\n",
      "Iteration 30, loss = 0.31322071\n",
      "Iteration 31, loss = 0.30475965\n",
      "Iteration 32, loss = 0.32415660\n",
      "Iteration 33, loss = 0.32941570\n",
      "Iteration 34, loss = 0.32549670\n",
      "Iteration 35, loss = 0.28854355\n",
      "Iteration 36, loss = 0.27797899\n",
      "Iteration 37, loss = 0.28037550\n",
      "Iteration 38, loss = 0.27048134\n",
      "Iteration 39, loss = 0.27961587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40, loss = 0.26568303\n",
      "Iteration 41, loss = 0.26305188\n",
      "Iteration 42, loss = 0.28567120\n",
      "Iteration 43, loss = 0.28090779\n",
      "Iteration 44, loss = 0.27805877\n",
      "Iteration 45, loss = 0.27662289\n",
      "Iteration 46, loss = 0.25899233\n",
      "Iteration 47, loss = 0.26219932\n",
      "Iteration 48, loss = 0.27212989\n",
      "Iteration 49, loss = 0.28749902\n",
      "Iteration 50, loss = 0.28034049\n",
      "Iteration 51, loss = 0.26580045\n",
      "Iteration 52, loss = 0.27242795\n",
      "Iteration 53, loss = 0.27338322\n",
      "Iteration 54, loss = 0.27811648\n",
      "Iteration 55, loss = 0.27333411\n",
      "Iteration 56, loss = 0.28774146\n",
      "Iteration 57, loss = 0.27254479\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.30155857\n",
      "Iteration 2, loss = 3.29230558\n",
      "Iteration 3, loss = 2.52752670\n",
      "Iteration 4, loss = 1.98696034\n",
      "Iteration 5, loss = 1.57647277\n",
      "Iteration 6, loss = 1.28473371\n",
      "Iteration 7, loss = 1.07413442\n",
      "Iteration 8, loss = 0.93042073\n",
      "Iteration 9, loss = 0.82257725\n",
      "Iteration 10, loss = 0.72128029\n",
      "Iteration 11, loss = 0.66545317\n",
      "Iteration 12, loss = 0.59633292\n",
      "Iteration 13, loss = 0.55908664\n",
      "Iteration 14, loss = 0.52740213\n",
      "Iteration 15, loss = 0.48269876\n",
      "Iteration 16, loss = 0.45558277\n",
      "Iteration 17, loss = 0.44197969\n",
      "Iteration 18, loss = 0.41966159\n",
      "Iteration 19, loss = 0.40364845\n",
      "Iteration 20, loss = 0.39351830\n",
      "Iteration 21, loss = 0.39379317\n",
      "Iteration 22, loss = 0.36825694\n",
      "Iteration 23, loss = 0.34466352\n",
      "Iteration 24, loss = 0.33712998\n",
      "Iteration 25, loss = 0.33290213\n",
      "Iteration 26, loss = 0.32653609\n",
      "Iteration 27, loss = 0.34110777\n",
      "Iteration 28, loss = 0.34258353\n",
      "Iteration 29, loss = 0.31552149\n",
      "Iteration 30, loss = 0.32764374\n",
      "Iteration 31, loss = 0.31671605\n",
      "Iteration 32, loss = 0.33635232\n",
      "Iteration 33, loss = 0.33332330\n",
      "Iteration 34, loss = 0.32839489\n",
      "Iteration 35, loss = 0.29764029\n",
      "Iteration 36, loss = 0.29060886\n",
      "Iteration 37, loss = 0.29601566\n",
      "Iteration 38, loss = 0.28253477\n",
      "Iteration 39, loss = 0.28970103\n",
      "Iteration 40, loss = 0.28074886\n",
      "Iteration 41, loss = 0.27656303\n",
      "Iteration 42, loss = 0.30066840\n",
      "Iteration 43, loss = 0.29830496\n",
      "Iteration 44, loss = 0.29230672\n",
      "Iteration 45, loss = 0.29181732\n",
      "Iteration 46, loss = 0.27471177\n",
      "Iteration 47, loss = 0.27470569\n",
      "Iteration 48, loss = 0.27798366\n",
      "Iteration 49, loss = 0.29468848\n",
      "Iteration 50, loss = 0.29343170\n",
      "Iteration 51, loss = 0.27590530\n",
      "Iteration 52, loss = 0.28178717\n",
      "Iteration 53, loss = 0.27146910\n",
      "Iteration 54, loss = 0.27982432\n",
      "Iteration 55, loss = 0.28243891\n",
      "Iteration 56, loss = 0.29272600\n",
      "Iteration 57, loss = 0.27690020\n",
      "Iteration 58, loss = 0.26814759\n",
      "Iteration 59, loss = 0.26160026\n",
      "Iteration 60, loss = 0.26587023\n",
      "Iteration 61, loss = 0.27421044\n",
      "Iteration 62, loss = 0.29373416\n",
      "Iteration 63, loss = 0.29541306\n",
      "Iteration 64, loss = 0.29467436\n",
      "Iteration 65, loss = 0.28114507\n",
      "Iteration 66, loss = 0.28345045\n",
      "Iteration 67, loss = 0.27184518\n",
      "Iteration 68, loss = 0.26672410\n",
      "Iteration 69, loss = 0.26165251\n",
      "Iteration 70, loss = 0.26267184\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.48986456\n",
      "Iteration 2, loss = 3.40991367\n",
      "Iteration 3, loss = 2.59350487\n",
      "Iteration 4, loss = 2.01707811\n",
      "Iteration 5, loss = 1.59115485\n",
      "Iteration 6, loss = 1.28943489\n",
      "Iteration 7, loss = 1.07598330\n",
      "Iteration 8, loss = 0.92991308\n",
      "Iteration 9, loss = 0.82061774\n",
      "Iteration 10, loss = 0.72090684\n",
      "Iteration 11, loss = 0.66326069\n",
      "Iteration 12, loss = 0.58760940\n",
      "Iteration 13, loss = 0.55551487\n",
      "Iteration 14, loss = 0.53355279\n",
      "Iteration 15, loss = 0.48975890\n",
      "Iteration 16, loss = 0.45696794\n",
      "Iteration 17, loss = 0.44359888\n",
      "Iteration 18, loss = 0.41885287\n",
      "Iteration 19, loss = 0.40852251\n",
      "Iteration 20, loss = 0.40257662\n",
      "Iteration 21, loss = 0.40400121\n",
      "Iteration 22, loss = 0.37037911\n",
      "Iteration 23, loss = 0.35129530\n",
      "Iteration 24, loss = 0.34988198\n",
      "Iteration 25, loss = 0.33401832\n",
      "Iteration 26, loss = 0.34004024\n",
      "Iteration 27, loss = 0.35564554\n",
      "Iteration 28, loss = 0.34671447\n",
      "Iteration 29, loss = 0.32107086\n",
      "Iteration 30, loss = 0.32776805\n",
      "Iteration 31, loss = 0.32405232\n",
      "Iteration 32, loss = 0.35377001\n",
      "Iteration 33, loss = 0.34704645\n",
      "Iteration 34, loss = 0.33390089\n",
      "Iteration 35, loss = 0.30835236\n",
      "Iteration 36, loss = 0.29914241\n",
      "Iteration 37, loss = 0.30578227\n",
      "Iteration 38, loss = 0.29361744\n",
      "Iteration 39, loss = 0.30342541\n",
      "Iteration 40, loss = 0.28745224\n",
      "Iteration 41, loss = 0.28473477\n",
      "Iteration 42, loss = 0.30815595\n",
      "Iteration 43, loss = 0.30761328\n",
      "Iteration 44, loss = 0.29847299\n",
      "Iteration 45, loss = 0.30057745\n",
      "Iteration 46, loss = 0.28652693\n",
      "Iteration 47, loss = 0.28684639\n",
      "Iteration 48, loss = 0.28928209\n",
      "Iteration 49, loss = 0.30069766\n",
      "Iteration 50, loss = 0.29974728\n",
      "Iteration 51, loss = 0.28705000\n",
      "Iteration 52, loss = 0.29150639\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.67768829\n",
      "Iteration 2, loss = 3.52719368\n",
      "Iteration 3, loss = 2.66279507\n",
      "Iteration 4, loss = 2.05358121\n",
      "Iteration 5, loss = 1.60940192\n",
      "Iteration 6, loss = 1.29634565\n",
      "Iteration 7, loss = 1.07874636\n",
      "Iteration 8, loss = 0.92716889\n",
      "Iteration 9, loss = 0.82321982\n",
      "Iteration 10, loss = 0.71867223\n",
      "Iteration 11, loss = 0.65835819\n",
      "Iteration 12, loss = 0.58878112\n",
      "Iteration 13, loss = 0.56681417\n",
      "Iteration 14, loss = 0.53237733\n",
      "Iteration 15, loss = 0.49431688\n",
      "Iteration 16, loss = 0.46170095\n",
      "Iteration 17, loss = 0.45150766\n",
      "Iteration 18, loss = 0.42262782\n",
      "Iteration 19, loss = 0.41185242\n",
      "Iteration 20, loss = 0.40381762\n",
      "Iteration 21, loss = 0.40004483\n",
      "Iteration 22, loss = 0.38468112\n",
      "Iteration 23, loss = 0.36162574\n",
      "Iteration 24, loss = 0.35590172\n",
      "Iteration 25, loss = 0.34549377\n",
      "Iteration 26, loss = 0.35077333\n",
      "Iteration 27, loss = 0.36507414\n",
      "Iteration 28, loss = 0.35727130\n",
      "Iteration 29, loss = 0.33321318\n",
      "Iteration 30, loss = 0.33721715\n",
      "Iteration 31, loss = 0.33615246\n",
      "Iteration 32, loss = 0.35975745\n",
      "Iteration 33, loss = 0.35432155\n",
      "Iteration 34, loss = 0.35021177\n",
      "Iteration 35, loss = 0.32116714\n",
      "Iteration 36, loss = 0.31405743\n",
      "Iteration 37, loss = 0.32344258\n",
      "Iteration 38, loss = 0.30705611\n",
      "Iteration 39, loss = 0.31325647\n",
      "Iteration 40, loss = 0.29925872\n",
      "Iteration 41, loss = 0.29792827\n",
      "Iteration 42, loss = 0.32119101\n",
      "Iteration 43, loss = 0.31419398\n",
      "Iteration 44, loss = 0.30740957\n",
      "Iteration 45, loss = 0.30862050\n",
      "Iteration 46, loss = 0.29909781\n",
      "Iteration 47, loss = 0.29744807\n",
      "Iteration 48, loss = 0.29937820\n",
      "Iteration 49, loss = 0.30828458\n",
      "Iteration 50, loss = 0.30947522\n",
      "Iteration 51, loss = 0.29408370\n",
      "Iteration 52, loss = 0.30336655\n",
      "Iteration 53, loss = 0.29203233\n",
      "Iteration 54, loss = 0.31341136\n",
      "Iteration 55, loss = 0.30191639\n",
      "Iteration 56, loss = 0.31540553\n",
      "Iteration 57, loss = 0.30265784\n",
      "Iteration 58, loss = 0.29381434\n",
      "Iteration 59, loss = 0.28525883\n",
      "Iteration 60, loss = 0.28565513\n",
      "Iteration 61, loss = 0.29243227\n",
      "Iteration 62, loss = 0.30369950\n",
      "Iteration 63, loss = 0.30760152\n",
      "Iteration 64, loss = 0.30288479\n",
      "Iteration 65, loss = 0.29856170\n",
      "Iteration 66, loss = 0.30738100\n",
      "Iteration 67, loss = 0.29904485\n",
      "Iteration 68, loss = 0.29213551\n",
      "Iteration 69, loss = 0.28872694\n",
      "Iteration 70, loss = 0.28899902\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.86385227\n",
      "Iteration 2, loss = 3.63930587\n",
      "Iteration 3, loss = 2.72300373\n",
      "Iteration 4, loss = 2.08371811\n",
      "Iteration 5, loss = 1.62714973\n",
      "Iteration 6, loss = 1.30801272\n",
      "Iteration 7, loss = 1.08501972\n",
      "Iteration 8, loss = 0.93715264\n",
      "Iteration 9, loss = 0.82124753\n",
      "Iteration 10, loss = 0.72135106\n",
      "Iteration 11, loss = 0.66283453\n",
      "Iteration 12, loss = 0.59775561\n",
      "Iteration 13, loss = 0.56400199\n",
      "Iteration 14, loss = 0.53378128\n",
      "Iteration 15, loss = 0.49175468\n",
      "Iteration 16, loss = 0.46776437\n",
      "Iteration 17, loss = 0.46022867\n",
      "Iteration 18, loss = 0.43262044\n",
      "Iteration 19, loss = 0.42102776\n",
      "Iteration 20, loss = 0.41054381\n",
      "Iteration 21, loss = 0.40335023\n",
      "Iteration 22, loss = 0.39054587\n",
      "Iteration 23, loss = 0.36821356\n",
      "Iteration 24, loss = 0.36842091\n",
      "Iteration 25, loss = 0.35337925\n",
      "Iteration 26, loss = 0.35597565\n",
      "Iteration 27, loss = 0.36989942\n",
      "Iteration 28, loss = 0.34569210\n",
      "Iteration 29, loss = 0.33511402\n",
      "Iteration 30, loss = 0.34452978\n",
      "Iteration 31, loss = 0.34117121\n",
      "Iteration 32, loss = 0.36246654\n",
      "Iteration 33, loss = 0.37629797\n",
      "Iteration 34, loss = 0.34588685\n",
      "Iteration 35, loss = 0.32768937\n",
      "Iteration 36, loss = 0.31813533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37, loss = 0.31918864\n",
      "Iteration 38, loss = 0.31159808\n",
      "Iteration 39, loss = 0.31877734\n",
      "Iteration 40, loss = 0.30863508\n",
      "Iteration 41, loss = 0.30545754\n",
      "Iteration 42, loss = 0.33132882\n",
      "Iteration 43, loss = 0.33141360\n",
      "Iteration 44, loss = 0.31692236\n",
      "Iteration 45, loss = 0.32166628\n",
      "Iteration 46, loss = 0.31061370\n",
      "Iteration 47, loss = 0.30760172\n",
      "Iteration 48, loss = 0.31002935\n",
      "Iteration 49, loss = 0.30958500\n",
      "Iteration 50, loss = 0.31460938\n",
      "Iteration 51, loss = 0.30004012\n",
      "Iteration 52, loss = 0.30600853\n",
      "Iteration 53, loss = 0.29598727\n",
      "Iteration 54, loss = 0.31109837\n",
      "Iteration 55, loss = 0.31064688\n",
      "Iteration 56, loss = 0.32363717\n",
      "Iteration 57, loss = 0.31039190\n",
      "Iteration 58, loss = 0.30111269\n",
      "Iteration 59, loss = 0.29517772\n",
      "Iteration 60, loss = 0.29923802\n",
      "Iteration 61, loss = 0.30257164\n",
      "Iteration 62, loss = 0.30580152\n",
      "Iteration 63, loss = 0.30818341\n",
      "Iteration 64, loss = 0.30914019\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.04895186\n",
      "Iteration 2, loss = 3.75101545\n",
      "Iteration 3, loss = 2.78334213\n",
      "Iteration 4, loss = 2.11859117\n",
      "Iteration 5, loss = 1.64130650\n",
      "Iteration 6, loss = 1.31386005\n",
      "Iteration 7, loss = 1.08578346\n",
      "Iteration 8, loss = 0.93547534\n",
      "Iteration 9, loss = 0.82419912\n",
      "Iteration 10, loss = 0.72291398\n",
      "Iteration 11, loss = 0.66759207\n",
      "Iteration 12, loss = 0.60218576\n",
      "Iteration 13, loss = 0.56506504\n",
      "Iteration 14, loss = 0.52825826\n",
      "Iteration 15, loss = 0.49450673\n",
      "Iteration 16, loss = 0.46766081\n",
      "Iteration 17, loss = 0.46074122\n",
      "Iteration 18, loss = 0.43827531\n",
      "Iteration 19, loss = 0.42992235\n",
      "Iteration 20, loss = 0.41870796\n",
      "Iteration 21, loss = 0.40876884\n",
      "Iteration 22, loss = 0.40001838\n",
      "Iteration 23, loss = 0.37840008\n",
      "Iteration 24, loss = 0.37924819\n",
      "Iteration 25, loss = 0.36326378\n",
      "Iteration 26, loss = 0.36789667\n",
      "Iteration 27, loss = 0.37961600\n",
      "Iteration 28, loss = 0.35189438\n",
      "Iteration 29, loss = 0.34493821\n",
      "Iteration 30, loss = 0.35373121\n",
      "Iteration 31, loss = 0.34672181\n",
      "Iteration 32, loss = 0.37513708\n",
      "Iteration 33, loss = 0.38572863\n",
      "Iteration 34, loss = 0.36032485\n",
      "Iteration 35, loss = 0.33885198\n",
      "Iteration 36, loss = 0.32633713\n",
      "Iteration 37, loss = 0.33115566\n",
      "Iteration 38, loss = 0.32225216\n",
      "Iteration 39, loss = 0.32908627\n",
      "Iteration 40, loss = 0.31621323\n",
      "Iteration 41, loss = 0.31755563\n",
      "Iteration 42, loss = 0.34856389\n",
      "Iteration 43, loss = 0.33044890\n",
      "Iteration 44, loss = 0.33023513\n",
      "Iteration 45, loss = 0.33728636\n",
      "Iteration 46, loss = 0.33281740\n",
      "Iteration 47, loss = 0.32329938\n",
      "Iteration 48, loss = 0.31741532\n",
      "Iteration 49, loss = 0.31593923\n",
      "Iteration 50, loss = 0.32307278\n",
      "Iteration 51, loss = 0.30819894\n",
      "Iteration 52, loss = 0.31448526\n",
      "Iteration 53, loss = 0.30886083\n",
      "Iteration 54, loss = 0.32282929\n",
      "Iteration 55, loss = 0.31656613\n",
      "Iteration 56, loss = 0.33550976\n",
      "Iteration 57, loss = 0.32927771\n",
      "Iteration 58, loss = 0.31569444\n",
      "Iteration 59, loss = 0.30654899\n",
      "Iteration 60, loss = 0.30753487\n",
      "Iteration 61, loss = 0.31512664\n",
      "Iteration 62, loss = 0.31281577\n",
      "Iteration 63, loss = 0.31827919\n",
      "Iteration 64, loss = 0.32119611\n",
      "Iteration 65, loss = 0.32035630\n",
      "Iteration 66, loss = 0.31922887\n",
      "Iteration 67, loss = 0.31409466\n",
      "Iteration 68, loss = 0.31093281\n",
      "Iteration 69, loss = 0.30590219\n",
      "Iteration 70, loss = 0.30444279\n",
      "Iteration 71, loss = 0.29967607\n",
      "Iteration 72, loss = 0.31340389\n",
      "Iteration 73, loss = 0.30922897\n",
      "Iteration 74, loss = 0.31205008\n",
      "Iteration 75, loss = 0.30809108\n",
      "Iteration 76, loss = 0.30084489\n",
      "Iteration 77, loss = 0.30764735\n",
      "Iteration 78, loss = 0.31912803\n",
      "Iteration 79, loss = 0.31191528\n",
      "Iteration 80, loss = 0.33046401\n",
      "Iteration 81, loss = 0.31067917\n",
      "Iteration 82, loss = 0.30712091\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.23381154\n",
      "Iteration 2, loss = 3.86223332\n",
      "Iteration 3, loss = 2.84342741\n",
      "Iteration 4, loss = 2.15087767\n",
      "Iteration 5, loss = 1.65974449\n",
      "Iteration 6, loss = 1.32734174\n",
      "Iteration 7, loss = 1.09360648\n",
      "Iteration 8, loss = 0.93883743\n",
      "Iteration 9, loss = 0.81978165\n",
      "Iteration 10, loss = 0.72230281\n",
      "Iteration 11, loss = 0.66963430\n",
      "Iteration 12, loss = 0.60660443\n",
      "Iteration 13, loss = 0.56436152\n",
      "Iteration 14, loss = 0.53176953\n",
      "Iteration 15, loss = 0.49643477\n",
      "Iteration 16, loss = 0.47387374\n",
      "Iteration 17, loss = 0.46866395\n",
      "Iteration 18, loss = 0.44828624\n",
      "Iteration 19, loss = 0.43544165\n",
      "Iteration 20, loss = 0.42080762\n",
      "Iteration 21, loss = 0.41602693\n",
      "Iteration 22, loss = 0.40729081\n",
      "Iteration 23, loss = 0.38267583\n",
      "Iteration 24, loss = 0.38641859\n",
      "Iteration 25, loss = 0.37273677\n",
      "Iteration 26, loss = 0.37672378\n",
      "Iteration 27, loss = 0.38039560\n",
      "Iteration 28, loss = 0.35955949\n",
      "Iteration 29, loss = 0.35301943\n",
      "Iteration 30, loss = 0.36084826\n",
      "Iteration 31, loss = 0.35798977\n",
      "Iteration 32, loss = 0.37924603\n",
      "Iteration 33, loss = 0.38171950\n",
      "Iteration 34, loss = 0.37146849\n",
      "Iteration 35, loss = 0.35063517\n",
      "Iteration 36, loss = 0.33684569\n",
      "Iteration 37, loss = 0.34430825\n",
      "Iteration 38, loss = 0.33557140\n",
      "Iteration 39, loss = 0.34045231\n",
      "Iteration 40, loss = 0.32564049\n",
      "Iteration 41, loss = 0.32788702\n",
      "Iteration 42, loss = 0.35712691\n",
      "Iteration 43, loss = 0.33882901\n",
      "Iteration 44, loss = 0.34487929\n",
      "Iteration 45, loss = 0.35461300\n",
      "Iteration 46, loss = 0.34825964\n",
      "Iteration 47, loss = 0.33231932\n",
      "Iteration 48, loss = 0.32850091\n",
      "Iteration 49, loss = 0.32583490\n",
      "Iteration 50, loss = 0.33114426\n",
      "Iteration 51, loss = 0.31724182\n",
      "Iteration 52, loss = 0.32429489\n",
      "Iteration 53, loss = 0.31890776\n",
      "Iteration 54, loss = 0.33674888\n",
      "Iteration 55, loss = 0.32529983\n",
      "Iteration 56, loss = 0.35019941\n",
      "Iteration 57, loss = 0.34272509\n",
      "Iteration 58, loss = 0.32947051\n",
      "Iteration 59, loss = 0.31828590\n",
      "Iteration 60, loss = 0.31906251\n",
      "Iteration 61, loss = 0.32203670\n",
      "Iteration 62, loss = 0.31949058\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.41765346\n",
      "Iteration 2, loss = 3.96785930\n",
      "Iteration 3, loss = 2.89914705\n",
      "Iteration 4, loss = 2.17574671\n",
      "Iteration 5, loss = 1.67123755\n",
      "Iteration 6, loss = 1.33104185\n",
      "Iteration 7, loss = 1.09495549\n",
      "Iteration 8, loss = 0.94113839\n",
      "Iteration 9, loss = 0.82025674\n",
      "Iteration 10, loss = 0.72145413\n",
      "Iteration 11, loss = 0.66650047\n",
      "Iteration 12, loss = 0.61098599\n",
      "Iteration 13, loss = 0.57291682\n",
      "Iteration 14, loss = 0.53449603\n",
      "Iteration 15, loss = 0.50432924\n",
      "Iteration 16, loss = 0.48038646\n",
      "Iteration 17, loss = 0.46830816\n",
      "Iteration 18, loss = 0.44514525\n",
      "Iteration 19, loss = 0.43813695\n",
      "Iteration 20, loss = 0.43019026\n",
      "Iteration 21, loss = 0.41778113\n",
      "Iteration 22, loss = 0.41110600\n",
      "Iteration 23, loss = 0.38715302\n",
      "Iteration 24, loss = 0.39274225\n",
      "Iteration 25, loss = 0.38510940\n",
      "Iteration 26, loss = 0.38524196\n",
      "Iteration 27, loss = 0.38972014\n",
      "Iteration 28, loss = 0.36986132\n",
      "Iteration 29, loss = 0.36091559\n",
      "Iteration 30, loss = 0.36633259\n",
      "Iteration 31, loss = 0.36449735\n",
      "Iteration 32, loss = 0.38850430\n",
      "Iteration 33, loss = 0.38929830\n",
      "Iteration 34, loss = 0.38207452\n",
      "Iteration 35, loss = 0.35760726\n",
      "Iteration 36, loss = 0.34750076\n",
      "Iteration 37, loss = 0.35096836\n",
      "Iteration 38, loss = 0.34193548\n",
      "Iteration 39, loss = 0.34704177\n",
      "Iteration 40, loss = 0.33459543\n",
      "Iteration 41, loss = 0.33693212\n",
      "Iteration 42, loss = 0.36209217\n",
      "Iteration 43, loss = 0.34607560\n",
      "Iteration 44, loss = 0.35564972\n",
      "Iteration 45, loss = 0.35540824\n",
      "Iteration 46, loss = 0.35227849\n",
      "Iteration 47, loss = 0.33896797\n",
      "Iteration 48, loss = 0.33368523\n",
      "Iteration 49, loss = 0.33018507\n",
      "Iteration 50, loss = 0.33752067\n",
      "Iteration 51, loss = 0.32548607\n",
      "Iteration 52, loss = 0.33419688\n",
      "Iteration 53, loss = 0.32528853\n",
      "Iteration 54, loss = 0.34019005\n",
      "Iteration 55, loss = 0.33571380\n",
      "Iteration 56, loss = 0.35562903\n",
      "Iteration 57, loss = 0.34879255\n",
      "Iteration 58, loss = 0.33625567\n",
      "Iteration 59, loss = 0.32771335\n",
      "Iteration 60, loss = 0.32999935\n",
      "Iteration 61, loss = 0.33582965\n",
      "Iteration 62, loss = 0.33462724\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65299290\n",
      "Iteration 2, loss = 0.43189822\n",
      "Iteration 3, loss = 0.37347053\n",
      "Iteration 4, loss = 0.31221855\n",
      "Iteration 5, loss = 0.26094373\n",
      "Iteration 6, loss = 0.21526122\n",
      "Iteration 7, loss = 0.17941168\n",
      "Iteration 8, loss = 0.14103339\n",
      "Iteration 9, loss = 0.12162078\n",
      "Iteration 10, loss = 0.14970778\n",
      "Iteration 11, loss = 0.10061794\n",
      "Iteration 12, loss = 0.06876051\n",
      "Iteration 13, loss = 0.05240769\n",
      "Iteration 14, loss = 0.03400678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.02535611\n",
      "Iteration 16, loss = 0.01977342\n",
      "Iteration 17, loss = 0.01919462\n",
      "Iteration 18, loss = 0.01260493\n",
      "Iteration 19, loss = 0.00800426\n",
      "Iteration 20, loss = 0.00663997\n",
      "Iteration 21, loss = 0.00498341\n",
      "Iteration 22, loss = 0.00396149\n",
      "Iteration 23, loss = 0.00339242\n",
      "Iteration 24, loss = 0.00294522\n",
      "Iteration 25, loss = 0.00248205\n",
      "Iteration 26, loss = 0.00226245\n",
      "Iteration 27, loss = 0.00194231\n",
      "Iteration 28, loss = 0.00175393\n",
      "Iteration 29, loss = 0.00151968\n",
      "Iteration 30, loss = 0.00145691\n",
      "Iteration 31, loss = 0.00133131\n",
      "Iteration 32, loss = 0.00115302\n",
      "Iteration 33, loss = 0.00106233\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.87049775\n",
      "Iteration 2, loss = 0.64754877\n",
      "Iteration 3, loss = 0.58597646\n",
      "Iteration 4, loss = 0.52266774\n",
      "Iteration 5, loss = 0.46896651\n",
      "Iteration 6, loss = 0.41997967\n",
      "Iteration 7, loss = 0.38541230\n",
      "Iteration 8, loss = 0.34233367\n",
      "Iteration 9, loss = 0.32006040\n",
      "Iteration 10, loss = 0.33727146\n",
      "Iteration 11, loss = 0.29105737\n",
      "Iteration 12, loss = 0.25467652\n",
      "Iteration 13, loss = 0.23767220\n",
      "Iteration 14, loss = 0.21552077\n",
      "Iteration 15, loss = 0.20595655\n",
      "Iteration 16, loss = 0.19640478\n",
      "Iteration 17, loss = 0.19132022\n",
      "Iteration 18, loss = 0.17887681\n",
      "Iteration 19, loss = 0.17816750\n",
      "Iteration 20, loss = 0.16574501\n",
      "Iteration 21, loss = 0.16005584\n",
      "Iteration 22, loss = 0.15422449\n",
      "Iteration 23, loss = 0.14931906\n",
      "Iteration 24, loss = 0.14517900\n",
      "Iteration 25, loss = 0.14105899\n",
      "Iteration 26, loss = 0.13693933\n",
      "Iteration 27, loss = 0.13297652\n",
      "Iteration 28, loss = 0.12908541\n",
      "Iteration 29, loss = 0.12579689\n",
      "Iteration 30, loss = 0.12201806\n",
      "Iteration 31, loss = 0.11867054\n",
      "Iteration 32, loss = 0.11540212\n",
      "Iteration 33, loss = 0.11261397\n",
      "Iteration 34, loss = 0.10965255\n",
      "Iteration 35, loss = 0.10666971\n",
      "Iteration 36, loss = 0.10364923\n",
      "Iteration 37, loss = 0.10101243\n",
      "Iteration 38, loss = 0.09860834\n",
      "Iteration 39, loss = 0.09589786\n",
      "Iteration 40, loss = 0.09320839\n",
      "Iteration 41, loss = 0.09107039\n",
      "Iteration 42, loss = 0.08843309\n",
      "Iteration 43, loss = 0.08617985\n",
      "Iteration 44, loss = 0.08390581\n",
      "Iteration 45, loss = 0.08204403\n",
      "Iteration 46, loss = 0.08020661\n",
      "Iteration 47, loss = 0.07857522\n",
      "Iteration 48, loss = 0.07599843\n",
      "Iteration 49, loss = 0.07443194\n",
      "Iteration 50, loss = 0.11841927\n",
      "Iteration 51, loss = 0.35353269\n",
      "Iteration 52, loss = 0.29018456\n",
      "Iteration 53, loss = 0.19935152\n",
      "Iteration 54, loss = 0.14282488\n",
      "Iteration 55, loss = 0.11394793\n",
      "Iteration 56, loss = 0.09832231\n",
      "Iteration 57, loss = 0.09114156\n",
      "Iteration 58, loss = 0.08782195\n",
      "Iteration 59, loss = 0.08478185\n",
      "Iteration 60, loss = 0.08236462\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08598376\n",
      "Iteration 2, loss = 0.85335194\n",
      "Iteration 3, loss = 0.77849279\n",
      "Iteration 4, loss = 0.70319292\n",
      "Iteration 5, loss = 0.64176818\n",
      "Iteration 6, loss = 0.58145923\n",
      "Iteration 7, loss = 0.53484565\n",
      "Iteration 8, loss = 0.48517768\n",
      "Iteration 9, loss = 0.45612869\n",
      "Iteration 10, loss = 0.46616286\n",
      "Iteration 11, loss = 0.41959935\n",
      "Iteration 12, loss = 0.36950075\n",
      "Iteration 13, loss = 0.34466923\n",
      "Iteration 14, loss = 0.31689977\n",
      "Iteration 15, loss = 0.30062723\n",
      "Iteration 16, loss = 0.28617849\n",
      "Iteration 17, loss = 0.28634895\n",
      "Iteration 18, loss = 0.26608464\n",
      "Iteration 19, loss = 0.25822620\n",
      "Iteration 20, loss = 0.24650118\n",
      "Iteration 21, loss = 0.23037751\n",
      "Iteration 22, loss = 0.21681727\n",
      "Iteration 23, loss = 0.20601170\n",
      "Iteration 24, loss = 0.19830755\n",
      "Iteration 25, loss = 0.19064963\n",
      "Iteration 26, loss = 0.18327296\n",
      "Iteration 27, loss = 0.17609841\n",
      "Iteration 28, loss = 0.16939270\n",
      "Iteration 29, loss = 0.16349040\n",
      "Iteration 30, loss = 0.15743409\n",
      "Iteration 31, loss = 0.15164034\n",
      "Iteration 32, loss = 0.14623410\n",
      "Iteration 33, loss = 0.14161410\n",
      "Iteration 34, loss = 0.13731668\n",
      "Iteration 35, loss = 0.13671163\n",
      "Iteration 36, loss = 0.13543067\n",
      "Iteration 37, loss = 0.13140349\n",
      "Iteration 38, loss = 0.15703544\n",
      "Iteration 39, loss = 0.18267900\n",
      "Iteration 40, loss = 0.17130776\n",
      "Iteration 41, loss = 0.17922417\n",
      "Iteration 42, loss = 0.16425249\n",
      "Iteration 43, loss = 0.15345805\n",
      "Iteration 44, loss = 0.13717185\n",
      "Iteration 45, loss = 0.12791616\n",
      "Iteration 46, loss = 0.11896478\n",
      "Iteration 47, loss = 0.11365761\n",
      "Iteration 48, loss = 0.10897670\n",
      "Iteration 49, loss = 0.10541475\n",
      "Iteration 50, loss = 0.10201430\n",
      "Iteration 51, loss = 0.09864636\n",
      "Iteration 52, loss = 0.09595315\n",
      "Iteration 53, loss = 0.09304417\n",
      "Iteration 54, loss = 0.09053155\n",
      "Iteration 55, loss = 0.08872031\n",
      "Iteration 56, loss = 0.08653764\n",
      "Iteration 57, loss = 0.08407548\n",
      "Iteration 58, loss = 0.08224350\n",
      "Iteration 59, loss = 0.08048158\n",
      "Iteration 60, loss = 0.07869944\n",
      "Iteration 61, loss = 0.07681915\n",
      "Iteration 62, loss = 0.07531033\n",
      "Iteration 63, loss = 0.07404414\n",
      "Iteration 64, loss = 0.07305570\n",
      "Iteration 65, loss = 0.07109927\n",
      "Iteration 66, loss = 0.07006920\n",
      "Iteration 67, loss = 0.06904987\n",
      "Iteration 68, loss = 0.07447153\n",
      "Iteration 69, loss = 0.11623278\n",
      "Iteration 70, loss = 0.17536346\n",
      "Iteration 71, loss = 0.21772539\n",
      "Iteration 72, loss = 0.20781701\n",
      "Iteration 73, loss = 0.16981728\n",
      "Iteration 74, loss = 0.11846285\n",
      "Iteration 75, loss = 0.10683960\n",
      "Iteration 76, loss = 0.09541023\n",
      "Iteration 77, loss = 0.09059126\n",
      "Iteration 78, loss = 0.08703569\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.29928818\n",
      "Iteration 2, loss = 1.04944170\n",
      "Iteration 3, loss = 0.95698585\n",
      "Iteration 4, loss = 0.86725090\n",
      "Iteration 5, loss = 0.78403266\n",
      "Iteration 6, loss = 0.71197897\n",
      "Iteration 7, loss = 0.65315473\n",
      "Iteration 8, loss = 0.59331851\n",
      "Iteration 9, loss = 0.55585147\n",
      "Iteration 10, loss = 0.56743420\n",
      "Iteration 11, loss = 0.49919437\n",
      "Iteration 12, loss = 0.44166073\n",
      "Iteration 13, loss = 0.41201659\n",
      "Iteration 14, loss = 0.37711745\n",
      "Iteration 15, loss = 0.35743051\n",
      "Iteration 16, loss = 0.33981867\n",
      "Iteration 17, loss = 0.34882678\n",
      "Iteration 18, loss = 0.31420712\n",
      "Iteration 19, loss = 0.28729933\n",
      "Iteration 20, loss = 0.27659163\n",
      "Iteration 21, loss = 0.25940372\n",
      "Iteration 22, loss = 0.24460864\n",
      "Iteration 23, loss = 0.23268350\n",
      "Iteration 24, loss = 0.22187073\n",
      "Iteration 25, loss = 0.21171788\n",
      "Iteration 26, loss = 0.20393873\n",
      "Iteration 27, loss = 0.19495305\n",
      "Iteration 28, loss = 0.18661922\n",
      "Iteration 29, loss = 0.18414853\n",
      "Iteration 30, loss = 0.18425900\n",
      "Iteration 31, loss = 0.17547924\n",
      "Iteration 32, loss = 0.17118365\n",
      "Iteration 33, loss = 0.17242235\n",
      "Iteration 34, loss = 0.17084966\n",
      "Iteration 35, loss = 0.21754698\n",
      "Iteration 36, loss = 0.27470572\n",
      "Iteration 37, loss = 0.23457556\n",
      "Iteration 38, loss = 0.20728222\n",
      "Iteration 39, loss = 0.17738634\n",
      "Iteration 40, loss = 0.16606356\n",
      "Iteration 41, loss = 0.17580590\n",
      "Iteration 42, loss = 0.16136648\n",
      "Iteration 43, loss = 0.14413732\n",
      "Iteration 44, loss = 0.13581440\n",
      "Iteration 45, loss = 0.13084161\n",
      "Iteration 46, loss = 0.12574434\n",
      "Iteration 47, loss = 0.12161141\n",
      "Iteration 48, loss = 0.11733924\n",
      "Iteration 49, loss = 0.11371568\n",
      "Iteration 50, loss = 0.11051576\n",
      "Iteration 51, loss = 0.10751679\n",
      "Iteration 52, loss = 0.10464089\n",
      "Iteration 53, loss = 0.10214651\n",
      "Iteration 54, loss = 0.09977562\n",
      "Iteration 55, loss = 0.09908568\n",
      "Iteration 56, loss = 0.09622190\n",
      "Iteration 57, loss = 0.09374526\n",
      "Iteration 58, loss = 0.09154790\n",
      "Iteration 59, loss = 0.09088974\n",
      "Iteration 60, loss = 0.09001105\n",
      "Iteration 61, loss = 0.08778684\n",
      "Iteration 62, loss = 0.08638018\n",
      "Iteration 63, loss = 0.08433759\n",
      "Iteration 64, loss = 0.08389648\n",
      "Iteration 65, loss = 0.08318306\n",
      "Iteration 66, loss = 0.08139454\n",
      "Iteration 67, loss = 0.08002866\n",
      "Iteration 68, loss = 0.08442131\n",
      "Iteration 69, loss = 0.09120183\n",
      "Iteration 70, loss = 0.22071845\n",
      "Iteration 71, loss = 0.21974145\n",
      "Iteration 72, loss = 0.16208440\n",
      "Iteration 73, loss = 0.12333900\n",
      "Iteration 74, loss = 0.10826240\n",
      "Iteration 75, loss = 0.09747064\n",
      "Iteration 76, loss = 0.09249309\n",
      "Iteration 77, loss = 0.08931457\n",
      "Iteration 78, loss = 0.08809759\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.51034304\n",
      "Iteration 2, loss = 1.24002636\n",
      "Iteration 3, loss = 1.12230635\n",
      "Iteration 4, loss = 1.01004045\n",
      "Iteration 5, loss = 0.90528086\n",
      "Iteration 6, loss = 0.81523837\n",
      "Iteration 7, loss = 0.74459844\n",
      "Iteration 8, loss = 0.67351362\n",
      "Iteration 9, loss = 0.62380476\n",
      "Iteration 10, loss = 0.61235323\n",
      "Iteration 11, loss = 0.54985046\n",
      "Iteration 12, loss = 0.49107732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.45560341\n",
      "Iteration 14, loss = 0.41673325\n",
      "Iteration 15, loss = 0.39224196\n",
      "Iteration 16, loss = 0.38306772\n",
      "Iteration 17, loss = 0.36790342\n",
      "Iteration 18, loss = 0.34735317\n",
      "Iteration 19, loss = 0.31646020\n",
      "Iteration 20, loss = 0.29524670\n",
      "Iteration 21, loss = 0.27506261\n",
      "Iteration 22, loss = 0.25928583\n",
      "Iteration 23, loss = 0.24702060\n",
      "Iteration 24, loss = 0.23657182\n",
      "Iteration 25, loss = 0.22509442\n",
      "Iteration 26, loss = 0.21769078\n",
      "Iteration 27, loss = 0.20563381\n",
      "Iteration 28, loss = 0.19784605\n",
      "Iteration 29, loss = 0.22202627\n",
      "Iteration 30, loss = 0.31470905\n",
      "Iteration 31, loss = 0.26594069\n",
      "Iteration 32, loss = 0.22254629\n",
      "Iteration 33, loss = 0.19750057\n",
      "Iteration 34, loss = 0.18290233\n",
      "Iteration 35, loss = 0.17198010\n",
      "Iteration 36, loss = 0.16671408\n",
      "Iteration 37, loss = 0.15898500\n",
      "Iteration 38, loss = 0.15265457\n",
      "Iteration 39, loss = 0.14787213\n",
      "Iteration 40, loss = 0.14241959\n",
      "Iteration 41, loss = 0.14180352\n",
      "Iteration 42, loss = 0.13763901\n",
      "Iteration 43, loss = 0.14338216\n",
      "Iteration 44, loss = 0.13075095\n",
      "Iteration 45, loss = 0.12695267\n",
      "Iteration 46, loss = 0.12303448\n",
      "Iteration 47, loss = 0.12365929\n",
      "Iteration 48, loss = 0.12036891\n",
      "Iteration 49, loss = 0.11900859\n",
      "Iteration 50, loss = 0.11899197\n",
      "Iteration 51, loss = 0.12807126\n",
      "Iteration 52, loss = 0.16669228\n",
      "Iteration 53, loss = 0.19853841\n",
      "Iteration 54, loss = 0.16455183\n",
      "Iteration 55, loss = 0.14388725\n",
      "Iteration 56, loss = 0.14597274\n",
      "Iteration 57, loss = 0.13886175\n",
      "Iteration 58, loss = 0.12420196\n",
      "Iteration 59, loss = 0.11810712\n",
      "Iteration 60, loss = 0.11091643\n",
      "Iteration 61, loss = 0.10622525\n",
      "Iteration 62, loss = 0.10276682\n",
      "Iteration 63, loss = 0.10038418\n",
      "Iteration 64, loss = 0.09837812\n",
      "Iteration 65, loss = 0.09765091\n",
      "Iteration 66, loss = 0.09519217\n",
      "Iteration 67, loss = 0.09491963\n",
      "Iteration 68, loss = 0.09366215\n",
      "Iteration 69, loss = 0.09727364\n",
      "Iteration 70, loss = 0.09748630\n",
      "Iteration 71, loss = 0.09256808\n",
      "Iteration 72, loss = 0.09107057\n",
      "Iteration 73, loss = 0.08984215\n",
      "Iteration 74, loss = 0.08800373\n",
      "Iteration 75, loss = 0.08701898\n",
      "Iteration 76, loss = 0.08605878\n",
      "Iteration 77, loss = 0.08570952\n",
      "Iteration 78, loss = 0.08442393\n",
      "Iteration 79, loss = 0.08534440\n",
      "Iteration 80, loss = 0.08322278\n",
      "Iteration 81, loss = 0.08323314\n",
      "Iteration 82, loss = 0.08304939\n",
      "Iteration 83, loss = 0.08443680\n",
      "Iteration 84, loss = 0.08227381\n",
      "Iteration 85, loss = 0.08296570\n",
      "Iteration 86, loss = 0.08244194\n",
      "Iteration 87, loss = 0.08140194\n",
      "Iteration 88, loss = 0.08089974\n",
      "Iteration 89, loss = 0.08515766\n",
      "Iteration 90, loss = 0.10760368\n",
      "Iteration 91, loss = 0.46159938\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.71951874\n",
      "Iteration 2, loss = 1.42055618\n",
      "Iteration 3, loss = 1.27236187\n",
      "Iteration 4, loss = 1.12977356\n",
      "Iteration 5, loss = 1.00906927\n",
      "Iteration 6, loss = 0.90101567\n",
      "Iteration 7, loss = 0.81485252\n",
      "Iteration 8, loss = 0.73496047\n",
      "Iteration 9, loss = 0.67497013\n",
      "Iteration 10, loss = 0.65286856\n",
      "Iteration 11, loss = 0.58898515\n",
      "Iteration 12, loss = 0.52501014\n",
      "Iteration 13, loss = 0.48584018\n",
      "Iteration 14, loss = 0.44404633\n",
      "Iteration 15, loss = 0.41457173\n",
      "Iteration 16, loss = 0.40426247\n",
      "Iteration 17, loss = 0.39504236\n",
      "Iteration 18, loss = 0.36359641\n",
      "Iteration 19, loss = 0.32946591\n",
      "Iteration 20, loss = 0.31134773\n",
      "Iteration 21, loss = 0.29155462\n",
      "Iteration 22, loss = 0.28180217\n",
      "Iteration 23, loss = 0.26068320\n",
      "Iteration 24, loss = 0.25372829\n",
      "Iteration 25, loss = 0.24282537\n",
      "Iteration 26, loss = 0.23753884\n",
      "Iteration 27, loss = 0.22186783\n",
      "Iteration 28, loss = 0.20727138\n",
      "Iteration 29, loss = 0.22415602\n",
      "Iteration 30, loss = 0.28043318\n",
      "Iteration 31, loss = 0.23951231\n",
      "Iteration 32, loss = 0.22522388\n",
      "Iteration 33, loss = 0.21949399\n",
      "Iteration 34, loss = 0.20466335\n",
      "Iteration 35, loss = 0.19198272\n",
      "Iteration 36, loss = 0.17888526\n",
      "Iteration 37, loss = 0.16752270\n",
      "Iteration 38, loss = 0.16100001\n",
      "Iteration 39, loss = 0.15536948\n",
      "Iteration 40, loss = 0.14922312\n",
      "Iteration 41, loss = 0.14620902\n",
      "Iteration 42, loss = 0.14288842\n",
      "Iteration 43, loss = 0.14809823\n",
      "Iteration 44, loss = 0.14040059\n",
      "Iteration 45, loss = 0.13524552\n",
      "Iteration 46, loss = 0.13310057\n",
      "Iteration 47, loss = 0.12887118\n",
      "Iteration 48, loss = 0.12670159\n",
      "Iteration 49, loss = 0.12738697\n",
      "Iteration 50, loss = 0.14590592\n",
      "Iteration 51, loss = 0.14253623\n",
      "Iteration 52, loss = 0.15863981\n",
      "Iteration 53, loss = 0.19340135\n",
      "Iteration 54, loss = 0.15924696\n",
      "Iteration 55, loss = 0.14834055\n",
      "Iteration 56, loss = 0.14427861\n",
      "Iteration 57, loss = 0.13624091\n",
      "Iteration 58, loss = 0.12519294\n",
      "Iteration 59, loss = 0.12119826\n",
      "Iteration 60, loss = 0.11607943\n",
      "Iteration 61, loss = 0.11273674\n",
      "Iteration 62, loss = 0.11080425\n",
      "Iteration 63, loss = 0.10994975\n",
      "Iteration 64, loss = 0.10982664\n",
      "Iteration 65, loss = 0.10776586\n",
      "Iteration 66, loss = 0.10589116\n",
      "Iteration 67, loss = 0.10586716\n",
      "Iteration 68, loss = 0.10683847\n",
      "Iteration 69, loss = 0.10444411\n",
      "Iteration 70, loss = 0.10933167\n",
      "Iteration 71, loss = 0.10438263\n",
      "Iteration 72, loss = 0.10190880\n",
      "Iteration 73, loss = 0.10288478\n",
      "Iteration 74, loss = 0.10073038\n",
      "Iteration 75, loss = 0.09988688\n",
      "Iteration 76, loss = 0.10043845\n",
      "Iteration 77, loss = 0.09940568\n",
      "Iteration 78, loss = 0.09880664\n",
      "Iteration 79, loss = 0.11800136\n",
      "Iteration 80, loss = 0.13710771\n",
      "Iteration 81, loss = 0.12640621\n",
      "Iteration 82, loss = 0.12978447\n",
      "Iteration 83, loss = 0.12378361\n",
      "Iteration 84, loss = 0.13011670\n",
      "Iteration 85, loss = 0.15448212\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92718875\n",
      "Iteration 2, loss = 1.59459913\n",
      "Iteration 3, loss = 1.41083997\n",
      "Iteration 4, loss = 1.23982994\n",
      "Iteration 5, loss = 1.09762307\n",
      "Iteration 6, loss = 0.96887880\n",
      "Iteration 7, loss = 0.86621973\n",
      "Iteration 8, loss = 0.78194535\n",
      "Iteration 9, loss = 0.71563634\n",
      "Iteration 10, loss = 0.68838204\n",
      "Iteration 11, loss = 0.61401192\n",
      "Iteration 12, loss = 0.54821054\n",
      "Iteration 13, loss = 0.50806022\n",
      "Iteration 14, loss = 0.46097263\n",
      "Iteration 15, loss = 0.42993814\n",
      "Iteration 16, loss = 0.41550390\n",
      "Iteration 17, loss = 0.40449383\n",
      "Iteration 18, loss = 0.37626736\n",
      "Iteration 19, loss = 0.33670270\n",
      "Iteration 20, loss = 0.31850041\n",
      "Iteration 21, loss = 0.29849441\n",
      "Iteration 22, loss = 0.29913830\n",
      "Iteration 23, loss = 0.27029832\n",
      "Iteration 24, loss = 0.26072371\n",
      "Iteration 25, loss = 0.25025515\n",
      "Iteration 26, loss = 0.24808671\n",
      "Iteration 27, loss = 0.22952093\n",
      "Iteration 28, loss = 0.21395129\n",
      "Iteration 29, loss = 0.22507001\n",
      "Iteration 30, loss = 0.21197266\n",
      "Iteration 31, loss = 0.19960596\n",
      "Iteration 32, loss = 0.19058424\n",
      "Iteration 33, loss = 0.18438865\n",
      "Iteration 34, loss = 0.18283573\n",
      "Iteration 35, loss = 0.18393367\n",
      "Iteration 36, loss = 0.23862572\n",
      "Iteration 37, loss = 0.26653520\n",
      "Iteration 38, loss = 0.24427815\n",
      "Iteration 39, loss = 0.22336357\n",
      "Iteration 40, loss = 0.19152978\n",
      "Iteration 41, loss = 0.18237731\n",
      "Iteration 42, loss = 0.17114420\n",
      "Iteration 43, loss = 0.16103612\n",
      "Iteration 44, loss = 0.15332167\n",
      "Iteration 45, loss = 0.14854542\n",
      "Iteration 46, loss = 0.14672203\n",
      "Iteration 47, loss = 0.14488179\n",
      "Iteration 48, loss = 0.13823683\n",
      "Iteration 49, loss = 0.13703497\n",
      "Iteration 50, loss = 0.13757006\n",
      "Iteration 51, loss = 0.13686933\n",
      "Iteration 52, loss = 0.13729179\n",
      "Iteration 53, loss = 0.13982051\n",
      "Iteration 54, loss = 0.13180323\n",
      "Iteration 55, loss = 0.13135405\n",
      "Iteration 56, loss = 0.13092041\n",
      "Iteration 57, loss = 0.12726753\n",
      "Iteration 58, loss = 0.12555507\n",
      "Iteration 59, loss = 0.12356550\n",
      "Iteration 60, loss = 0.12257874\n",
      "Iteration 61, loss = 0.12742628\n",
      "Iteration 62, loss = 0.13149406\n",
      "Iteration 63, loss = 0.12933629\n",
      "Iteration 64, loss = 0.12648951\n",
      "Iteration 65, loss = 0.14269623\n",
      "Iteration 66, loss = 0.16670641\n",
      "Iteration 67, loss = 0.15115486\n",
      "Iteration 68, loss = 0.15042424\n",
      "Iteration 69, loss = 0.15072027\n",
      "Iteration 70, loss = 0.13415969\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13334191\n",
      "Iteration 2, loss = 1.76339246\n",
      "Iteration 3, loss = 1.54284025\n",
      "Iteration 4, loss = 1.34152718\n",
      "Iteration 5, loss = 1.17566693\n",
      "Iteration 6, loss = 1.03089294\n",
      "Iteration 7, loss = 0.91643390\n",
      "Iteration 8, loss = 0.82469051\n",
      "Iteration 9, loss = 0.74336735\n",
      "Iteration 10, loss = 0.70332982\n",
      "Iteration 11, loss = 0.63343338\n",
      "Iteration 12, loss = 0.56557109\n",
      "Iteration 13, loss = 0.52646585\n",
      "Iteration 14, loss = 0.47562396\n",
      "Iteration 15, loss = 0.44820115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.42603835\n",
      "Iteration 17, loss = 0.40774768\n",
      "Iteration 18, loss = 0.38483879\n",
      "Iteration 19, loss = 0.34571769\n",
      "Iteration 20, loss = 0.33294232\n",
      "Iteration 21, loss = 0.31222995\n",
      "Iteration 22, loss = 0.30636897\n",
      "Iteration 23, loss = 0.27770751\n",
      "Iteration 24, loss = 0.27324467\n",
      "Iteration 25, loss = 0.26783350\n",
      "Iteration 26, loss = 0.26165030\n",
      "Iteration 27, loss = 0.23975657\n",
      "Iteration 28, loss = 0.22474307\n",
      "Iteration 29, loss = 0.24466584\n",
      "Iteration 30, loss = 0.25939094\n",
      "Iteration 31, loss = 0.23452293\n",
      "Iteration 32, loss = 0.22076667\n",
      "Iteration 33, loss = 0.20839936\n",
      "Iteration 34, loss = 0.20154773\n",
      "Iteration 35, loss = 0.19845111\n",
      "Iteration 36, loss = 0.19143417\n",
      "Iteration 37, loss = 0.19869416\n",
      "Iteration 38, loss = 0.19063544\n",
      "Iteration 39, loss = 0.17775447\n",
      "Iteration 40, loss = 0.17421114\n",
      "Iteration 41, loss = 0.16445686\n",
      "Iteration 42, loss = 0.16850896\n",
      "Iteration 43, loss = 0.17728398\n",
      "Iteration 44, loss = 0.16874824\n",
      "Iteration 45, loss = 0.15682663\n",
      "Iteration 46, loss = 0.15396386\n",
      "Iteration 47, loss = 0.15601589\n",
      "Iteration 48, loss = 0.15071026\n",
      "Iteration 49, loss = 0.15519806\n",
      "Iteration 50, loss = 0.17257559\n",
      "Iteration 51, loss = 0.17245158\n",
      "Iteration 52, loss = 0.20348146\n",
      "Iteration 53, loss = 0.25112437\n",
      "Iteration 54, loss = 0.22456890\n",
      "Iteration 55, loss = 0.21313892\n",
      "Iteration 56, loss = 0.18672967\n",
      "Iteration 57, loss = 0.16228223\n",
      "Iteration 58, loss = 0.14878860\n",
      "Iteration 59, loss = 0.14514896\n",
      "Iteration 60, loss = 0.14044012\n",
      "Iteration 61, loss = 0.13618500\n",
      "Iteration 62, loss = 0.13477589\n",
      "Iteration 63, loss = 0.13402645\n",
      "Iteration 64, loss = 0.13617501\n",
      "Iteration 65, loss = 0.13403628\n",
      "Iteration 66, loss = 0.13114766\n",
      "Iteration 67, loss = 0.13301918\n",
      "Iteration 68, loss = 0.13508326\n",
      "Iteration 69, loss = 0.13199038\n",
      "Iteration 70, loss = 0.14145127\n",
      "Iteration 71, loss = 0.13188328\n",
      "Iteration 72, loss = 0.12917887\n",
      "Iteration 73, loss = 0.12827495\n",
      "Iteration 74, loss = 0.12638506\n",
      "Iteration 75, loss = 0.12695803\n",
      "Iteration 76, loss = 0.12941688\n",
      "Iteration 77, loss = 0.12751902\n",
      "Iteration 78, loss = 0.12666993\n",
      "Iteration 79, loss = 0.12852352\n",
      "Iteration 80, loss = 0.12354063\n",
      "Iteration 81, loss = 0.12420496\n",
      "Iteration 82, loss = 0.12703359\n",
      "Iteration 83, loss = 0.13288795\n",
      "Iteration 84, loss = 0.14632454\n",
      "Iteration 85, loss = 0.14002169\n",
      "Iteration 86, loss = 0.13642968\n",
      "Iteration 87, loss = 0.13111664\n",
      "Iteration 88, loss = 0.12467922\n",
      "Iteration 89, loss = 0.13112856\n",
      "Iteration 90, loss = 0.13342171\n",
      "Iteration 91, loss = 0.13240778\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.33740152\n",
      "Iteration 2, loss = 1.92543364\n",
      "Iteration 3, loss = 1.66427647\n",
      "Iteration 4, loss = 1.43147747\n",
      "Iteration 5, loss = 1.24037176\n",
      "Iteration 6, loss = 1.07786026\n",
      "Iteration 7, loss = 0.95309624\n",
      "Iteration 8, loss = 0.85210712\n",
      "Iteration 9, loss = 0.76519842\n",
      "Iteration 10, loss = 0.72477351\n",
      "Iteration 11, loss = 0.64940329\n",
      "Iteration 12, loss = 0.57810429\n",
      "Iteration 13, loss = 0.53105487\n",
      "Iteration 14, loss = 0.49164405\n",
      "Iteration 15, loss = 0.46500968\n",
      "Iteration 16, loss = 0.44435662\n",
      "Iteration 17, loss = 0.42570913\n",
      "Iteration 18, loss = 0.39437248\n",
      "Iteration 19, loss = 0.35484363\n",
      "Iteration 20, loss = 0.34527784\n",
      "Iteration 21, loss = 0.31985048\n",
      "Iteration 22, loss = 0.31215568\n",
      "Iteration 23, loss = 0.28498425\n",
      "Iteration 24, loss = 0.28105155\n",
      "Iteration 25, loss = 0.27390072\n",
      "Iteration 26, loss = 0.26966657\n",
      "Iteration 27, loss = 0.24899921\n",
      "Iteration 28, loss = 0.23157274\n",
      "Iteration 29, loss = 0.24755351\n",
      "Iteration 30, loss = 0.23938255\n",
      "Iteration 31, loss = 0.22739790\n",
      "Iteration 32, loss = 0.22268364\n",
      "Iteration 33, loss = 0.22242437\n",
      "Iteration 34, loss = 0.21512778\n",
      "Iteration 35, loss = 0.20845768\n",
      "Iteration 36, loss = 0.22474840\n",
      "Iteration 37, loss = 0.22600759\n",
      "Iteration 38, loss = 0.22560890\n",
      "Iteration 39, loss = 0.20928940\n",
      "Iteration 40, loss = 0.18801900\n",
      "Iteration 41, loss = 0.18469899\n",
      "Iteration 42, loss = 0.19100023\n",
      "Iteration 43, loss = 0.19767954\n",
      "Iteration 44, loss = 0.19359352\n",
      "Iteration 45, loss = 0.18795652\n",
      "Iteration 46, loss = 0.18590999\n",
      "Iteration 47, loss = 0.18605083\n",
      "Iteration 48, loss = 0.18162966\n",
      "Iteration 49, loss = 0.16793511\n",
      "Iteration 50, loss = 0.16228544\n",
      "Iteration 51, loss = 0.16178913\n",
      "Iteration 52, loss = 0.16124480\n",
      "Iteration 53, loss = 0.17121215\n",
      "Iteration 54, loss = 0.15761656\n",
      "Iteration 55, loss = 0.15538095\n",
      "Iteration 56, loss = 0.15532493\n",
      "Iteration 57, loss = 0.15072450\n",
      "Iteration 58, loss = 0.14866404\n",
      "Iteration 59, loss = 0.14874498\n",
      "Iteration 60, loss = 0.15020731\n",
      "Iteration 61, loss = 0.14951697\n",
      "Iteration 62, loss = 0.14838489\n",
      "Iteration 63, loss = 0.14850331\n",
      "Iteration 64, loss = 0.15419217\n",
      "Iteration 65, loss = 0.15181703\n",
      "Iteration 66, loss = 0.15353908\n",
      "Iteration 67, loss = 0.15030168\n",
      "Iteration 68, loss = 0.16772376\n",
      "Iteration 69, loss = 0.16630841\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.54001621\n",
      "Iteration 2, loss = 2.08271279\n",
      "Iteration 3, loss = 1.78156424\n",
      "Iteration 4, loss = 1.51527083\n",
      "Iteration 5, loss = 1.29684179\n",
      "Iteration 6, loss = 1.11986614\n",
      "Iteration 7, loss = 0.98421281\n",
      "Iteration 8, loss = 0.87517606\n",
      "Iteration 9, loss = 0.78520864\n",
      "Iteration 10, loss = 0.73820864\n",
      "Iteration 11, loss = 0.65459218\n",
      "Iteration 12, loss = 0.59162868\n",
      "Iteration 13, loss = 0.54426589\n",
      "Iteration 14, loss = 0.49498277\n",
      "Iteration 15, loss = 0.46099669\n",
      "Iteration 16, loss = 0.44272974\n",
      "Iteration 17, loss = 0.42268786\n",
      "Iteration 18, loss = 0.39243350\n",
      "Iteration 19, loss = 0.35476802\n",
      "Iteration 20, loss = 0.34464561\n",
      "Iteration 21, loss = 0.32295305\n",
      "Iteration 22, loss = 0.32275370\n",
      "Iteration 23, loss = 0.29255816\n",
      "Iteration 24, loss = 0.28392822\n",
      "Iteration 25, loss = 0.28358408\n",
      "Iteration 26, loss = 0.28493087\n",
      "Iteration 27, loss = 0.26693584\n",
      "Iteration 28, loss = 0.25179779\n",
      "Iteration 29, loss = 0.29595612\n",
      "Iteration 30, loss = 0.30474336\n",
      "Iteration 31, loss = 0.27106399\n",
      "Iteration 32, loss = 0.24746392\n",
      "Iteration 33, loss = 0.23335588\n",
      "Iteration 34, loss = 0.21857389\n",
      "Iteration 35, loss = 0.21695353\n",
      "Iteration 36, loss = 0.21487497\n",
      "Iteration 37, loss = 0.22464438\n",
      "Iteration 38, loss = 0.21945713\n",
      "Iteration 39, loss = 0.19996518\n",
      "Iteration 40, loss = 0.19039733\n",
      "Iteration 41, loss = 0.19580523\n",
      "Iteration 42, loss = 0.19286496\n",
      "Iteration 43, loss = 0.19212351\n",
      "Iteration 44, loss = 0.18265743\n",
      "Iteration 45, loss = 0.17471670\n",
      "Iteration 46, loss = 0.17565421\n",
      "Iteration 47, loss = 0.17113891\n",
      "Iteration 48, loss = 0.16972256\n",
      "Iteration 49, loss = 0.17318399\n",
      "Iteration 50, loss = 0.19150051\n",
      "Iteration 51, loss = 0.20284349\n",
      "Iteration 52, loss = 0.19813826\n",
      "Iteration 53, loss = 0.21332717\n",
      "Iteration 54, loss = 0.18411954\n",
      "Iteration 55, loss = 0.17605748\n",
      "Iteration 56, loss = 0.16995614\n",
      "Iteration 57, loss = 0.16373312\n",
      "Iteration 58, loss = 0.16358052\n",
      "Iteration 59, loss = 0.16357749\n",
      "Iteration 60, loss = 0.16569455\n",
      "Iteration 61, loss = 0.16402829\n",
      "Iteration 62, loss = 0.15998005\n",
      "Iteration 63, loss = 0.16326404\n",
      "Iteration 64, loss = 0.16206408\n",
      "Iteration 65, loss = 0.15807141\n",
      "Iteration 66, loss = 0.15733183\n",
      "Iteration 67, loss = 0.15488029\n",
      "Iteration 68, loss = 0.16493789\n",
      "Iteration 69, loss = 0.16021298\n",
      "Iteration 70, loss = 0.15957760\n",
      "Iteration 71, loss = 0.15702016\n",
      "Iteration 72, loss = 0.15458664\n",
      "Iteration 73, loss = 0.15411869\n",
      "Iteration 74, loss = 0.15249459\n",
      "Iteration 75, loss = 0.15300814\n",
      "Iteration 76, loss = 0.15589955\n",
      "Iteration 77, loss = 0.15529319\n",
      "Iteration 78, loss = 0.15624857\n",
      "Iteration 79, loss = 0.16324008\n",
      "Iteration 80, loss = 0.15724739\n",
      "Iteration 81, loss = 0.16109039\n",
      "Iteration 82, loss = 0.16400569\n",
      "Iteration 83, loss = 0.15780483\n",
      "Iteration 84, loss = 0.16105673\n",
      "Iteration 85, loss = 0.15461769\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.74146616\n",
      "Iteration 2, loss = 2.23472747\n",
      "Iteration 3, loss = 1.88703568\n",
      "Iteration 4, loss = 1.58839223\n",
      "Iteration 5, loss = 1.34936772\n",
      "Iteration 6, loss = 1.15615489\n",
      "Iteration 7, loss = 1.00774644\n",
      "Iteration 8, loss = 0.89374195\n",
      "Iteration 9, loss = 0.79459022\n",
      "Iteration 10, loss = 0.74625558\n",
      "Iteration 11, loss = 0.66539309\n",
      "Iteration 12, loss = 0.59747132\n",
      "Iteration 13, loss = 0.55014023\n",
      "Iteration 14, loss = 0.49724860\n",
      "Iteration 15, loss = 0.46183119\n",
      "Iteration 16, loss = 0.44423903\n",
      "Iteration 17, loss = 0.41784720\n",
      "Iteration 18, loss = 0.38594285\n",
      "Iteration 19, loss = 0.37203615\n",
      "Iteration 20, loss = 0.37234958\n",
      "Iteration 21, loss = 0.34693948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.32602626\n",
      "Iteration 23, loss = 0.29882104\n",
      "Iteration 24, loss = 0.29397432\n",
      "Iteration 25, loss = 0.28069961\n",
      "Iteration 26, loss = 0.27145431\n",
      "Iteration 27, loss = 0.25456724\n",
      "Iteration 28, loss = 0.24535277\n",
      "Iteration 29, loss = 0.27103506\n",
      "Iteration 30, loss = 0.28820588\n",
      "Iteration 31, loss = 0.25686417\n",
      "Iteration 32, loss = 0.25123968\n",
      "Iteration 33, loss = 0.25233931\n",
      "Iteration 34, loss = 0.23916925\n",
      "Iteration 35, loss = 0.23577965\n",
      "Iteration 36, loss = 0.21513059\n",
      "Iteration 37, loss = 0.21188687\n",
      "Iteration 38, loss = 0.21664493\n",
      "Iteration 39, loss = 0.20480146\n",
      "Iteration 40, loss = 0.19426994\n",
      "Iteration 41, loss = 0.19255826\n",
      "Iteration 42, loss = 0.20356050\n",
      "Iteration 43, loss = 0.22094227\n",
      "Iteration 44, loss = 0.19918890\n",
      "Iteration 45, loss = 0.18982037\n",
      "Iteration 46, loss = 0.19279531\n",
      "Iteration 47, loss = 0.18504552\n",
      "Iteration 48, loss = 0.18494786\n",
      "Iteration 49, loss = 0.19723905\n",
      "Iteration 50, loss = 0.21074121\n",
      "Iteration 51, loss = 0.21374414\n",
      "Iteration 52, loss = 0.21305563\n",
      "Iteration 53, loss = 0.22665380\n",
      "Iteration 54, loss = 0.19839603\n",
      "Iteration 55, loss = 0.21007941\n",
      "Iteration 56, loss = 0.20526180\n",
      "Iteration 57, loss = 0.18448713\n",
      "Iteration 58, loss = 0.17827089\n",
      "Iteration 59, loss = 0.18050089\n",
      "Iteration 60, loss = 0.17428656\n",
      "Iteration 61, loss = 0.17236231\n",
      "Iteration 62, loss = 0.17066351\n",
      "Iteration 63, loss = 0.17216996\n",
      "Iteration 64, loss = 0.17355366\n",
      "Iteration 65, loss = 0.16986213\n",
      "Iteration 66, loss = 0.16784827\n",
      "Iteration 67, loss = 0.16641307\n",
      "Iteration 68, loss = 0.18537168\n",
      "Iteration 69, loss = 0.18586807\n",
      "Iteration 70, loss = 0.17598627\n",
      "Iteration 71, loss = 0.17013216\n",
      "Iteration 72, loss = 0.16791509\n",
      "Iteration 73, loss = 0.16904330\n",
      "Iteration 74, loss = 0.16641926\n",
      "Iteration 75, loss = 0.16594589\n",
      "Iteration 76, loss = 0.16826995\n",
      "Iteration 77, loss = 0.16527622\n",
      "Iteration 78, loss = 0.16895286\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.94159621\n",
      "Iteration 2, loss = 2.38278234\n",
      "Iteration 3, loss = 1.99034731\n",
      "Iteration 4, loss = 1.65680686\n",
      "Iteration 5, loss = 1.39664903\n",
      "Iteration 6, loss = 1.18755383\n",
      "Iteration 7, loss = 1.02742579\n",
      "Iteration 8, loss = 0.90826807\n",
      "Iteration 9, loss = 0.80715211\n",
      "Iteration 10, loss = 0.76295962\n",
      "Iteration 11, loss = 0.67224450\n",
      "Iteration 12, loss = 0.60509237\n",
      "Iteration 13, loss = 0.55851265\n",
      "Iteration 14, loss = 0.50561630\n",
      "Iteration 15, loss = 0.47682815\n",
      "Iteration 16, loss = 0.44879223\n",
      "Iteration 17, loss = 0.43802802\n",
      "Iteration 18, loss = 0.40416952\n",
      "Iteration 19, loss = 0.37565892\n",
      "Iteration 20, loss = 0.36731264\n",
      "Iteration 21, loss = 0.34143208\n",
      "Iteration 22, loss = 0.33438668\n",
      "Iteration 23, loss = 0.30703252\n",
      "Iteration 24, loss = 0.30021899\n",
      "Iteration 25, loss = 0.29184948\n",
      "Iteration 26, loss = 0.28473139\n",
      "Iteration 27, loss = 0.26884152\n",
      "Iteration 28, loss = 0.25731590\n",
      "Iteration 29, loss = 0.29070572\n",
      "Iteration 30, loss = 0.31318684\n",
      "Iteration 31, loss = 0.27173018\n",
      "Iteration 32, loss = 0.25462708\n",
      "Iteration 33, loss = 0.24177740\n",
      "Iteration 34, loss = 0.23578088\n",
      "Iteration 35, loss = 0.23825906\n",
      "Iteration 36, loss = 0.23426239\n",
      "Iteration 37, loss = 0.24076984\n",
      "Iteration 38, loss = 0.24349379\n",
      "Iteration 39, loss = 0.22585131\n",
      "Iteration 40, loss = 0.21064172\n",
      "Iteration 41, loss = 0.20758885\n",
      "Iteration 42, loss = 0.21593856\n",
      "Iteration 43, loss = 0.22350397\n",
      "Iteration 44, loss = 0.21573902\n",
      "Iteration 45, loss = 0.20392651\n",
      "Iteration 46, loss = 0.20304323\n",
      "Iteration 47, loss = 0.19533942\n",
      "Iteration 48, loss = 0.19609431\n",
      "Iteration 49, loss = 0.19831249\n",
      "Iteration 50, loss = 0.21615831\n",
      "Iteration 51, loss = 0.21699256\n",
      "Iteration 52, loss = 0.20259503\n",
      "Iteration 53, loss = 0.21808398\n",
      "Iteration 54, loss = 0.20392466\n",
      "Iteration 55, loss = 0.21320983\n",
      "Iteration 56, loss = 0.20200678\n",
      "Iteration 57, loss = 0.19157787\n",
      "Iteration 58, loss = 0.18492805\n",
      "Iteration 59, loss = 0.18682848\n",
      "Iteration 60, loss = 0.18486673\n",
      "Iteration 61, loss = 0.18378354\n",
      "Iteration 62, loss = 0.19068540\n",
      "Iteration 63, loss = 0.19006468\n",
      "Iteration 64, loss = 0.18978873\n",
      "Iteration 65, loss = 0.18877096\n",
      "Iteration 66, loss = 0.18550564\n",
      "Iteration 67, loss = 0.18425954\n",
      "Iteration 68, loss = 0.20754119\n",
      "Iteration 69, loss = 0.21647463\n",
      "Iteration 70, loss = 0.19909028\n",
      "Iteration 71, loss = 0.18744207\n",
      "Iteration 72, loss = 0.18676468\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.13959042\n",
      "Iteration 2, loss = 2.52544428\n",
      "Iteration 3, loss = 2.08361054\n",
      "Iteration 4, loss = 1.71701510\n",
      "Iteration 5, loss = 1.43346353\n",
      "Iteration 6, loss = 1.20839054\n",
      "Iteration 7, loss = 1.04182009\n",
      "Iteration 8, loss = 0.91811429\n",
      "Iteration 9, loss = 0.81224465\n",
      "Iteration 10, loss = 0.75777439\n",
      "Iteration 11, loss = 0.67795923\n",
      "Iteration 12, loss = 0.60997162\n",
      "Iteration 13, loss = 0.55988649\n",
      "Iteration 14, loss = 0.50625496\n",
      "Iteration 15, loss = 0.47602839\n",
      "Iteration 16, loss = 0.45452811\n",
      "Iteration 17, loss = 0.44803280\n",
      "Iteration 18, loss = 0.41186130\n",
      "Iteration 19, loss = 0.37877799\n",
      "Iteration 20, loss = 0.36564519\n",
      "Iteration 21, loss = 0.34388656\n",
      "Iteration 22, loss = 0.33703887\n",
      "Iteration 23, loss = 0.31163361\n",
      "Iteration 24, loss = 0.30450728\n",
      "Iteration 25, loss = 0.29961421\n",
      "Iteration 26, loss = 0.29199197\n",
      "Iteration 27, loss = 0.27718361\n",
      "Iteration 28, loss = 0.26377089\n",
      "Iteration 29, loss = 0.29726969\n",
      "Iteration 30, loss = 0.31682778\n",
      "Iteration 31, loss = 0.28267995\n",
      "Iteration 32, loss = 0.26653095\n",
      "Iteration 33, loss = 0.26307443\n",
      "Iteration 34, loss = 0.24964958\n",
      "Iteration 35, loss = 0.24773846\n",
      "Iteration 36, loss = 0.23463502\n",
      "Iteration 37, loss = 0.24274171\n",
      "Iteration 38, loss = 0.25458390\n",
      "Iteration 39, loss = 0.23637076\n",
      "Iteration 40, loss = 0.21921193\n",
      "Iteration 41, loss = 0.21560482\n",
      "Iteration 42, loss = 0.22338121\n",
      "Iteration 43, loss = 0.23504237\n",
      "Iteration 44, loss = 0.22290281\n",
      "Iteration 45, loss = 0.21140651\n",
      "Iteration 46, loss = 0.21421707\n",
      "Iteration 47, loss = 0.20565807\n",
      "Iteration 48, loss = 0.20614848\n",
      "Iteration 49, loss = 0.21283590\n",
      "Iteration 50, loss = 0.22317308\n",
      "Iteration 51, loss = 0.22473594\n",
      "Iteration 52, loss = 0.21614695\n",
      "Iteration 53, loss = 0.23862266\n",
      "Iteration 54, loss = 0.21657236\n",
      "Iteration 55, loss = 0.22713134\n",
      "Iteration 56, loss = 0.21872030\n",
      "Iteration 57, loss = 0.20381408\n",
      "Iteration 58, loss = 0.19814195\n",
      "Iteration 59, loss = 0.20117318\n",
      "Iteration 60, loss = 0.19638288\n",
      "Iteration 61, loss = 0.19555358\n",
      "Iteration 62, loss = 0.19937795\n",
      "Iteration 63, loss = 0.20546435\n",
      "Iteration 64, loss = 0.20450034\n",
      "Iteration 65, loss = 0.20008228\n",
      "Iteration 66, loss = 0.19351599\n",
      "Iteration 67, loss = 0.19139171\n",
      "Iteration 68, loss = 0.21039400\n",
      "Iteration 69, loss = 0.21079648\n",
      "Iteration 70, loss = 0.20305277\n",
      "Iteration 71, loss = 0.19423039\n",
      "Iteration 72, loss = 0.19246590\n",
      "Iteration 73, loss = 0.19607802\n",
      "Iteration 74, loss = 0.19398158\n",
      "Iteration 75, loss = 0.19684047\n",
      "Iteration 76, loss = 0.19768928\n",
      "Iteration 77, loss = 0.19720298\n",
      "Iteration 78, loss = 0.19771426\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.33656843\n",
      "Iteration 2, loss = 2.66499782\n",
      "Iteration 3, loss = 2.17822013\n",
      "Iteration 4, loss = 1.77763484\n",
      "Iteration 5, loss = 1.47278205\n",
      "Iteration 6, loss = 1.23838023\n",
      "Iteration 7, loss = 1.06049769\n",
      "Iteration 8, loss = 0.92945449\n",
      "Iteration 9, loss = 0.81668744\n",
      "Iteration 10, loss = 0.76149536\n",
      "Iteration 11, loss = 0.67911210\n",
      "Iteration 12, loss = 0.61024193\n",
      "Iteration 13, loss = 0.55734044\n",
      "Iteration 14, loss = 0.50819248\n",
      "Iteration 15, loss = 0.47804162\n",
      "Iteration 16, loss = 0.45539612\n",
      "Iteration 17, loss = 0.44458715\n",
      "Iteration 18, loss = 0.41819779\n",
      "Iteration 19, loss = 0.37777733\n",
      "Iteration 20, loss = 0.36803853\n",
      "Iteration 21, loss = 0.34461810\n",
      "Iteration 22, loss = 0.34538894\n",
      "Iteration 23, loss = 0.31481048\n",
      "Iteration 24, loss = 0.30870741\n",
      "Iteration 25, loss = 0.30848504\n",
      "Iteration 26, loss = 0.30565218\n",
      "Iteration 27, loss = 0.28797005\n",
      "Iteration 28, loss = 0.27601347\n",
      "Iteration 29, loss = 0.30393551\n",
      "Iteration 30, loss = 0.30413009\n",
      "Iteration 31, loss = 0.28332011\n",
      "Iteration 32, loss = 0.26761294\n",
      "Iteration 33, loss = 0.25551700\n",
      "Iteration 34, loss = 0.25142008\n",
      "Iteration 35, loss = 0.25063272\n",
      "Iteration 36, loss = 0.24711403\n",
      "Iteration 37, loss = 0.25505326\n",
      "Iteration 38, loss = 0.25457324\n",
      "Iteration 39, loss = 0.23914254\n",
      "Iteration 40, loss = 0.22673050\n",
      "Iteration 41, loss = 0.22515742\n",
      "Iteration 42, loss = 0.23589893\n",
      "Iteration 43, loss = 0.24888288\n",
      "Iteration 44, loss = 0.23044864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45, loss = 0.22424318\n",
      "Iteration 46, loss = 0.22984045\n",
      "Iteration 47, loss = 0.21783575\n",
      "Iteration 48, loss = 0.21827321\n",
      "Iteration 49, loss = 0.23174543\n",
      "Iteration 50, loss = 0.24602574\n",
      "Iteration 51, loss = 0.24842106\n",
      "Iteration 52, loss = 0.23193297\n",
      "Iteration 53, loss = 0.24632973\n",
      "Iteration 54, loss = 0.22411077\n",
      "Iteration 55, loss = 0.22712665\n",
      "Iteration 56, loss = 0.22547737\n",
      "Iteration 57, loss = 0.21484603\n",
      "Iteration 58, loss = 0.21057814\n",
      "Iteration 59, loss = 0.21049930\n",
      "Iteration 60, loss = 0.20790915\n",
      "Iteration 61, loss = 0.20804810\n",
      "Iteration 62, loss = 0.21228968\n",
      "Iteration 63, loss = 0.21431603\n",
      "Iteration 64, loss = 0.21193091\n",
      "Iteration 65, loss = 0.21366369\n",
      "Iteration 66, loss = 0.20607293\n",
      "Iteration 67, loss = 0.20424778\n",
      "Iteration 68, loss = 0.22740493\n",
      "Iteration 69, loss = 0.23572929\n",
      "Iteration 70, loss = 0.21782004\n",
      "Iteration 71, loss = 0.20761860\n",
      "Iteration 72, loss = 0.20336806\n",
      "Iteration 73, loss = 0.20661824\n",
      "Iteration 74, loss = 0.20622043\n",
      "Iteration 75, loss = 0.20566239\n",
      "Iteration 76, loss = 0.20797332\n",
      "Iteration 77, loss = 0.20920834\n",
      "Iteration 78, loss = 0.20927025\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.53227896\n",
      "Iteration 2, loss = 2.79955523\n",
      "Iteration 3, loss = 2.26084524\n",
      "Iteration 4, loss = 1.82755244\n",
      "Iteration 5, loss = 1.50079148\n",
      "Iteration 6, loss = 1.25342998\n",
      "Iteration 7, loss = 1.06825952\n",
      "Iteration 8, loss = 0.93595353\n",
      "Iteration 9, loss = 0.81870577\n",
      "Iteration 10, loss = 0.76617413\n",
      "Iteration 11, loss = 0.67817002\n",
      "Iteration 12, loss = 0.60629514\n",
      "Iteration 13, loss = 0.55824319\n",
      "Iteration 14, loss = 0.50858683\n",
      "Iteration 15, loss = 0.48013536\n",
      "Iteration 16, loss = 0.45556642\n",
      "Iteration 17, loss = 0.44542634\n",
      "Iteration 18, loss = 0.41574034\n",
      "Iteration 19, loss = 0.38341762\n",
      "Iteration 20, loss = 0.38563861\n",
      "Iteration 21, loss = 0.36139041\n",
      "Iteration 22, loss = 0.35347043\n",
      "Iteration 23, loss = 0.32681395\n",
      "Iteration 24, loss = 0.31728074\n",
      "Iteration 25, loss = 0.31609389\n",
      "Iteration 26, loss = 0.31313557\n",
      "Iteration 27, loss = 0.29232427\n",
      "Iteration 28, loss = 0.28372173\n",
      "Iteration 29, loss = 0.31284901\n",
      "Iteration 30, loss = 0.31837284\n",
      "Iteration 31, loss = 0.29540654\n",
      "Iteration 32, loss = 0.27370402\n",
      "Iteration 33, loss = 0.26640263\n",
      "Iteration 34, loss = 0.26679993\n",
      "Iteration 35, loss = 0.26540113\n",
      "Iteration 36, loss = 0.25604796\n",
      "Iteration 37, loss = 0.27715832\n",
      "Iteration 38, loss = 0.27769246\n",
      "Iteration 39, loss = 0.25113591\n",
      "Iteration 40, loss = 0.24136590\n",
      "Iteration 41, loss = 0.24540491\n",
      "Iteration 42, loss = 0.25711659\n",
      "Iteration 43, loss = 0.25632027\n",
      "Iteration 44, loss = 0.24207658\n",
      "Iteration 45, loss = 0.23718071\n",
      "Iteration 46, loss = 0.23560315\n",
      "Iteration 47, loss = 0.23241612\n",
      "Iteration 48, loss = 0.22939808\n",
      "Iteration 49, loss = 0.23535287\n",
      "Iteration 50, loss = 0.24406892\n",
      "Iteration 51, loss = 0.25440923\n",
      "Iteration 52, loss = 0.24334411\n",
      "Iteration 53, loss = 0.25757889\n",
      "Iteration 54, loss = 0.23087092\n",
      "Iteration 55, loss = 0.23295254\n",
      "Iteration 56, loss = 0.23136094\n",
      "Iteration 57, loss = 0.22015441\n",
      "Iteration 58, loss = 0.21932609\n",
      "Iteration 59, loss = 0.22170140\n",
      "Iteration 60, loss = 0.21876512\n",
      "Iteration 61, loss = 0.21854207\n",
      "Iteration 62, loss = 0.22677119\n",
      "Iteration 63, loss = 0.22204371\n",
      "Iteration 64, loss = 0.22237607\n",
      "Iteration 65, loss = 0.22088955\n",
      "Iteration 66, loss = 0.21544147\n",
      "Iteration 67, loss = 0.21499303\n",
      "Iteration 68, loss = 0.23914105\n",
      "Iteration 69, loss = 0.24265627\n",
      "Iteration 70, loss = 0.23073148\n",
      "Iteration 71, loss = 0.22017125\n",
      "Iteration 72, loss = 0.21792494\n",
      "Iteration 73, loss = 0.21932453\n",
      "Iteration 74, loss = 0.21683522\n",
      "Iteration 75, loss = 0.21925912\n",
      "Iteration 76, loss = 0.22307270\n",
      "Iteration 77, loss = 0.21992096\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.72637254\n",
      "Iteration 2, loss = 2.93208000\n",
      "Iteration 3, loss = 2.34203735\n",
      "Iteration 4, loss = 1.87511462\n",
      "Iteration 5, loss = 1.52836999\n",
      "Iteration 6, loss = 1.26820457\n",
      "Iteration 7, loss = 1.07539341\n",
      "Iteration 8, loss = 0.94141363\n",
      "Iteration 9, loss = 0.82061010\n",
      "Iteration 10, loss = 0.75210770\n",
      "Iteration 11, loss = 0.67493016\n",
      "Iteration 12, loss = 0.61168382\n",
      "Iteration 13, loss = 0.55787826\n",
      "Iteration 14, loss = 0.51131023\n",
      "Iteration 15, loss = 0.48725115\n",
      "Iteration 16, loss = 0.45829687\n",
      "Iteration 17, loss = 0.44964013\n",
      "Iteration 18, loss = 0.41265535\n",
      "Iteration 19, loss = 0.38877110\n",
      "Iteration 20, loss = 0.38028122\n",
      "Iteration 21, loss = 0.35865645\n",
      "Iteration 22, loss = 0.35927746\n",
      "Iteration 23, loss = 0.33072226\n",
      "Iteration 24, loss = 0.32633409\n",
      "Iteration 25, loss = 0.32730886\n",
      "Iteration 26, loss = 0.32619867\n",
      "Iteration 27, loss = 0.30550645\n",
      "Iteration 28, loss = 0.29590968\n",
      "Iteration 29, loss = 0.32157201\n",
      "Iteration 30, loss = 0.32440131\n",
      "Iteration 31, loss = 0.29647808\n",
      "Iteration 32, loss = 0.27251802\n",
      "Iteration 33, loss = 0.27130374\n",
      "Iteration 34, loss = 0.27243982\n",
      "Iteration 35, loss = 0.28042939\n",
      "Iteration 36, loss = 0.26796586\n",
      "Iteration 37, loss = 0.28738984\n",
      "Iteration 38, loss = 0.29156136\n",
      "Iteration 39, loss = 0.26247220\n",
      "Iteration 40, loss = 0.25343895\n",
      "Iteration 41, loss = 0.25046954\n",
      "Iteration 42, loss = 0.25850271\n",
      "Iteration 43, loss = 0.25837012\n",
      "Iteration 44, loss = 0.25335844\n",
      "Iteration 45, loss = 0.25386197\n",
      "Iteration 46, loss = 0.27062265\n",
      "Iteration 47, loss = 0.25143572\n",
      "Iteration 48, loss = 0.24324138\n",
      "Iteration 49, loss = 0.25161135\n",
      "Iteration 50, loss = 0.25909106\n",
      "Iteration 51, loss = 0.28106512\n",
      "Iteration 52, loss = 0.25757849\n",
      "Iteration 53, loss = 0.25949607\n",
      "Iteration 54, loss = 0.23922455\n",
      "Iteration 55, loss = 0.24605844\n",
      "Iteration 56, loss = 0.24850308\n",
      "Iteration 57, loss = 0.23517401\n",
      "Iteration 58, loss = 0.23394743\n",
      "Iteration 59, loss = 0.23727405\n",
      "Iteration 60, loss = 0.23049163\n",
      "Iteration 61, loss = 0.23351095\n",
      "Iteration 62, loss = 0.24006546\n",
      "Iteration 63, loss = 0.23575438\n",
      "Iteration 64, loss = 0.23510962\n",
      "Iteration 65, loss = 0.23320844\n",
      "Iteration 66, loss = 0.22674534\n",
      "Iteration 67, loss = 0.22637281\n",
      "Iteration 68, loss = 0.25296771\n",
      "Iteration 69, loss = 0.25859285\n",
      "Iteration 70, loss = 0.23826255\n",
      "Iteration 71, loss = 0.22800931\n",
      "Iteration 72, loss = 0.22505383\n",
      "Iteration 73, loss = 0.23065816\n",
      "Iteration 74, loss = 0.22942819\n",
      "Iteration 75, loss = 0.23427528\n",
      "Iteration 76, loss = 0.23766638\n",
      "Iteration 77, loss = 0.23463634\n",
      "Iteration 78, loss = 0.23523229\n",
      "Iteration 79, loss = 0.23076559\n",
      "Iteration 80, loss = 0.22281074\n",
      "Iteration 81, loss = 0.22532505\n",
      "Iteration 82, loss = 0.22850594\n",
      "Iteration 83, loss = 0.22860133\n",
      "Iteration 84, loss = 0.23022752\n",
      "Iteration 85, loss = 0.22689484\n",
      "Iteration 86, loss = 0.22601249\n",
      "Iteration 87, loss = 0.22731646\n",
      "Iteration 88, loss = 0.22508072\n",
      "Iteration 89, loss = 0.23214598\n",
      "Iteration 90, loss = 0.23937398\n",
      "Iteration 91, loss = 0.23637802\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.91943954\n",
      "Iteration 2, loss = 3.05994814\n",
      "Iteration 3, loss = 2.41817733\n",
      "Iteration 4, loss = 1.91903578\n",
      "Iteration 5, loss = 1.55385588\n",
      "Iteration 6, loss = 1.27951278\n",
      "Iteration 7, loss = 1.08376101\n",
      "Iteration 8, loss = 0.94460424\n",
      "Iteration 9, loss = 0.82372859\n",
      "Iteration 10, loss = 0.75960704\n",
      "Iteration 11, loss = 0.68625246\n",
      "Iteration 12, loss = 0.61457074\n",
      "Iteration 13, loss = 0.56706950\n",
      "Iteration 14, loss = 0.51875604\n",
      "Iteration 15, loss = 0.49768360\n",
      "Iteration 16, loss = 0.46763848\n",
      "Iteration 17, loss = 0.46363879\n",
      "Iteration 18, loss = 0.42398553\n",
      "Iteration 19, loss = 0.38802178\n",
      "Iteration 20, loss = 0.38894288\n",
      "Iteration 21, loss = 0.36720615\n",
      "Iteration 22, loss = 0.36351641\n",
      "Iteration 23, loss = 0.33955824\n",
      "Iteration 24, loss = 0.33268762\n",
      "Iteration 25, loss = 0.32756581\n",
      "Iteration 26, loss = 0.32798001\n",
      "Iteration 27, loss = 0.31212029\n",
      "Iteration 28, loss = 0.30501580\n",
      "Iteration 29, loss = 0.32952819\n",
      "Iteration 30, loss = 0.33524534\n",
      "Iteration 31, loss = 0.31145828\n",
      "Iteration 32, loss = 0.28646142\n",
      "Iteration 33, loss = 0.28185082\n",
      "Iteration 34, loss = 0.28754446\n",
      "Iteration 35, loss = 0.29248668\n",
      "Iteration 36, loss = 0.28275665\n",
      "Iteration 37, loss = 0.29761510\n",
      "Iteration 38, loss = 0.30598875\n",
      "Iteration 39, loss = 0.27533017\n",
      "Iteration 40, loss = 0.26327266\n",
      "Iteration 41, loss = 0.26015441\n",
      "Iteration 42, loss = 0.26938033\n",
      "Iteration 43, loss = 0.26259286\n",
      "Iteration 44, loss = 0.25971081\n",
      "Iteration 45, loss = 0.26119375\n",
      "Iteration 46, loss = 0.26569940\n",
      "Iteration 47, loss = 0.25716456\n",
      "Iteration 48, loss = 0.25410058\n",
      "Iteration 49, loss = 0.26003333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, loss = 0.27007829\n",
      "Iteration 51, loss = 0.28657163\n",
      "Iteration 52, loss = 0.26421451\n",
      "Iteration 53, loss = 0.26579942\n",
      "Iteration 54, loss = 0.25058881\n",
      "Iteration 55, loss = 0.25869198\n",
      "Iteration 56, loss = 0.26343694\n",
      "Iteration 57, loss = 0.24940972\n",
      "Iteration 58, loss = 0.25089256\n",
      "Iteration 59, loss = 0.25481627\n",
      "Iteration 60, loss = 0.24643324\n",
      "Iteration 61, loss = 0.24292578\n",
      "Iteration 62, loss = 0.25278549\n",
      "Iteration 63, loss = 0.25129184\n",
      "Iteration 64, loss = 0.24854472\n",
      "Iteration 65, loss = 0.24146941\n",
      "Iteration 66, loss = 0.23847361\n",
      "Iteration 67, loss = 0.23913009\n",
      "Iteration 68, loss = 0.25859979\n",
      "Iteration 69, loss = 0.25109337\n",
      "Iteration 70, loss = 0.23961934\n",
      "Iteration 71, loss = 0.23631305\n",
      "Iteration 72, loss = 0.23527512\n",
      "Iteration 73, loss = 0.24394969\n",
      "Iteration 74, loss = 0.23974152\n",
      "Iteration 75, loss = 0.24218940\n",
      "Iteration 76, loss = 0.24416370\n",
      "Iteration 77, loss = 0.23929377\n",
      "Iteration 78, loss = 0.24355573\n",
      "Iteration 79, loss = 0.24544148\n",
      "Iteration 80, loss = 0.23650618\n",
      "Iteration 81, loss = 0.23832101\n",
      "Iteration 82, loss = 0.24039385\n",
      "Iteration 83, loss = 0.24292826\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.11145912\n",
      "Iteration 2, loss = 3.18664668\n",
      "Iteration 3, loss = 2.49427449\n",
      "Iteration 4, loss = 1.96226480\n",
      "Iteration 5, loss = 1.57857128\n",
      "Iteration 6, loss = 1.29330398\n",
      "Iteration 7, loss = 1.09287191\n",
      "Iteration 8, loss = 0.95193264\n",
      "Iteration 9, loss = 0.82570800\n",
      "Iteration 10, loss = 0.76417431\n",
      "Iteration 11, loss = 0.68121898\n",
      "Iteration 12, loss = 0.60917300\n",
      "Iteration 13, loss = 0.56390933\n",
      "Iteration 14, loss = 0.52138350\n",
      "Iteration 15, loss = 0.49188576\n",
      "Iteration 16, loss = 0.46923557\n",
      "Iteration 17, loss = 0.46736356\n",
      "Iteration 18, loss = 0.43739537\n",
      "Iteration 19, loss = 0.39732676\n",
      "Iteration 20, loss = 0.39694177\n",
      "Iteration 21, loss = 0.36912183\n",
      "Iteration 22, loss = 0.36738669\n",
      "Iteration 23, loss = 0.34567495\n",
      "Iteration 24, loss = 0.34166728\n",
      "Iteration 25, loss = 0.33702965\n",
      "Iteration 26, loss = 0.33500309\n",
      "Iteration 27, loss = 0.31741341\n",
      "Iteration 28, loss = 0.31271198\n",
      "Iteration 29, loss = 0.33761423\n",
      "Iteration 30, loss = 0.34903026\n",
      "Iteration 31, loss = 0.31952601\n",
      "Iteration 32, loss = 0.29315637\n",
      "Iteration 33, loss = 0.28876388\n",
      "Iteration 34, loss = 0.29166930\n",
      "Iteration 35, loss = 0.30041397\n",
      "Iteration 36, loss = 0.29983380\n",
      "Iteration 37, loss = 0.31512983\n",
      "Iteration 38, loss = 0.31894924\n",
      "Iteration 39, loss = 0.29570509\n",
      "Iteration 40, loss = 0.27579847\n",
      "Iteration 41, loss = 0.27403253\n",
      "Iteration 42, loss = 0.27785141\n",
      "Iteration 43, loss = 0.27213030\n",
      "Iteration 44, loss = 0.27180652\n",
      "Iteration 45, loss = 0.26789369\n",
      "Iteration 46, loss = 0.27209221\n",
      "Iteration 47, loss = 0.26590698\n",
      "Iteration 48, loss = 0.26488869\n",
      "Iteration 49, loss = 0.27778706\n",
      "Iteration 50, loss = 0.28468243\n",
      "Iteration 51, loss = 0.30347853\n",
      "Iteration 52, loss = 0.27324304\n",
      "Iteration 53, loss = 0.27775741\n",
      "Iteration 54, loss = 0.26020386\n",
      "Iteration 55, loss = 0.26976255\n",
      "Iteration 56, loss = 0.26979878\n",
      "Iteration 57, loss = 0.25741836\n",
      "Iteration 58, loss = 0.25712164\n",
      "Iteration 59, loss = 0.26521005\n",
      "Iteration 60, loss = 0.25807889\n",
      "Iteration 61, loss = 0.25550131\n",
      "Iteration 62, loss = 0.26342828\n",
      "Iteration 63, loss = 0.26272762\n",
      "Iteration 64, loss = 0.26225791\n",
      "Iteration 65, loss = 0.25737604\n",
      "Iteration 66, loss = 0.25001089\n",
      "Iteration 67, loss = 0.25306647\n",
      "Iteration 68, loss = 0.26871723\n",
      "Iteration 69, loss = 0.26881642\n",
      "Iteration 70, loss = 0.25881390\n",
      "Iteration 71, loss = 0.25189415\n",
      "Iteration 72, loss = 0.24875889\n",
      "Iteration 73, loss = 0.25389550\n",
      "Iteration 74, loss = 0.25064190\n",
      "Iteration 75, loss = 0.25514754\n",
      "Iteration 76, loss = 0.25777041\n",
      "Iteration 77, loss = 0.25053105\n",
      "Iteration 78, loss = 0.25599340\n",
      "Iteration 79, loss = 0.25496599\n",
      "Iteration 80, loss = 0.24722109\n",
      "Iteration 81, loss = 0.24998485\n",
      "Iteration 82, loss = 0.25450604\n",
      "Iteration 83, loss = 0.25467734\n",
      "Iteration 84, loss = 0.25865716\n",
      "Iteration 85, loss = 0.25097555\n",
      "Iteration 86, loss = 0.25229762\n",
      "Iteration 87, loss = 0.25032785\n",
      "Iteration 88, loss = 0.24805662\n",
      "Iteration 89, loss = 0.25446291\n",
      "Iteration 90, loss = 0.26269025\n",
      "Iteration 91, loss = 0.25662599\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.30222102\n",
      "Iteration 2, loss = 3.30881821\n",
      "Iteration 3, loss = 2.56532102\n",
      "Iteration 4, loss = 2.00143422\n",
      "Iteration 5, loss = 1.59919415\n",
      "Iteration 6, loss = 1.30490148\n",
      "Iteration 7, loss = 1.09571063\n",
      "Iteration 8, loss = 0.94862902\n",
      "Iteration 9, loss = 0.82932635\n",
      "Iteration 10, loss = 0.75893699\n",
      "Iteration 11, loss = 0.68378176\n",
      "Iteration 12, loss = 0.61469587\n",
      "Iteration 13, loss = 0.56541031\n",
      "Iteration 14, loss = 0.52108498\n",
      "Iteration 15, loss = 0.49772340\n",
      "Iteration 16, loss = 0.46534307\n",
      "Iteration 17, loss = 0.46715984\n",
      "Iteration 18, loss = 0.44598223\n",
      "Iteration 19, loss = 0.40427466\n",
      "Iteration 20, loss = 0.40168546\n",
      "Iteration 21, loss = 0.37705325\n",
      "Iteration 22, loss = 0.37196458\n",
      "Iteration 23, loss = 0.35491304\n",
      "Iteration 24, loss = 0.34821077\n",
      "Iteration 25, loss = 0.33878276\n",
      "Iteration 26, loss = 0.34577066\n",
      "Iteration 27, loss = 0.32672435\n",
      "Iteration 28, loss = 0.32116651\n",
      "Iteration 29, loss = 0.34804103\n",
      "Iteration 30, loss = 0.33614311\n",
      "Iteration 31, loss = 0.32178632\n",
      "Iteration 32, loss = 0.29701452\n",
      "Iteration 33, loss = 0.29768119\n",
      "Iteration 34, loss = 0.30532993\n",
      "Iteration 35, loss = 0.31877219\n",
      "Iteration 36, loss = 0.32013140\n",
      "Iteration 37, loss = 0.29940611\n",
      "Iteration 38, loss = 0.32549503\n",
      "Iteration 39, loss = 0.30067461\n",
      "Iteration 40, loss = 0.28256875\n",
      "Iteration 41, loss = 0.28193475\n",
      "Iteration 42, loss = 0.28905461\n",
      "Iteration 43, loss = 0.28681750\n",
      "Iteration 44, loss = 0.28598840\n",
      "Iteration 45, loss = 0.28350585\n",
      "Iteration 46, loss = 0.28255610\n",
      "Iteration 47, loss = 0.28562192\n",
      "Iteration 48, loss = 0.28352609\n",
      "Iteration 49, loss = 0.28607354\n",
      "Iteration 50, loss = 0.28216648\n",
      "Iteration 51, loss = 0.30739348\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.49187648\n",
      "Iteration 2, loss = 3.42806414\n",
      "Iteration 3, loss = 2.63386045\n",
      "Iteration 4, loss = 2.03883699\n",
      "Iteration 5, loss = 1.61705393\n",
      "Iteration 6, loss = 1.31543406\n",
      "Iteration 7, loss = 1.10080618\n",
      "Iteration 8, loss = 0.95367667\n",
      "Iteration 9, loss = 0.82932945\n",
      "Iteration 10, loss = 0.76260584\n",
      "Iteration 11, loss = 0.69100251\n",
      "Iteration 12, loss = 0.61720507\n",
      "Iteration 13, loss = 0.56963234\n",
      "Iteration 14, loss = 0.52792127\n",
      "Iteration 15, loss = 0.50238686\n",
      "Iteration 16, loss = 0.47447676\n",
      "Iteration 17, loss = 0.47077802\n",
      "Iteration 18, loss = 0.44586127\n",
      "Iteration 19, loss = 0.40986874\n",
      "Iteration 20, loss = 0.40118285\n",
      "Iteration 21, loss = 0.38390797\n",
      "Iteration 22, loss = 0.37524646\n",
      "Iteration 23, loss = 0.36248959\n",
      "Iteration 24, loss = 0.36159731\n",
      "Iteration 25, loss = 0.35634176\n",
      "Iteration 26, loss = 0.35419228\n",
      "Iteration 27, loss = 0.33954253\n",
      "Iteration 28, loss = 0.33139639\n",
      "Iteration 29, loss = 0.35140557\n",
      "Iteration 30, loss = 0.32975542\n",
      "Iteration 31, loss = 0.32889947\n",
      "Iteration 32, loss = 0.30793294\n",
      "Iteration 33, loss = 0.30667116\n",
      "Iteration 34, loss = 0.31463089\n",
      "Iteration 35, loss = 0.32759668\n",
      "Iteration 36, loss = 0.32314834\n",
      "Iteration 37, loss = 0.31449603\n",
      "Iteration 38, loss = 0.34471097\n",
      "Iteration 39, loss = 0.31914326\n",
      "Iteration 40, loss = 0.30167906\n",
      "Iteration 41, loss = 0.29583519\n",
      "Iteration 42, loss = 0.29533204\n",
      "Iteration 43, loss = 0.28836212\n",
      "Iteration 44, loss = 0.29491102\n",
      "Iteration 45, loss = 0.28974291\n",
      "Iteration 46, loss = 0.29098748\n",
      "Iteration 47, loss = 0.29204710\n",
      "Iteration 48, loss = 0.29017414\n",
      "Iteration 49, loss = 0.29387541\n",
      "Iteration 50, loss = 0.29023041\n",
      "Iteration 51, loss = 0.31664205\n",
      "Iteration 52, loss = 0.29246567\n",
      "Iteration 53, loss = 0.29714008\n",
      "Iteration 54, loss = 0.28126783\n",
      "Iteration 55, loss = 0.29034790\n",
      "Iteration 56, loss = 0.30122507\n",
      "Iteration 57, loss = 0.28473391\n",
      "Iteration 58, loss = 0.28668671\n",
      "Iteration 59, loss = 0.28867537\n",
      "Iteration 60, loss = 0.28221157\n",
      "Iteration 61, loss = 0.27239339\n",
      "Iteration 62, loss = 0.28116017\n",
      "Iteration 63, loss = 0.28970699\n",
      "Iteration 64, loss = 0.28844075\n",
      "Iteration 65, loss = 0.27765008\n",
      "Iteration 66, loss = 0.27567241\n",
      "Iteration 67, loss = 0.27673024\n",
      "Iteration 68, loss = 0.28689960\n",
      "Iteration 69, loss = 0.27873748\n",
      "Iteration 70, loss = 0.27581046\n",
      "Iteration 71, loss = 0.26912286\n",
      "Iteration 72, loss = 0.26755276\n",
      "Iteration 73, loss = 0.27549741\n",
      "Iteration 74, loss = 0.27379695\n",
      "Iteration 75, loss = 0.27323850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76, loss = 0.27270849\n",
      "Iteration 77, loss = 0.26976756\n",
      "Iteration 78, loss = 0.27123198\n",
      "Iteration 79, loss = 0.27077392\n",
      "Iteration 80, loss = 0.26927344\n",
      "Iteration 81, loss = 0.27142369\n",
      "Iteration 82, loss = 0.27410035\n",
      "Iteration 83, loss = 0.27370926\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.68031789\n",
      "Iteration 2, loss = 3.54568964\n",
      "Iteration 3, loss = 2.69791378\n",
      "Iteration 4, loss = 2.07357860\n",
      "Iteration 5, loss = 1.63564918\n",
      "Iteration 6, loss = 1.32562796\n",
      "Iteration 7, loss = 1.10562487\n",
      "Iteration 8, loss = 0.95356654\n",
      "Iteration 9, loss = 0.82877001\n",
      "Iteration 10, loss = 0.76190757\n",
      "Iteration 11, loss = 0.68809673\n",
      "Iteration 12, loss = 0.61362331\n",
      "Iteration 13, loss = 0.56804126\n",
      "Iteration 14, loss = 0.52306289\n",
      "Iteration 15, loss = 0.50662682\n",
      "Iteration 16, loss = 0.47733945\n",
      "Iteration 17, loss = 0.47924531\n",
      "Iteration 18, loss = 0.44725072\n",
      "Iteration 19, loss = 0.41405008\n",
      "Iteration 20, loss = 0.40636119\n",
      "Iteration 21, loss = 0.38843490\n",
      "Iteration 22, loss = 0.38442923\n",
      "Iteration 23, loss = 0.37197046\n",
      "Iteration 24, loss = 0.36787190\n",
      "Iteration 25, loss = 0.35925000\n",
      "Iteration 26, loss = 0.35514124\n",
      "Iteration 27, loss = 0.34069570\n",
      "Iteration 28, loss = 0.33466674\n",
      "Iteration 29, loss = 0.35721630\n",
      "Iteration 30, loss = 0.35574206\n",
      "Iteration 31, loss = 0.34114570\n",
      "Iteration 32, loss = 0.31764532\n",
      "Iteration 33, loss = 0.31657808\n",
      "Iteration 34, loss = 0.32452654\n",
      "Iteration 35, loss = 0.33387125\n",
      "Iteration 36, loss = 0.32130465\n",
      "Iteration 37, loss = 0.32083180\n",
      "Iteration 38, loss = 0.33967426\n",
      "Iteration 39, loss = 0.31750933\n",
      "Iteration 40, loss = 0.30191822\n",
      "Iteration 41, loss = 0.30259690\n",
      "Iteration 42, loss = 0.30508215\n",
      "Iteration 43, loss = 0.29608224\n",
      "Iteration 44, loss = 0.29786136\n",
      "Iteration 45, loss = 0.29870830\n",
      "Iteration 46, loss = 0.30326590\n",
      "Iteration 47, loss = 0.30713917\n",
      "Iteration 48, loss = 0.30262947\n",
      "Iteration 49, loss = 0.30385165\n",
      "Iteration 50, loss = 0.30329786\n",
      "Iteration 51, loss = 0.32702390\n",
      "Iteration 52, loss = 0.30349020\n",
      "Iteration 53, loss = 0.31152534\n",
      "Iteration 54, loss = 0.29234586\n",
      "Iteration 55, loss = 0.30044300\n",
      "Iteration 56, loss = 0.31138080\n",
      "Iteration 57, loss = 0.29763779\n",
      "Iteration 58, loss = 0.29914812\n",
      "Iteration 59, loss = 0.30053401\n",
      "Iteration 60, loss = 0.29097980\n",
      "Iteration 61, loss = 0.28152368\n",
      "Iteration 62, loss = 0.29121470\n",
      "Iteration 63, loss = 0.29817073\n",
      "Iteration 64, loss = 0.30095935\n",
      "Iteration 65, loss = 0.28999419\n",
      "Iteration 66, loss = 0.28542232\n",
      "Iteration 67, loss = 0.28482631\n",
      "Iteration 68, loss = 0.29714925\n",
      "Iteration 69, loss = 0.29061016\n",
      "Iteration 70, loss = 0.28261727\n",
      "Iteration 71, loss = 0.27719686\n",
      "Iteration 72, loss = 0.27804711\n",
      "Iteration 73, loss = 0.28273623\n",
      "Iteration 74, loss = 0.28272063\n",
      "Iteration 75, loss = 0.28416092\n",
      "Iteration 76, loss = 0.28639707\n",
      "Iteration 77, loss = 0.28193244\n",
      "Iteration 78, loss = 0.28499577\n",
      "Iteration 79, loss = 0.28067245\n",
      "Iteration 80, loss = 0.27777564\n",
      "Iteration 81, loss = 0.28279299\n",
      "Iteration 82, loss = 0.28481868\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.86813109\n",
      "Iteration 2, loss = 3.66094510\n",
      "Iteration 3, loss = 2.76284042\n",
      "Iteration 4, loss = 2.10625507\n",
      "Iteration 5, loss = 1.65278708\n",
      "Iteration 6, loss = 1.33465272\n",
      "Iteration 7, loss = 1.10898014\n",
      "Iteration 8, loss = 0.95620577\n",
      "Iteration 9, loss = 0.82953004\n",
      "Iteration 10, loss = 0.76728518\n",
      "Iteration 11, loss = 0.68331013\n",
      "Iteration 12, loss = 0.61142197\n",
      "Iteration 13, loss = 0.57082387\n",
      "Iteration 14, loss = 0.53017589\n",
      "Iteration 15, loss = 0.50991900\n",
      "Iteration 16, loss = 0.47574199\n",
      "Iteration 17, loss = 0.47341502\n",
      "Iteration 18, loss = 0.45323940\n",
      "Iteration 19, loss = 0.41875827\n",
      "Iteration 20, loss = 0.41278322\n",
      "Iteration 21, loss = 0.39289372\n",
      "Iteration 22, loss = 0.39262129\n",
      "Iteration 23, loss = 0.37927712\n",
      "Iteration 24, loss = 0.37625965\n",
      "Iteration 25, loss = 0.36691808\n",
      "Iteration 26, loss = 0.36499726\n",
      "Iteration 27, loss = 0.35460177\n",
      "Iteration 28, loss = 0.34741620\n",
      "Iteration 29, loss = 0.36890914\n",
      "Iteration 30, loss = 0.34987004\n",
      "Iteration 31, loss = 0.34778982\n",
      "Iteration 32, loss = 0.32595481\n",
      "Iteration 33, loss = 0.32549358\n",
      "Iteration 34, loss = 0.34064867\n",
      "Iteration 35, loss = 0.34237898\n",
      "Iteration 36, loss = 0.32505695\n",
      "Iteration 37, loss = 0.32843293\n",
      "Iteration 38, loss = 0.34922046\n",
      "Iteration 39, loss = 0.33389006\n",
      "Iteration 40, loss = 0.31491362\n",
      "Iteration 41, loss = 0.31555575\n",
      "Iteration 42, loss = 0.31410857\n",
      "Iteration 43, loss = 0.30751374\n",
      "Iteration 44, loss = 0.31794047\n",
      "Iteration 45, loss = 0.31062635\n",
      "Iteration 46, loss = 0.30788450\n",
      "Iteration 47, loss = 0.31346199\n",
      "Iteration 48, loss = 0.30837231\n",
      "Iteration 49, loss = 0.30902641\n",
      "Iteration 50, loss = 0.30899128\n",
      "Iteration 51, loss = 0.33492866\n",
      "Iteration 52, loss = 0.30631802\n",
      "Iteration 53, loss = 0.31554588\n",
      "Iteration 54, loss = 0.30269906\n",
      "Iteration 55, loss = 0.31084385\n",
      "Iteration 56, loss = 0.31722664\n",
      "Iteration 57, loss = 0.30452512\n",
      "Iteration 58, loss = 0.30704221\n",
      "Iteration 59, loss = 0.30800663\n",
      "Iteration 60, loss = 0.30085243\n",
      "Iteration 61, loss = 0.29075424\n",
      "Iteration 62, loss = 0.30080667\n",
      "Iteration 63, loss = 0.31236512\n",
      "Iteration 64, loss = 0.31014665\n",
      "Iteration 65, loss = 0.29686795\n",
      "Iteration 66, loss = 0.29409734\n",
      "Iteration 67, loss = 0.29367495\n",
      "Iteration 68, loss = 0.31276801\n",
      "Iteration 69, loss = 0.30266522\n",
      "Iteration 70, loss = 0.29587627\n",
      "Iteration 71, loss = 0.28928264\n",
      "Iteration 72, loss = 0.28886774\n",
      "Iteration 73, loss = 0.29472367\n",
      "Iteration 74, loss = 0.29545714\n",
      "Iteration 75, loss = 0.29363172\n",
      "Iteration 76, loss = 0.29389643\n",
      "Iteration 77, loss = 0.29352973\n",
      "Iteration 78, loss = 0.29985847\n",
      "Iteration 79, loss = 0.29354655\n",
      "Iteration 80, loss = 0.29026237\n",
      "Iteration 81, loss = 0.29336732\n",
      "Iteration 82, loss = 0.30028419\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.05448902\n",
      "Iteration 2, loss = 3.77422214\n",
      "Iteration 3, loss = 2.82464952\n",
      "Iteration 4, loss = 2.13849964\n",
      "Iteration 5, loss = 1.66762185\n",
      "Iteration 6, loss = 1.34109938\n",
      "Iteration 7, loss = 1.11122605\n",
      "Iteration 8, loss = 0.95452111\n",
      "Iteration 9, loss = 0.83033775\n",
      "Iteration 10, loss = 0.75292119\n",
      "Iteration 11, loss = 0.68275677\n",
      "Iteration 12, loss = 0.61196639\n",
      "Iteration 13, loss = 0.56789527\n",
      "Iteration 14, loss = 0.53115033\n",
      "Iteration 15, loss = 0.50718828\n",
      "Iteration 16, loss = 0.47876103\n",
      "Iteration 17, loss = 0.48039658\n",
      "Iteration 18, loss = 0.45486564\n",
      "Iteration 19, loss = 0.42377340\n",
      "Iteration 20, loss = 0.41818960\n",
      "Iteration 21, loss = 0.39677611\n",
      "Iteration 22, loss = 0.39634398\n",
      "Iteration 23, loss = 0.38484361\n",
      "Iteration 24, loss = 0.38248895\n",
      "Iteration 25, loss = 0.37611191\n",
      "Iteration 26, loss = 0.37117286\n",
      "Iteration 27, loss = 0.36164540\n",
      "Iteration 28, loss = 0.35503068\n",
      "Iteration 29, loss = 0.37974710\n",
      "Iteration 30, loss = 0.36737276\n",
      "Iteration 31, loss = 0.36539359\n",
      "Iteration 32, loss = 0.34154122\n",
      "Iteration 33, loss = 0.33531074\n",
      "Iteration 34, loss = 0.34218485\n",
      "Iteration 35, loss = 0.33874277\n",
      "Iteration 36, loss = 0.33272942\n",
      "Iteration 37, loss = 0.33698151\n",
      "Iteration 38, loss = 0.35724777\n",
      "Iteration 39, loss = 0.34556943\n",
      "Iteration 40, loss = 0.32319789\n",
      "Iteration 41, loss = 0.32524725\n",
      "Iteration 42, loss = 0.32279867\n",
      "Iteration 43, loss = 0.31538142\n",
      "Iteration 44, loss = 0.32990328\n",
      "Iteration 45, loss = 0.32075636\n",
      "Iteration 46, loss = 0.31519547\n",
      "Iteration 47, loss = 0.32399102\n",
      "Iteration 48, loss = 0.32041234\n",
      "Iteration 49, loss = 0.31803258\n",
      "Iteration 50, loss = 0.32283152\n",
      "Iteration 51, loss = 0.33929794\n",
      "Iteration 52, loss = 0.32259572\n",
      "Iteration 53, loss = 0.32846179\n",
      "Iteration 54, loss = 0.31154227\n",
      "Iteration 55, loss = 0.31917766\n",
      "Iteration 56, loss = 0.32846706\n",
      "Iteration 57, loss = 0.31642411\n",
      "Iteration 58, loss = 0.31600591\n",
      "Iteration 59, loss = 0.31408828\n",
      "Iteration 60, loss = 0.30845336\n",
      "Iteration 61, loss = 0.30057597\n",
      "Iteration 62, loss = 0.31424158\n",
      "Iteration 63, loss = 0.32236170\n",
      "Iteration 64, loss = 0.31690308\n",
      "Iteration 65, loss = 0.30819436\n",
      "Iteration 66, loss = 0.30290051\n",
      "Iteration 67, loss = 0.30229658\n",
      "Iteration 68, loss = 0.31507100\n",
      "Iteration 69, loss = 0.30956386\n",
      "Iteration 70, loss = 0.30266628\n",
      "Iteration 71, loss = 0.29761486\n",
      "Iteration 72, loss = 0.29731924\n",
      "Iteration 73, loss = 0.30481221\n",
      "Iteration 74, loss = 0.30811593\n",
      "Iteration 75, loss = 0.30348280\n",
      "Iteration 76, loss = 0.30513230\n",
      "Iteration 77, loss = 0.30647753\n",
      "Iteration 78, loss = 0.31104357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 79, loss = 0.30470536\n",
      "Iteration 80, loss = 0.30072151\n",
      "Iteration 81, loss = 0.30127348\n",
      "Iteration 82, loss = 0.30513061\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.24033253\n",
      "Iteration 2, loss = 3.88577406\n",
      "Iteration 3, loss = 2.88309299\n",
      "Iteration 4, loss = 2.16854045\n",
      "Iteration 5, loss = 1.68028749\n",
      "Iteration 6, loss = 1.34448807\n",
      "Iteration 7, loss = 1.11016868\n",
      "Iteration 8, loss = 0.95431476\n",
      "Iteration 9, loss = 0.82699588\n",
      "Iteration 10, loss = 0.75267259\n",
      "Iteration 11, loss = 0.68099398\n",
      "Iteration 12, loss = 0.61025363\n",
      "Iteration 13, loss = 0.56927654\n",
      "Iteration 14, loss = 0.53449535\n",
      "Iteration 15, loss = 0.50683754\n",
      "Iteration 16, loss = 0.47395851\n",
      "Iteration 17, loss = 0.47981292\n",
      "Iteration 18, loss = 0.45718284\n",
      "Iteration 19, loss = 0.42619038\n",
      "Iteration 20, loss = 0.42252190\n",
      "Iteration 21, loss = 0.40289023\n",
      "Iteration 22, loss = 0.40047342\n",
      "Iteration 23, loss = 0.38956144\n",
      "Iteration 24, loss = 0.38996658\n",
      "Iteration 25, loss = 0.38118743\n",
      "Iteration 26, loss = 0.37828503\n",
      "Iteration 27, loss = 0.37023289\n",
      "Iteration 28, loss = 0.36336140\n",
      "Iteration 29, loss = 0.38159705\n",
      "Iteration 30, loss = 0.36796201\n",
      "Iteration 31, loss = 0.36592720\n",
      "Iteration 32, loss = 0.34489693\n",
      "Iteration 33, loss = 0.34533317\n",
      "Iteration 34, loss = 0.35178300\n",
      "Iteration 35, loss = 0.34582639\n",
      "Iteration 36, loss = 0.34215237\n",
      "Iteration 37, loss = 0.34664846\n",
      "Iteration 38, loss = 0.37493204\n",
      "Iteration 39, loss = 0.36796912\n",
      "Iteration 40, loss = 0.33644373\n",
      "Iteration 41, loss = 0.33420541\n",
      "Iteration 42, loss = 0.33174028\n",
      "Iteration 43, loss = 0.32494176\n",
      "Iteration 44, loss = 0.33787032\n",
      "Iteration 45, loss = 0.33022233\n",
      "Iteration 46, loss = 0.32561389\n",
      "Iteration 47, loss = 0.33322298\n",
      "Iteration 48, loss = 0.33039475\n",
      "Iteration 49, loss = 0.32974707\n",
      "Iteration 50, loss = 0.33604267\n",
      "Iteration 51, loss = 0.34441667\n",
      "Iteration 52, loss = 0.33190908\n",
      "Iteration 53, loss = 0.34020811\n",
      "Iteration 54, loss = 0.32406214\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.42504611\n",
      "Iteration 2, loss = 3.99306027\n",
      "Iteration 3, loss = 2.94125762\n",
      "Iteration 4, loss = 2.19845963\n",
      "Iteration 5, loss = 1.69610991\n",
      "Iteration 6, loss = 1.35268780\n",
      "Iteration 7, loss = 1.11538734\n",
      "Iteration 8, loss = 0.95480301\n",
      "Iteration 9, loss = 0.82539355\n",
      "Iteration 10, loss = 0.74868396\n",
      "Iteration 11, loss = 0.68190319\n",
      "Iteration 12, loss = 0.61245181\n",
      "Iteration 13, loss = 0.56922341\n",
      "Iteration 14, loss = 0.53779336\n",
      "Iteration 15, loss = 0.50967993\n",
      "Iteration 16, loss = 0.47936262\n",
      "Iteration 17, loss = 0.48376264\n",
      "Iteration 18, loss = 0.46116544\n",
      "Iteration 19, loss = 0.43489741\n",
      "Iteration 20, loss = 0.42676213\n",
      "Iteration 21, loss = 0.40729787\n",
      "Iteration 22, loss = 0.40585348\n",
      "Iteration 23, loss = 0.39873412\n",
      "Iteration 24, loss = 0.40195179\n",
      "Iteration 25, loss = 0.39633573\n",
      "Iteration 26, loss = 0.38030190\n",
      "Iteration 27, loss = 0.37857178\n",
      "Iteration 28, loss = 0.37319308\n",
      "Iteration 29, loss = 0.38701233\n",
      "Iteration 30, loss = 0.37326793\n",
      "Iteration 31, loss = 0.36966607\n",
      "Iteration 32, loss = 0.34998818\n",
      "Iteration 33, loss = 0.35456035\n",
      "Iteration 34, loss = 0.36476843\n",
      "Iteration 35, loss = 0.36079273\n",
      "Iteration 36, loss = 0.35355567\n",
      "Iteration 37, loss = 0.35903844\n",
      "Iteration 38, loss = 0.37698762\n",
      "Iteration 39, loss = 0.37148402\n",
      "Iteration 40, loss = 0.34601072\n",
      "Iteration 41, loss = 0.34598241\n",
      "Iteration 42, loss = 0.34237906\n",
      "Iteration 43, loss = 0.33320543\n",
      "Iteration 44, loss = 0.34835546\n",
      "Iteration 45, loss = 0.33850724\n",
      "Iteration 46, loss = 0.33112059\n",
      "Iteration 47, loss = 0.33931559\n",
      "Iteration 48, loss = 0.33692108\n",
      "Iteration 49, loss = 0.34033969\n",
      "Iteration 50, loss = 0.34597495\n",
      "Iteration 51, loss = 0.35321795\n",
      "Iteration 52, loss = 0.34364101\n",
      "Iteration 53, loss = 0.34800968\n",
      "Iteration 54, loss = 0.33482374\n",
      "Iteration 55, loss = 0.33631203\n",
      "Iteration 56, loss = 0.34675007\n",
      "Iteration 57, loss = 0.33504180\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118737b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standardize the full data kek\n",
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "'''\n",
    "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n",
    "              beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
    "              epsilon=1e-08, hidden_layer_sizes=(15,),\n",
    "              learning_rate='constant', learning_rate_init=0.001,\n",
    "              max_iter=200, momentum=0.9, n_iter_no_change=10,\n",
    "              nesterovs_momentum=True, power_t=0.5,  random_state=1,\n",
    "              shuffle=True, solver='lbfgs', tol=0.0001,\n",
    "              validation_fraction=0.1, verbose=False, warm_start=False)\n",
    "\n",
    "'''\n",
    "estimator = MLPClassifier(solver='adam', alpha=1e-5,\n",
    "                       hidden_layer_sizes=(150,150), random_state=1,\n",
    "                        activation='relu', learning_rate='constant', beta_1=0.9, \n",
    "                          beta_2=0.999, learning_rate_init=0.001, verbose=True,\n",
    "                          power_t=0.5, tol=0.001, early_stopping=False, max_iter=200\n",
    "                      )\n",
    "# train_sizes, train_scores, valid_scores = learning_curve(estimator, X_standardized, y, cv=kf)\n",
    "plot_validation_curve(estimator, \"NN Learning Curves\", X_standardized, y, cv=kf,\n",
    "                        param_name='alpha', param_range=np.arange(1e-5, 5, 0.2), scoring=\"accuracy\")\n",
    "\n",
    "# Observations: 'sgd' too slow and not better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
