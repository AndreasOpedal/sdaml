{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Script -- 28/11/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "seed = 10\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random as rn\n",
    "import pywt\n",
    "import math\n",
    "from itertools import product\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import scipy\n",
    "import csv\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.decomposition import (PCA, LatentDirichletAllocation)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor, GradientBoostingClassifier )\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import xgboost as xgb\n",
    "from biosppy.signals import (ecg, tools)\n",
    "\n",
    "'''import data'''\n",
    "sample_path = \"sample.csv\"\n",
    "train_path = \"X_train.csv\"\n",
    "target_path =  \"y_train.csv\"\n",
    "test_path = \"X_test.csv\"\n",
    "\n",
    "wavelets = False\n",
    "num_cols = 18154\n",
    "sampling_rate = 300\n",
    "cols = [\"id\"] + [\"x\" + str(i) for i in range(num_cols)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5117, 18154) (3411, 18154) (5117, 1)\n"
     ]
    }
   ],
   "source": [
    "xtrain = pd.read_csv(train_path, names = cols)[1:]\n",
    "xtrain.drop(\"id\", axis=1, inplace = True)\n",
    "\n",
    "xtest =  pd.read_csv(test_path, names = cols)[1:]\n",
    "id_test = xtest.columns[0]\n",
    "xtest.drop(\"id\", axis=1, inplace = True)\n",
    "\n",
    "ytrain = pd.read_csv(target_path)\n",
    "ytrain.drop(\"id\", axis=1, inplace = True)\n",
    "\n",
    "print(xtrain.shape, xtest.shape, ytrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions -- (Feature Extraction and Submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_sqrd_diff(rpeaks):\n",
    "    diff = np.diff(rpeaks)\n",
    "    mean_sqrd = np.mean(diff*diff)\n",
    "    \n",
    "    return mean_sqrd \n",
    "\n",
    "def obtain_features(signal, sampling_rate):\n",
    "    X = list()\n",
    "    \n",
    "    '''features obtained from biosppy'''\n",
    "    ts, filtered, rpeaks, templates_ts, templates, heart_rate_ts, heart_rate = ecg.ecg(signal, sampling_rate, show = False)\n",
    "    \n",
    "    '''\n",
    "    Correct R-peak locations to the maximum --- introduce some tolerance level\n",
    "    '''\n",
    "    rpeaks = ecg.correct_rpeaks(signal = signal, rpeaks = rpeaks, sampling_rate = sampling_rate, tol = 0.01)  \n",
    "    \n",
    "    '''\n",
    "    extracting values of R-peaks -- Note: rpeaks gives only indices for R-peaks location\n",
    "    '''\n",
    "    peak_values = signal[rpeaks]\n",
    "    \n",
    "    if len(heart_rate) < 2:\n",
    "        heart_rate = [0, 1]\n",
    "    if len(heart_rate_ts) < 2:\n",
    "        heart_rate_ts = [0, 1]\n",
    "           \n",
    "    X.append(np.mean(peak_values))\n",
    "    X.append(np.median(peak_values))\n",
    "    X.append(np.min(peak_values))\n",
    "    X.append(np.max(peak_values))\n",
    "    X.append(np.std(peak_values))\n",
    "    X.append(np.sqrt(mean_sqrd_diff(rpeaks)))\n",
    "    X.append(np.mean(rpeaks))\n",
    "    X.append(np.median(rpeaks))\n",
    "    X.append(np.min(rpeaks))\n",
    "    X.append(np.max(rpeaks))\n",
    "    X.append(np.std(rpeaks))\n",
    "    X.append(np.mean(np.diff(rpeaks)))\n",
    "    X.append(np.median(np.diff(rpeaks)))\n",
    "    X.append(np.min(np.diff(rpeaks)))\n",
    "    X.append(np.max(np.diff(rpeaks)))\n",
    "    X.append(np.std(np.diff(rpeaks)))           \n",
    "    X.append(np.mean(heart_rate))\n",
    "    X.append(np.median(heart_rate))\n",
    "    X.append(np.min(heart_rate))\n",
    "    X.append(np.max(heart_rate))\n",
    "    X.append(np.std(heart_rate))\n",
    "    X.append(np.mean(np.diff(heart_rate)))\n",
    "    X.append(np.median(np.diff(heart_rate)))\n",
    "    X.append(np.min(np.diff(heart_rate)))\n",
    "    X.append(np.max(np.diff(heart_rate)))\n",
    "    X.append(np.std(np.diff(heart_rate)))\n",
    "    X.append(np.mean(heart_rate_ts))\n",
    "    X.append(np.median(heart_rate_ts))\n",
    "    X.append(np.min(heart_rate_ts))\n",
    "    X.append(np.max(heart_rate_ts))\n",
    "    X.append(np.std(heart_rate_ts))\n",
    "    X.append(np.mean(np.diff(heart_rate_ts)))\n",
    "    X.append(np.median(np.diff(heart_rate_ts)))\n",
    "    X.append(np.min(np.diff(heart_rate_ts)))\n",
    "    X.append(np.min(np.diff(heart_rate_ts)))\n",
    "    X.append(np.max(np.diff(heart_rate_ts)))\n",
    "    X.append(np.std(np.diff(heart_rate_ts)))\n",
    "    X.append(np.sum(filtered-signal))\n",
    "    \n",
    "    X += list(np.mean(templates, axis = 0))\n",
    "    X += list(np.median(templates, axis = 0))\n",
    "    \n",
    "    '''removed fft -- no improvements by adding it'''\n",
    "    #X += list(np.abs(np.fft.rfft(np.mean(templates, axis=0), axis=0))[0:45])   ### adding FFT (choose only half of entries)\n",
    "    \n",
    "    X += list(np.min(templates, axis=0))\n",
    "    X += list(np.max(templates, axis=0))\n",
    "    X += list(np.std(templates, axis = 0))\n",
    "    \n",
    "    '''convert X into numpy array for later analysis'''\n",
    "    X = np.array(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def make_submission(name_csv, predictions_vec):\n",
    "    sample =  pd.read_csv(sample_path)\n",
    "    sample[\"y\"] = predictions_vec\n",
    "    sample.to_csv(name_csv, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain features from raw signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65736db50ded4dca89761d0297e9f83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5117), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf287bf7228488d80e6a898fbdabe1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3411), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5117, 938) (5117,) (3411, 938)\n"
     ]
    }
   ],
   "source": [
    "sampling_rate = float(sampling_rate)\n",
    "hb_features_train = list()\n",
    "hb_features_test = list()\n",
    "\n",
    "for id in tqdm(range(xtrain.shape[0])):\n",
    "    signal_train = np.array(pd.to_numeric(xtrain.iloc[id].dropna()))\n",
    "    hb_features_train.append(obtain_features(signal_train, sampling_rate))\n",
    "    \n",
    "for id in tqdm(range(xtest.shape[0])):\n",
    "    signal_test = np.array(pd.to_numeric(xtest.iloc[id].dropna()))\n",
    "    hb_features_test.append(obtain_features(signal_test, sampling_rate))\n",
    "    \n",
    "X_train = np.array(hb_features_train)\n",
    "y_train = np.ravel(np.array(ytrain.values))    \n",
    "X_test = np.array(hb_features_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2047,) (2047, 938)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "use random subset of initial dataframe X for model selection  \n",
    "'''\n",
    "\n",
    "X_train = pd.DataFrame(X_train) \n",
    "X_train['y'] = y_train\n",
    "X_sub = pd.DataFrame(X_train).sample(frac = 0.40, replace = False, axis = 0)\n",
    "y_sub = X_sub['y']\n",
    "X_sub = X_sub.drop('y', axis = 1).values\n",
    "X_train = X_train.drop('y', axis = 1)\n",
    "print(y_sub.shape, X_sub.shape)\n",
    "\n",
    "'''define score function'''\n",
    "scorer_f1 = make_scorer(f1_score, greater_is_better = True, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 648 candidates, totalling 1296 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1296 out of 1296 | elapsed: 577.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8151260504201681\n",
      "{'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__max_features': 60, 'classifier__n_estimators': 250, 'impute__fill_value': 0, 'impute__strategy': 'median'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n## XGB APPROACH -- GRID-SEARCH CV\\n\\n\\nsteps = [(\"scaler\", preprocessing.StandardScaler()), (\"classifier\", xgb.XGBClassifier())]\\npipeline = Pipeline(steps = steps)\\n\\nparameters = {\"classifier__max_depth\": [5,10,15],\\n              \"classifier__n_estimators\": [200],\\n              \"classifier__learning_rate\": [0.05,0.1],\\n              \"classifier__max_features\": [20,40]\\n             }\\n\\ngrid = GridSearchCV(pipeline, parameters, cv = 2, scoring = scorer_f1, verbose = 1)\\n\\ngrid.fit(X_sub, y_sub)\\nprint(grid.best_score_)\\nprint(grid.best_params_)\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Gradient Boosting APPROACH -- GRID-SEARCH CV\n",
    "''' \n",
    "steps = [(\"impute\", SimpleImputer()),\n",
    "         (\"scaler\", preprocessing.StandardScaler()), \n",
    "         (\"classifier\", GradientBoostingClassifier())]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"impute__strategy\": [\"mean\", \"median\", \"constant\"],\n",
    "              \"impute__fill_value\": [0],\n",
    "              \"classifier__max_depth\": [3,4,5,6,7,8],\n",
    "              \"classifier__n_estimators\": [200,250,300],\n",
    "              \"classifier__learning_rate\": [0.1,0.08,0.05,0.03],\n",
    "              \"classifier__max_features\": [40,50,60]\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, cv = 2, scoring = scorer_f1, verbose = 1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "\n",
    "\n",
    "'''\n",
    "## SVC APPROACH -- GRID-SEARCH CV\n",
    "\n",
    "steps = [(\"scaler\", preprocessing.StandardScaler()), (\"classifier\", SVC())]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__kernel\": [\"rbf\", \"poly\"],\n",
    "              \"classifier__gamma\": [\"auto\"],\n",
    "              \"classifier__C\": [15,30,45,60,75],  \n",
    "              \"classifier__class_weight\": [\"balanced\"],\n",
    "              \"classifier__degree\": [2,4,6,8]\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, cv = 5, scoring = scorer_f1, verbose = 2)\n",
    "\n",
    "grid.fit(X, y)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "\n",
    "estimator = SVC(C = grid.best_params_['classifier__C'], gamma = 'auto', \n",
    "                class_weight = 'balanced', \n",
    "                kernel = grid.best_params_['classifier__kernel'], \n",
    "                degree = grid.best_params_['classifier__degree'])\n",
    "\n",
    "estimator.fit(xtrain_scaled, y)\n",
    "pred = estimator.predict(xtest_scaled)\n",
    "make_submission(\"prediction_trial.csv\", pred)\n",
    "'''\n",
    "\n",
    "'''\n",
    "## XGB APPROACH -- GRID-SEARCH CV\n",
    "\n",
    "\n",
    "steps = [(\"scaler\", preprocessing.StandardScaler()), (\"classifier\", xgb.XGBClassifier())]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__max_depth\": [5,10,15],\n",
    "              \"classifier__n_estimators\": [200],\n",
    "              \"classifier__learning_rate\": [0.05,0.1],\n",
    "              \"classifier__max_features\": [20,40]\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, cv = 2, scoring = scorer_f1, verbose = 1)\n",
    "\n",
    "grid.fit(X_sub, y_sub)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''replacing NaNs with median of columns'''\n",
    "\n",
    "impute = SimpleImputer(strategy = 'median', fill_value = 0)\n",
    "X_train = impute.fit_transform(X_train)\n",
    "X_test = impute.fit_transform(X_test)\n",
    "\n",
    "'''rescaling data'''\n",
    "scaler = StandardScaler() \n",
    "scaler.fit(X_train)\n",
    "x_train_scaled = scaler.transform(X_train)\n",
    "x_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "'''using best parameter given by GS'''\n",
    "estimator = GradientBoostingClassifier(n_estimators = 250, \n",
    "                                       max_depth = 5,\n",
    "                                       learning_rate = 0.1, \n",
    "                                       max_features = 60)\n",
    "\n",
    "\n",
    "'''\n",
    "estimator = GradientBoostingClassifier(n_estimators = grid.best_params_['classifier__n_estimators'], \n",
    "                                       max_depth = grid.best_params_['classifier__max_depth'],\n",
    "                                       learning_rate = grid.best_params_['classifier__learning_rate'], \n",
    "                                       max_features= grid.best_params_['classifier__max_features'])\n",
    "'''\n",
    "'''making predictions'''\n",
    "estimator.fit(x_train_scaled, y_train)\n",
    "predictions = estimator.predict(x_test_scaled)\n",
    "make_submission(\"FINAL_SUBMISSION.csv\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
